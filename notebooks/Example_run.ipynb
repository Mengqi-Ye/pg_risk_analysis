{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5263e9-087d-4863-a4e5-839dc33017bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from osgeo import ogr,gdal\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pyproj\n",
    "from pygeos import from_wkb,from_wkt\n",
    "import pygeos\n",
    "from tqdm import tqdm\n",
    "from shapely.wkb import loads\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11a902-f512-4fd3-b035-af38041ee083",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdal.SetConfigOption(\"OSM_CONFIG_FILE\", os.path.join('..',\"osmconf.ini\"))\n",
    "\n",
    "# change paths to make it work on your own machine\n",
    "data_path = os.path.join('C:\\\\','data','pg_risk_analysis')\n",
    "tc_path = os.path.join(data_path,'tc_netcdf')\n",
    "osm_data_path = os.path.join('C:\\\\','data','country_osm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc83d355-8c98-4ae6-97b1-3203a29a4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_b(geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    This function builds an SQL query from the values passed to the retrieve() function.\n",
    "    Arguments:\n",
    "         *geoType* : Type of geometry (osm layer) to search for.\n",
    "         *keyCol* : A list of keys/columns that should be selected from the layer.\n",
    "         ***valConstraint* : A dictionary of constraints for the values. e.g. WHERE 'value'>20 or 'value'='constraint'\n",
    "    Returns:\n",
    "        *string: : a SQL query string.\n",
    "    \"\"\"\n",
    "    query = \"SELECT \" + \"osm_id\"\n",
    "    for a in keyCol: query+= \",\"+ a  \n",
    "    query += \" FROM \" + geoType + \" WHERE \"\n",
    "    # If there are values in the dictionary, add constraint clauses\n",
    "    if valConstraint: \n",
    "        for a in [*valConstraint]:\n",
    "            # For each value of the key, add the constraint\n",
    "            for b in valConstraint[a]: query += a + b\n",
    "        query+= \" AND \"\n",
    "    # Always ensures the first key/col provided is not Null.\n",
    "    query+= \"\"+str(keyCol[0]) +\" IS NOT NULL\" \n",
    "    return query \n",
    "\n",
    "\n",
    "def retrieve(osm_path,geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    Function to extract specified geometry and keys/values from OpenStreetMap\n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.     \n",
    "        *geoType* : Type of Geometry to retrieve. e.g. lines, multipolygons, etc.\n",
    "        *keyCol* : These keys will be returned as columns in the dataframe.\n",
    "        ***valConstraint: A dictionary specifiying the value constraints.  \n",
    "        A key can have multiple values (as a list) for more than one constraint for key/value.  \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with all columns, geometries, and constraints specified.    \n",
    "    \"\"\"\n",
    "    driver=ogr.GetDriverByName('OSM')\n",
    "    data = driver.Open(osm_path)\n",
    "    query = query_b(geoType,keyCol,**valConstraint)\n",
    "    sql_lyr = data.ExecuteSQL(query)\n",
    "    features =[]\n",
    "    # cl = columns \n",
    "    cl = ['osm_id'] \n",
    "    for a in keyCol: cl.append(a)\n",
    "    if data is not None:\n",
    "        print('query is finished, lets start the loop')\n",
    "        for feature in tqdm(sql_lyr,desc='extract'):\n",
    "            #try:\n",
    "            if feature.GetField(keyCol[0]) is not None:\n",
    "                geom1 = (feature.geometry().ExportToWkt())\n",
    "                #print(geom1)\n",
    "                geom = from_wkt(feature.geometry().ExportToWkt()) \n",
    "                if geom is None:\n",
    "                    continue\n",
    "                # field will become a row in the dataframe.\n",
    "                field = []\n",
    "                for i in cl: field.append(feature.GetField(i))\n",
    "                field.append(geom)   \n",
    "                features.append(field)\n",
    "            #except:\n",
    "            #    print(\"WARNING: skipped OSM feature\")   \n",
    "    else:\n",
    "        print(\"ERROR: Nonetype error when requesting SQL. Check required.\")    \n",
    "    cl.append('geometry')                   \n",
    "    if len(features) > 0:\n",
    "        return pd.DataFrame(features,columns=cl)\n",
    "    else:\n",
    "        print(\"WARNING: No features or No Memory. returning empty GeoDataFrame\") \n",
    "        return pd.DataFrame(columns=['osm_id','geometry'])\n",
    "\n",
    "def power_polyline(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract all energy linestrings from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'lines',['power','voltage'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).rename(columns={'power': 'asset'})\n",
    "   \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def power_polygon(osm_path): # check with joel, something was wrong here with extracting substations\n",
    "    \"\"\"\n",
    "    Function to extract energy polygons from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['other_tags']) \n",
    "    \n",
    "    df = df.loc[(df.other_tags.str.contains('power'))]   #keep rows containing power data         \n",
    "    df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})     \n",
    "    \n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"substation\"', case=False)]  = 'substation' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"plant\"', case=False)] = 'plant' #specify row\n",
    "    \n",
    "    df = df.loc[(df.asset == 'substation') | (df.asset == 'plant')]\n",
    "            \n",
    "    return df.reset_index(drop=True) \n",
    "\n",
    "def electricity(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract building polygons from OpenStreetMap    \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with all unique building polygons.    \n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['power'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).rename(columns={'power': 'asset'})\n",
    "    \n",
    "    df = df[df.asset!='generator']\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"substation\"', case=False)]  = 'substation' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"plant\"', case=False)] = 'plant' #specify row\n",
    "    \n",
    "    df = df.loc[(df.asset == 'substation') | (df.asset == 'plant')]\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def retrieve_poly_subs(osm_path, w_list, b_list):\n",
    "    \"\"\"\n",
    "    Function to extract electricity substation polygons from OpenStreetMap\n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region\n",
    "        for which we want to do the analysis.\n",
    "        *w_list* :  white list of keywords to search in the other_tags columns\n",
    "        *b_list* :  black list of keywords of rows that should not be selected\n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique substation.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['other_tags'])\n",
    "    df = df[df.other_tags.str.contains('substation', case=False, na=False)]\n",
    "    #df = df.loc[(df.other_tags.str.contains('substation'))]\n",
    "    df = df[~df.other_tags.str.contains('|'.join(b_list))]\n",
    "    #df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})\n",
    "    df['asset']  = 'substation' #specify row\n",
    "    #df = df.loc[(df.asset == 'substation')] #specify row\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def power_point(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract energy points from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"   \n",
    "    df = retrieve(osm_path,'points',['other_tags']) \n",
    "    df = df.loc[(df.other_tags.str.contains('power'))]  #keep rows containing power data       \n",
    "    df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})     \n",
    "    print(df)\n",
    "    \n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"tower\"', case=False)]  = 'power_tower' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"pole\"', case=False)] = 'power_pole' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"utility\"=>\"power\"', case=False)] = 'power_tower' #specify row\n",
    "    \n",
    "    df = df.loc[(df.asset == 'power_tower') | (df.asset == 'power_pole')]\n",
    "            \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ae72c-6b39-4445-9015-3f5c9b8050a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_point(osm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe9f2e-0be0-47fd-83ca-67f3a272279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject(df_ds,current_crs=\"epsg:4326\",approximate_crs = \"epsg:3857\"):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        df_ds ([type]): [description]\n",
    "        current_crs (str, optional): [description]. Defaults to \"epsg:3857\".\n",
    "        approximate_crs (str, optional): [description]. Defaults to \"epsg:4326\".\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"    \n",
    "\n",
    "    geometries = df_ds['geometry']\n",
    "    coords = pygeos.get_coordinates(geometries)\n",
    "    transformer=pyproj.Transformer.from_crs(current_crs, approximate_crs,always_xy=True)\n",
    "    new_coords = transformer.transform(coords[:, 0], coords[:, 1])\n",
    "    \n",
    "    return pygeos.set_coordinates(geometries.copy(), np.array(new_coords).T) \n",
    "\n",
    "def load_curves_maxdam(data_path,hazard='wind'): \n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        data_path ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    \n",
    "    if hazard == 'wind':\n",
    "        sheet_name = 'flooding_curves'\n",
    "    elif hazard == 'flood':\n",
    "        sheet_name = 'flooding_curves'\n",
    "    \n",
    "    # load curves and maximum damages as separate inputs\n",
    "    curves = pd.read_excel(data_path,sheet_name=sheet_name,skiprows=8,index_col=[0])\n",
    "    maxdam=pd.read_excel(data_path,sheet_name=sheet_name,index_col=[0]).iloc[:5]\n",
    "    \n",
    "    curves.columns = maxdam.columns\n",
    "\n",
    "    #transpose maxdam so its easier work with the dataframe\n",
    "    maxdam = maxdam.T\n",
    "\n",
    "    #interpolate the curves to fill missing values\n",
    "    curves = curves.interpolate()\n",
    "   \n",
    "    return curves,maxdam\n",
    "\n",
    "def buffer_assets(assets,buffer_size=100):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        assets ([type]): [description]\n",
    "        buffer_size (int, optional): [description]. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"    \n",
    "    assets['buffered'] = pygeos.buffer(assets.geometry.values,buffer_size)\n",
    "    return assets\n",
    "\n",
    "def overlay_hazard_assets(df_ds,assets):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        df_ds ([type]): [description]\n",
    "        assets ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    #overlay \n",
    "    hazard_tree = pygeos.STRtree(df_ds.geometry.values)\n",
    "    if (pygeos.get_type_id(assets.iloc[0].geometry) == 3) | (pygeos.get_type_id(assets.iloc[0].geometry) == 6):\n",
    "        return  hazard_tree.query_bulk(assets.geometry,predicate='intersects')    \n",
    "    else:\n",
    "        return  hazard_tree.query_bulk(assets.buffered,predicate='intersects')\n",
    "    \n",
    "def get_damage_per_asset_per_rp(asset,df_ds,assets,curves,maxdam,return_period,country):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        asset ([type]): [description]\n",
    "        df_ds ([type]): [description]\n",
    "        assets ([type]): [description]\n",
    "        grid_size (int, optional): [description]. Defaults to 90.\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"    \n",
    "\n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = df_ds.iloc[asset[1]['hazard_point'].values].reset_index()\n",
    "    get_hazard_points = get_hazard_points.loc[pygeos.intersects(get_hazard_points.geometry.values,assets.iloc[asset[0]].geometry)]\n",
    "\n",
    "    asset_type = assets.iloc[asset[0]].asset\n",
    "    asset_geom = assets.iloc[asset[0]].geometry\n",
    "\n",
    "    if asset_type in ['plant','substation','generator']:\n",
    "        maxdam_asset = maxdam.loc[asset_type].MaxDam/pygeos.area(asset_geom)\n",
    "    else:\n",
    "        maxdam_asset = maxdam.loc[asset_type].MaxDam\n",
    "\n",
    "\n",
    "    hazard_intensity = curves[asset_type].index.values\n",
    "    fragility_values = curves[asset_type].values\n",
    "    \n",
    "    if len(get_hazard_points) == 0:\n",
    "        return asset[0],0\n",
    "    else:\n",
    "        \n",
    "        if pygeos.get_type_id(asset_geom) == 1:\n",
    "            get_hazard_points['overlay_meters'] = pygeos.length(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            return asset[0],np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,fragility_values))*get_hazard_points.overlay_meters*maxdam_asset)\n",
    "        \n",
    "        elif  pygeos.get_type_id(asset_geom) == 3:\n",
    "            get_hazard_points['overlay_m2'] = pygeos.area(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            return asset[0],get_hazard_points.apply(lambda x: np.interp(x[return_period], hazard_intensity, fragility_values)*maxdam_asset*x.overlay_m2,axis=1).sum()     \n",
    "        \n",
    "        else:\n",
    "            return asset[0],np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,fragility_values))*maxdam_asset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ff6dd-a3f2-4e2a-becd-e5cffa38edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_hazard_data(climate_model,hazard_type='storm'):\n",
    "    \n",
    "    if hazard_type == 'storm':\n",
    "        with xr.open_dataset(os.path.join(tc_path,'STORM_FIXED_RETURN_PERIODS{}_WP.nc'.format(climate_model))) as ds:\n",
    "            \"\"\"\n",
    "            TC climate model:\n",
    "                CMCC-CM2-VHR4\n",
    "                CNRM-CM6-1-HR\n",
    "                EC-Earth3P-HR\n",
    "                HadGEM3-GC31-HM\n",
    "            \"\"\"\n",
    "            print(ds.keys())\n",
    "            \n",
    "            # get the mean values\n",
    "            df_ds = ds['mean'].to_dataframe().unstack(level=2).reset_index()\n",
    "\n",
    "            # create geometry values and drop lat lon columns\n",
    "            df_ds['geometry'] = [pygeos.points(x) for x in list(zip(df_ds['lon'],df_ds['lat']))]\n",
    "            df_ds = df_ds.drop(['lat','lon'],axis=1,level=0)\n",
    "\n",
    "            #rename columns to return periods\n",
    "            return_periods = ['1_{}{}'.format(int(x),climate_model) for x in ds['rp']]\n",
    "            df_ds.columns = ['1_{}{}'.format(int(x),climate_model) for x in list(df_ds.columns.get_level_values(1))[:-1]]+['geometry']     \n",
    "            df_ds['geometry'] = pygeos.buffer(df_ds.geometry,radius=0.1/2,cap_style='square').values\n",
    "            df_ds['geometry'] = reproject(df_ds)\n",
    "            \n",
    "            # drop all non values to reduce size\n",
    "            if climate_model == '':\n",
    "                df_ds = df_ds.loc[~df_ds['1_10000'].isna()].reset_index(drop=True)\n",
    "            \n",
    "            df_ds = df_ds.fillna(0)\n",
    "                        \n",
    "    elif hazard_type == 'flood':\n",
    "        \n",
    "        # THIS STILL NEEDS TO BE TESTED WITH GLOFRIS DATA\n",
    "        with xr.open_dataset(os.path.join(fl_path,'HIST/inuncoast_historical_nosub_hist_rp0500_0.nc')) as ds: #, engine=\"rasterio\"\n",
    "            df_ds = ds.to_dataframe().reset_index()\n",
    "            df_ds['geometry'] = pygeos.points(df_ds.x,y=df_ds.y)\n",
    "            df_ds = df_ds.rename(columns={'band_data': 'hazard_intensity'})\n",
    "            df_ds = df_ds.drop(['band','x', 'y','spatial_ref'], axis=1)\n",
    "            df_ds = df_ds.dropna()\n",
    "            df_ds = df_ds.reset_index(drop=True)\n",
    "            df_ds.geometry= pygeos.buffer(df_ds.geometry,radius=20/2,cap_style='square').values\n",
    "            df_ds['geometry'] = reproject(df_ds)\n",
    "        \n",
    "    return df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c984c03-a4fd-43d8-b638-6165609fbc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_data_path = os.path.join('C:\\\\','data','country_osm')\n",
    "country_code = 'VNM'\n",
    "osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d37345b-a1b0-486a-9d2c-86400fe78aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# load hazard data \n",
    "climate_model = '_CMCC-CM2-VHR4' #_CMCC-CM2-VHR4\n",
    "df_ds = open_hazard_data(climate_model, hazard_type='storm')\n",
    "df_ds.head()\n",
    "#df_ds.to_csv(os.path.join(data_path,'retrieve_data/df_ds.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc7c1d-20fd-4fc6-9cc1-ea8d54084a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_data_path = os.path.join('C:\\\\','data','country_osm')\n",
    "filelist = []\n",
    "country_codes = []\n",
    "for i in os.listdir(osm_data_path):\n",
    "    osm_path = os.path.join(osm_data_path,i)\n",
    "    if os.path.isfile(osm_path):\n",
    "        filelist.append(i)\n",
    "        country_codes.append(os.path.splitext(os.path.splitext(i)[0])[0])\n",
    "        print(country_codes)\n",
    "        osm_path = os.path.join(osm_data_path,i)\n",
    "        print(osm_path)\n",
    "country_codes = tuple(country_codes)\n",
    "country_codes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46532d04-d3a9-46f0-87e6-1553b0c76c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all data from OSM for power infrastructure\n",
    "# reproject to epsg:3857 to have coordinate system in meters\n",
    "#osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "\n",
    "country_code = ''\n",
    "\n",
    "#lines\n",
    "power_lines = {country_code:[]}\n",
    "for country_code in country_codes:\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    print(osm_path)\n",
    "    power_lines_country = power_polyline(osm_path)\n",
    "    power_lines_country['geometry'] = reproject(power_lines_country)\n",
    "    power_lines_country = buffer_assets(power_lines_country.loc[power_lines_country.asset.isin(\n",
    "        ['cable','minor_cable','line','minor_line'])],buffer_size=100).reset_index(drop=True)\n",
    "    power_lines[country_code] = power_lines_country\n",
    "\n",
    "#polygons\n",
    "power_poly = {country_code:[]}\n",
    "for country_code in country_codes:\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    print(osm_path)\n",
    "    power_poly_country = electricity(osm_path)\n",
    "    power_poly_country['geometry'] = reproject(power_poly_country)\n",
    "    power_poly[country_code] = power_poly_country\n",
    "    \n",
    "\"\"\" \n",
    "    power_poly_country = power_polygon(osm_path)\n",
    "    power_poly_country['geometry'] = reproject(power_poly_country)\n",
    "    #power_poly = power_poly.loc[power_poly.asset.isin(['plant','substation'])].reset_index(drop=True)\n",
    "    power_poly[country_code] = power_poly_country\n",
    "   \n",
    "    #substations polygons\n",
    "    ### KEY:VAL IN OTHER_TAGS ###\n",
    "    w_list = ['substation'] # add in funtion '|'.join(w_list) if more than one key word\n",
    "    b_list = ['minor_distribution', 'converter', 'indoor', 'pipeline', 'gas']\n",
    "    power_sub = retrieve_poly_subs(osm_path,w_list,b_list)\n",
    "    #power_sub = retrieve_poly_subs.rename(columns={'power':'asset'})\n",
    "    power_sub['geometry'] = reproject(power_sub)\n",
    "\"\"\"\n",
    "#points\n",
    "power_points = {country_code:[]}\n",
    "for country_code in country_codes:\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    print(osm_path)\n",
    "    power_points_country = power_point(osm_path)\n",
    "    power_points_country['geometry'] = reproject(power_points_country)\n",
    "    power_points_country = buffer_assets(power_points_country.loc[power_points_country.asset.isin(\n",
    "        ['power_tower','power_pole'])],buffer_size=100).reset_index(drop=True)\n",
    "    power_points[country_code] = power_points_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb709e5-71af-4380-b6f5-51784e50f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load curves and maxdam\n",
    "curves,maxdam = load_curves_maxdam(data_path=os.path.join('..','data','infra_vulnerability_data.xlsx'))\n",
    "curves['line'] = 1 # remove this when things work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de7639-9846-4a03-bd07-c53e9353c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return periods\n",
    "return_periods = ['1_10{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),'1_1000{}'.format(climate_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8de6ab-88b0-479d-b04e-01845fc6c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines in loop\n",
    "damaged_lines = {country_code:[]}\n",
    "for country_code in country_codes:\n",
    "    overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds,power_lines.get(country_code)).T,columns=['asset','hazard_point'])\n",
    "    collect_line_damages = []\n",
    "    for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),desc='polyline damage calculation for {}'.format(country_code)):\n",
    "        for return_period in return_periods:\n",
    "            collect_line_damages.append([return_period,get_damage_per_asset_per_rp(asset,df_ds,power_lines.get(country_code),curves,maxdam,return_period,country_code)])\n",
    "\n",
    "    collect_line_damages = [(line[0],line[1][0],line[1][1]) for line in collect_line_damages]\n",
    "    damaged_lines_country = power_lines.get(country_code).merge(pd.DataFrame(collect_line_damages,columns=['return_period','index','damage']),left_index=True,right_on='index')\n",
    "    damaged_lines_country = damaged_lines_country.drop(['buffered'],axis=1)\n",
    "    damaged_lines[country_code] = damaged_lines_country\n",
    "#damaged_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b530c2-8141-46af-bc88-9c56cd83362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "damaged_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b232c05-9697-47da-a243-e2435a429594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polygons in loop\n",
    "damaged_poly = {country_code:[]}\n",
    "for country_code in country_codes:\n",
    "    overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds,power_poly.get(country_code)).T,columns=['asset','hazard_point'])\n",
    "    collect_poly_damages = []\n",
    "    for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),desc='polygon damage calculation for {}'.format(country_code)):\n",
    "        for return_period in return_periods:\n",
    "            collect_poly_damages.append([return_period,get_damage_per_asset_per_rp(asset,df_ds,power_poly.get(country_code),curves,maxdam,return_period,country_code)])\n",
    "\n",
    "    collect_poly_damages = [(line[0],line[1][0],line[1][1]) for line in collect_poly_damages]\n",
    "    damaged_poly_country = power_poly.get(country_code).merge(pd.DataFrame(collect_poly_damages,columns=['return_period','index','damage']),left_index=True,right_on='index')\n",
    "    damaged_poly[country_code] = damaged_poly_country\n",
    "#damaged_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a9dce-bd12-4412-87a9-64b61110b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# points in loop\n",
    "damaged_points = {country_code:[]}\n",
    "for country_code in country_codes:\n",
    "    overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds,power_points.get(country_code)).T,columns=['asset','hazard_point'])\n",
    "    collect_point_damages = []\n",
    "    for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),desc='point damage calculation for {}'.format(country_code)):\n",
    "        for return_period in return_periods:\n",
    "            collect_point_damages.append([return_period,get_damage_per_asset_per_rp(asset,df_ds,power_points.get(country_code),curves,maxdam,return_period,country_code)])\n",
    "\n",
    "    collect_point_damages = [(line[0],line[1][0],line[1][1]) for line in collect_point_damages]\n",
    "    damaged_points_country = power_points.get(country_code).merge(pd.DataFrame(collect_point_damages,columns=['return_period','index','damage']),left_index=True,right_on='index')\n",
    "    damaged_points_country = damaged_points_country.drop(['buffered'],axis=1)\n",
    "    damaged_points[country_code] = damaged_points_country\n",
    "#damaged_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a261d6-f964-41cf-a165-9865200f16e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all data from OSM for power infrastructurepower_polyline\n",
    "#reproject to epsg:3857 to have coordinate system in meters\n",
    "#lines\n",
    "power_lines = power_polyline(osm_path)\n",
    "power_lines = power_lines.rename(columns={'power':'asset'})\n",
    "power_lines['geometry'] = reproject(power_lines) \n",
    "power_lines = buffer_assets(power_lines.loc[power_lines.asset.isin(\n",
    "    ['cable','minor_cable','line','minor_line'])],buffer_size=100).reset_index(drop=True)\n",
    "    \n",
    "#polygons\n",
    "power_poly = electricity(osm_path)\n",
    "power_poly['geometry'] = reproject(power_poly)\n",
    "\"\"\"\n",
    "#polygons\n",
    "power_poly = power_polygon(osm_path)\n",
    "power_poly = power_poly.rename(columns={'power':'asset'})\n",
    "power_poly['geometry'] = reproject(power_poly)\n",
    "#power_poly = power_poly.loc[power_poly.asset.isin(['plant','substation'])].reset_index(drop=True)\n",
    "\n",
    "#substations polygons\n",
    "### KEY:VAL IN OTHER_TAGS ###\n",
    "w_list = ['substation'] # add in funtion '|'.join(w_list) if more than one key word\n",
    "b_list = ['minor_distribution', 'converter', 'indoor', 'pipeline', 'gas']\n",
    "power_sub = retrieve_poly_subs(osm_path,w_list,b_list)\n",
    "#power_sub = retrieve_poly_subs.rename(columns={'power':'asset'})\n",
    "power_sub['geometry'] = reproject(power_sub)\n",
    "\"\"\"\n",
    "    \n",
    "#points\n",
    "power_points = power_point(osm_path)\n",
    "#power_points = power_points.rename(columns={'power':'asset'})\n",
    "power_points['geometry'] = reproject(power_points)\n",
    "power_points = buffer_assets(power_points.loc[power_points.asset.isin(\n",
    "    ['power_tower','power_pole'])],buffer_size=100).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a16126-948e-445e-9696-4f1711d69342",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4c666-1e3a-4c52-b77b-9cea8a8475fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#power_points.to_csv(os.path.join(data_path,'retrieve_data/power_points.csv'))\n",
    "#power_lines.to_csv(os.path.join(data_path,'retrieve_data/power_lines.csv'))\n",
    "#power_poly.to_csv(os.path.join(data_path,'retrieve_data/power_poly.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187cda7-970d-4824-9e6a-d70a5071cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines\n",
    "overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds,power_lines).T,columns=['asset','hazard_point'])\n",
    "\n",
    "collect_line_damages = []\n",
    "for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),desc='polyline damage calculation for {}'.format(country_code)):\n",
    "    for return_period in return_periods:\n",
    "        collect_line_damages.append([return_period,get_damage_per_asset_per_rp(asset,df_ds,power_lines,curves,maxdam,return_period,country_code)])\n",
    "\n",
    "collect_line_damages = [(line[0],line[1][0],line[1][1]) for line in collect_line_damages]\n",
    "damaged_lines = power_lines.merge(pd.DataFrame(collect_line_damages,columns=['return_period','index','damage']),left_index=True,right_on='index')\n",
    "damaged_lines = damaged_lines.drop(['buffered'],axis=1) \n",
    "damaged_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ca7b7-bd0b-4249-8edf-90102cad5b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polygons\n",
    "overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds,power_poly).T,columns=['asset','hazard_point'])\n",
    "\n",
    "collect_poly_damages = []\n",
    "for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),desc='polygon damage calculation for {}'.format(country_code)):\n",
    "    for return_period in return_periods:\n",
    "        collect_poly_damages.append([return_period,get_damage_per_asset_per_rp(asset,df_ds,power_poly,curves,maxdam,return_period,country_code)])\n",
    "\n",
    "collect_poly_damages = [(line[0],line[1][0],line[1][1]) for line in collect_poly_damages]\n",
    "damaged_poly = power_poly.merge(pd.DataFrame(collect_poly_damages,columns=['return_period','index','damage']),left_index=True,right_on='index')\n",
    "damaged_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802acca9-4447-4bc2-910b-7a396dc45b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# points\n",
    "overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds,power_points).T,columns=['asset','hazard_point'])\n",
    "\n",
    "collect_point_damages = []\n",
    "#return_periods = ['1_10','1_50']\n",
    "for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),desc='point damage calculation for {}'.format(country_code)):\n",
    "    for return_period in return_periods:\n",
    "        collect_point_damages.append([return_period,get_damage_per_asset_per_rp(asset,df_ds,power_points,curves,maxdam,return_period,country_code)])\n",
    "\n",
    "collect_point_damages = [(line[0],line[1][0],line[1][1]) for line in collect_point_damages]\n",
    "damaged_points = power_points.merge(pd.DataFrame(collect_point_damages,columns=['return_period','index','damage']),left_index=True,right_on='index')\n",
    "damaged_points = damaged_points.drop(['buffered'],axis=1)\n",
    "\n",
    "#damaged_points.to_csv(os.path.join(data_path,'retrieve_data/damaged_points.csv'))\n",
    "damaged_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7765246-dc29-43e2-82fe-907c018e2432",
   "metadata": {},
   "outputs": [],
   "source": [
    "damaged_points\n",
    "#damaged_points['LAO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f54e2-624d-4113-863b-f633bf096f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load collected power grid data\n",
    "pg_data_path = os.path.join(data_path, 'pg_data')\n",
    "pg_data = gpd.read_file(os.path.join(pg_data_path,\"lao_line.gpkg\"))\n",
    "pg_data = pd.DataFrame(pg_data.copy())\n",
    "print(pg_data.head())\n",
    "pg_data.geometry = pygeos.from_shapely(pg_data.geometry)\n",
    "\n",
    "pg_data['geometry'] = reproject(pg_data)\n",
    "pg_data = buffer_assets(pg_data.loc[pg_data.asset.isin(['line'])],buffer_size=100).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0baa84d-4bab-4ffb-b139-0f3bafd7d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines\n",
    "overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds,pg_data).T,columns=['asset','hazard_point'])\n",
    "#print(overlay_lines.asset.unique())\n",
    "\n",
    "collect_line_damages = []\n",
    "for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),desc='polyline damage calculation for {}'.format(country_code)):\n",
    "    for return_period in return_periods:\n",
    "        collect_line_damages.append([return_period,get_damage_per_asset_per_rp(asset,df_ds,pg_data,curves,maxdam,return_period,country_code)])\n",
    "\n",
    "collect_line_damages = [(line[0],line[1][0],line[1][1]) for line in collect_line_damages]\n",
    "damaged_lines = pg_data.merge(pd.DataFrame(collect_line_damages,columns=['return_period','index','damage']),left_index=True,right_on='index')\n",
    "damaged_lines = damaged_lines.drop(['buffered'],axis=1)\n",
    "damaged_lines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
