{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5263e9-087d-4863-a4e5-839dc33017bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from osgeo import ogr,gdal\n",
    "import os\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pyproj\n",
    "from pygeos import from_wkb,from_wkt\n",
    "import pygeos\n",
    "from tqdm import tqdm\n",
    "from shapely.wkb import loads\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from shapely.geometry import mapping\n",
    "pd.options.mode.chained_assignment = None\n",
    "from rasterio.mask import mask\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f11a902-f512-4fd3-b035-af38041ee083",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdal.SetConfigOption(\"OSM_CONFIG_FILE\", os.path.join('..',\"osmconf.ini\"))\n",
    "\n",
    "# change paths to make it work on your own machine\n",
    "data_path = os.path.join('C:\\\\','Data','pg_risk_analysis')\n",
    "tc_path = os.path.join(data_path,'tc_netcdf')\n",
    "fl_path = os.path.join(data_path,'GLOFRIS')\n",
    "osm_data_path = os.path.join('C:\\\\','Data','country_osm')\n",
    "pg_data_path = os.path.join(data_path,'pg_data')\n",
    "vul_curve_path = os.path.join(data_path,'vulnerability_curves','input_vulnerability_data.xlsx')\n",
    "output_path = os.path.join('C:\\\\','projects','pg_risk_analysis','output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc83d355-8c98-4ae6-97b1-3203a29a4fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_b(geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    This function builds an SQL query from the values passed to the retrieve() function.\n",
    "    Arguments:\n",
    "         *geoType* : Type of geometry (osm layer) to search for.\n",
    "         *keyCol* : A list of keys/columns that should be selected from the layer.\n",
    "         ***valConstraint* : A dictionary of constraints for the values. e.g. WHERE 'value'>20 or 'value'='constraint'\n",
    "    Returns:\n",
    "        *string: : a SQL query string.\n",
    "    \"\"\"\n",
    "    query = \"SELECT \" + \"osm_id\"\n",
    "    for a in keyCol: query+= \",\"+ a  \n",
    "    query += \" FROM \" + geoType + \" WHERE \"\n",
    "    # If there are values in the dictionary, add constraint clauses\n",
    "    if valConstraint: \n",
    "        for a in [*valConstraint]:\n",
    "            # For each value of the key, add the constraint\n",
    "            for b in valConstraint[a]: query += a + b\n",
    "        query+= \" AND \"\n",
    "    # Always ensures the first key/col provided is not Null.\n",
    "    query+= \"\"+str(keyCol[0]) +\" IS NOT NULL\" \n",
    "    return query \n",
    "\n",
    "\n",
    "def retrieve(osm_path,geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    Function to extract specified geometry and keys/values from OpenStreetMap\n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.     \n",
    "        *geoType* : Type of Geometry to retrieve. e.g. lines, multipolygons, etc.\n",
    "        *keyCol* : These keys will be returned as columns in the dataframe.\n",
    "        ***valConstraint: A dictionary specifiying the value constraints.  \n",
    "        A key can have multiple values (as a list) for more than one constraint for key/value.  \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with all columns, geometries, and constraints specified.    \n",
    "    \"\"\"\n",
    "    driver=ogr.GetDriverByName('OSM')\n",
    "    data = driver.Open(osm_path)\n",
    "    query = query_b(geoType,keyCol,**valConstraint)\n",
    "    sql_lyr = data.ExecuteSQL(query)\n",
    "    features =[]\n",
    "    # cl = columns \n",
    "    cl = ['osm_id'] \n",
    "    for a in keyCol: cl.append(a)\n",
    "    if data is not None:\n",
    "        print('query is finished, lets start the loop')\n",
    "        for feature in tqdm(sql_lyr,desc='extract'):\n",
    "            #try:\n",
    "            if feature.GetField(keyCol[0]) is not None:\n",
    "                geom1 = (feature.geometry().ExportToWkt())\n",
    "                #print(geom1)\n",
    "                geom = from_wkt(feature.geometry().ExportToWkt()) \n",
    "                if geom is None:\n",
    "                    continue\n",
    "                # field will become a row in the dataframe.\n",
    "                field = []\n",
    "                for i in cl: field.append(feature.GetField(i))\n",
    "                field.append(geom)   \n",
    "                features.append(field)\n",
    "            #except:\n",
    "            #    print(\"WARNING: skipped OSM feature\")   \n",
    "    else:\n",
    "        print(\"ERROR: Nonetype error when requesting SQL. Check required.\")    \n",
    "    cl.append('geometry')                   \n",
    "    if len(features) > 0:\n",
    "        return pd.DataFrame(features,columns=cl)\n",
    "    else:\n",
    "        print(\"WARNING: No features or No Memory. returning empty GeoDataFrame\") \n",
    "        return pd.DataFrame(columns=['osm_id','geometry'])\n",
    "\n",
    "def power_polyline(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract all energy linestrings from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'lines',['power','voltage'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).rename(columns={'power': 'asset'})\n",
    "    \n",
    "    #print(df) #check infra keys\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def power_polygon(osm_path): # check with joel, something was wrong here with extracting substations\n",
    "    \"\"\"\n",
    "    Function to extract energy polygons from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['other_tags']) \n",
    "    \n",
    "    df = df.loc[(df.other_tags.str.contains('power'))]   #keep rows containing power data         \n",
    "    df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})     \n",
    "    \n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"substation\"', case=False)]  = 'substation' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"plant\"', case=False)] = 'plant' #specify row\n",
    "    \n",
    "    df = df.loc[(df.asset == 'substation') | (df.asset == 'plant')]\n",
    "            \n",
    "    return df.reset_index(drop=True) \n",
    "\n",
    "def electricity(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract building polygons from OpenStreetMap    \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with all unique building polygons.    \n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['power'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).rename(columns={'power': 'asset'})\n",
    "    \n",
    "    #df = df[df.asset!='generator']\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"substation\"', case=False)]  = 'substation' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"plant\"', case=False)] = 'plant' #specify row\n",
    "    \n",
    "    #print(df)  #check infra keys\n",
    "    \n",
    "    df = df.loc[(df.asset == 'substation') | (df.asset == 'plant')]\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def retrieve_poly_subs(osm_path, w_list, b_list):\n",
    "    \"\"\"\n",
    "    Function to extract electricity substation polygons from OpenStreetMap\n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region\n",
    "        for which we want to do the analysis.\n",
    "        *w_list* :  white list of keywords to search in the other_tags columns\n",
    "        *b_list* :  black list of keywords of rows that should not be selected\n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique substation.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['other_tags'])\n",
    "    df = df[df.other_tags.str.contains('substation', case=False, na=False)]\n",
    "    #df = df.loc[(df.other_tags.str.contains('substation'))]\n",
    "    df = df[~df.other_tags.str.contains('|'.join(b_list))]\n",
    "    #df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})\n",
    "    df['asset']  = 'substation' #specify row\n",
    "    #df = df.loc[(df.asset == 'substation')] #specify row\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def power_point(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract energy points from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"   \n",
    "    df = retrieve(osm_path,'points',['other_tags']) \n",
    "    df = df.loc[(df.other_tags.str.contains('power'))]  #keep rows containing power data       \n",
    "    df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})     \n",
    "    \n",
    "    #print(df)\n",
    "    \n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"tower\"', case=False)]  = 'power_tower' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"pole\"', case=False)] = 'power_pole' #specify row\n",
    "    #df['asset'].loc[df['asset'].str.contains('\"utility\"=>\"power\"', case=False)] = 'power_tower' #specify row\n",
    "    \n",
    "    df = df.loc[(df.asset == 'power_tower') | (df.asset == 'power_pole')]\n",
    "            \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "36fe9f2e-0be0-47fd-83ca-67f3a272279d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reproject(df_ds,current_crs=\"epsg:4326\",approximate_crs = \"epsg:3857\"):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        df_ds ([type]): [description]\n",
    "        current_crs (str, optional): [description]. Defaults to \"epsg:3857\".\n",
    "        approximate_crs (str, optional): [description]. Defaults to \"epsg:4326\".\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"    \n",
    "\n",
    "    geometries = df_ds['geometry']\n",
    "    coords = pygeos.get_coordinates(geometries)\n",
    "    transformer=pyproj.Transformer.from_crs(current_crs, approximate_crs,always_xy=True)\n",
    "    new_coords = transformer.transform(coords[:, 0], coords[:, 1])\n",
    "    \n",
    "    return pygeos.set_coordinates(geometries.copy(), np.array(new_coords).T) \n",
    "\n",
    "def load_curves_maxdam(vul_curve_path,hazard_type):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        data_path ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "\n",
    "    if hazard_type == 'tc':\n",
    "        sheet_name = 'wind_curves'\n",
    "    \n",
    "    elif hazard_type == 'fl':\n",
    "        sheet_name = 'flooding_curves'\n",
    "    \n",
    "    # load curves and maximum damages as separate inputs\n",
    "    curves = pd.read_excel(vul_curve_path,sheet_name=sheet_name,skiprows=11,index_col=[0])\n",
    "    \n",
    "    if hazard_type == 'fl':\n",
    "        maxdam = pd.read_excel(vul_curve_path,sheet_name=sheet_name,index_col=[0]).iloc[:8]\n",
    "    elif hazard_type == 'tc':\n",
    "        maxdam = pd.read_excel(vul_curve_path,sheet_name=sheet_name,index_col=[0],header=[0,1]).iloc[:8]\n",
    "        maxdam = maxdam.rename({'substation_point':'substation'},level=0,axis=1)\n",
    "            \n",
    "    curves.columns = maxdam.columns\n",
    "        \n",
    "    #transpose maxdam so its easier work with the dataframe\n",
    "    maxdam = maxdam.T\n",
    "\n",
    "    #interpolate the curves to fill missing values\n",
    "    curves = curves.interpolate()\n",
    "       \n",
    "    return curves,maxdam\n",
    "\n",
    "def buffer_assets(assets, buffer_size=100):\n",
    "    \"\"\"\n",
    "    Create a buffer of a specified size around the geometries in a GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        assets (GeoDataFrame): A GeoDataFrame containing geometries to be buffered.\n",
    "        buffer_size (int, optional): The distance in the units of the GeoDataFrame's CRS to buffer the geometries.\n",
    "            Defaults to 100.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A new GeoDataFrame with an additional column named 'buffered' containing the buffered\n",
    "            geometries.\n",
    "    \"\"\"\n",
    "    # Create a buffer of the specified size around the geometries\n",
    "    assets['buffered'] = pygeos.buffer(assets.geometry.values, buffer_size)\n",
    "    \n",
    "    return assets\n",
    "\n",
    "\n",
    "def overlay_hazard_assets(df_ds, assets):\n",
    "    \"\"\"\n",
    "    Overlay a set of assets with a hazard dataset and return the subset of assets that intersect with\n",
    "    one or more hazard polygons or lines.\n",
    "    \n",
    "    Args:\n",
    "        df_ds (GeoDataFrame): A GeoDataFrame containing the hazard dataset.\n",
    "        assets (GeoDataFrame): A GeoDataFrame containing the assets to be overlaid with the hazard dataset.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: A numpy array of integers representing the indices of the hazard geometries that intersect with\n",
    "            the assets. If the assets have a 'buffered' column, the buffered geometries are used for the overlay.\n",
    "    \"\"\"\n",
    "    hazard_tree = pygeos.STRtree(df_ds.geometry.values)\n",
    "    if (pygeos.get_type_id(assets.iloc[0].geometry) == 3) | (pygeos.get_type_id(assets.iloc[0].geometry) == 6):\n",
    "        return  hazard_tree.query_bulk(assets.geometry,predicate='intersects')    \n",
    "    else:\n",
    "        return  hazard_tree.query_bulk(assets.buffered,predicate='intersects')\n",
    "    \n",
    "def get_damage_per_asset_per_rp(asset,df_ds,assets,curves,maxdam,return_period,country):\n",
    "    \"\"\"\n",
    "    Calculates the damage per asset per return period based on asset type, hazard curves and maximum damage\n",
    "\n",
    "    Args:\n",
    "        asset (tuple): Tuple with two dictionaries, containing the asset index and the hazard point index of the asset\n",
    "        df_ds (pandas.DataFrame): A pandas DataFrame containing hazard points with a 'geometry' column\n",
    "        assets (geopandas.GeoDataFrame): A GeoDataFrame containing asset geometries and asset type information\n",
    "        curves (dict): A dictionary with the asset types as keys and their corresponding hazard curves as values\n",
    "        maxdam (pandas.DataFrame): A pandas DataFrame containing the maximum damage for each asset type\n",
    "        return_period (str): The return period for which the damage should be calculated\n",
    "        country (str): The country for which the damage should be calculated\n",
    "\n",
    "    Returns:\n",
    "        list or tuple: Depending on the input, the function either returns a list of tuples with the asset index, the curve name and the calculated damage, or a tuple with None, None, None if no hazard points are found\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = df_ds.iloc[asset[1]['hazard_point'].values].reset_index()\n",
    "    get_hazard_points = get_hazard_points.loc[pygeos.intersects(get_hazard_points.geometry.values,assets.iloc[asset[0]].geometry)]\n",
    "\n",
    "    asset_type = assets.iloc[asset[0]].asset\n",
    "    asset_geom = assets.iloc[asset[0]].geometry\n",
    "\n",
    "    if asset_type in ['plant','substation','generator']:\n",
    "        maxdam_asset = maxdam.loc[asset_type].MaxDam/pygeos.area(asset_geom)\n",
    "    else:\n",
    "        maxdam_asset = maxdam.loc[asset_type].MaxDam\n",
    "\n",
    "    hazard_intensity = curves[asset_type].index.values\n",
    "    \n",
    "    if isinstance(curves[asset_type],pd.core.series.Series):\n",
    "        fragility_values = curves[asset_type].values.flatten()\n",
    "        only_one = True\n",
    "        curve_name = curves[asset_type].name\n",
    "    elif len(curves[asset_type].columns) == 1:\n",
    "        fragility_values = curves[asset_type].values.flatten()      \n",
    "        only_one = True   \n",
    "        curve_name = curves[asset_type].columns[0]\n",
    "    else:\n",
    "        fragility_values = curves[asset_type].values#.T[0]\n",
    "        maxdam_asset = maxdam_asset.values#[0]\n",
    "        only_one = False\n",
    "\n",
    "    if len(get_hazard_points) == 0:\n",
    "        return None,None,None\n",
    "    else:\n",
    "        if only_one:    \n",
    "            # run the calculation as normal when the asset just has a single curve\n",
    "            if pygeos.get_type_id(asset_geom) == 1:            \n",
    "                get_hazard_points['overlay_meters'] = pygeos.length(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "                return asset[0],curve_name,np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,\n",
    "                                                             fragility_values))*get_hazard_points.overlay_meters*maxdam_asset)\n",
    "\n",
    "            elif (pygeos.get_type_id(asset_geom) == 3) | (pygeos.get_type_id(asset_geom) == 6) :\n",
    "                get_hazard_points['overlay_m2'] = pygeos.area(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "                return asset[0],curve_name,get_hazard_points.apply(lambda x: np.interp(x[return_period], \n",
    "                                                                                  hazard_intensity, \n",
    "                                                                                  fragility_values)*maxdam_asset*x.overlay_m2,axis=1).sum()     \n",
    "\n",
    "            else:\n",
    "                return asset[0],curve_name,np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                             hazard_intensity,fragility_values))*maxdam_asset)\n",
    "        else:\n",
    "            if pygeos.get_type_id(asset_geom) == 1:            \n",
    "                get_hazard_points['overlay_meters'] = pygeos.length(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            elif (pygeos.get_type_id(asset_geom) == 3) | (pygeos.get_type_id(asset_geom) == 6) :\n",
    "                get_hazard_points['overlay_m2'] = pygeos.area(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            \n",
    "            collect_all = []\n",
    "            for iter_,curve_ids in enumerate(curves[asset_type].columns):\n",
    "                if pygeos.get_type_id(asset_geom) == 1:                           \n",
    "                    collect_all.append([asset[0],curves[asset_type].columns[iter_],np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                      hazard_intensity,\n",
    "                                      fragility_values.T[iter_]))*get_hazard_points.overlay_meters*maxdam_asset[iter_])])\n",
    "                \n",
    "                elif (pygeos.get_type_id(asset_geom) == 3) | (pygeos.get_type_id(asset_geom) == 6) :\n",
    "                    collect_all.append([asset[0],curves[asset_type].columns[iter_],get_hazard_points.apply(lambda x: np.interp(x[return_period], \n",
    "                                                              hazard_intensity, \n",
    "                                                              fragility_values.T[iter_])*maxdam_asset[iter_]*x.overlay_m2,axis=1).sum()])     \n",
    "\n",
    "                else:\n",
    "                    collect_all.append([asset[0],curves[asset_type].columns[iter_],\n",
    "                                              np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,\n",
    "                                                                fragility_values.T[iter_]))*maxdam_asset[iter_])])\n",
    "            return collect_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1bf0e992-b4d1-46b0-a952-488ac8c5b2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Specific occupancy</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Type vulnerability data</th>\n",
       "      <th>Unit</th>\n",
       "      <th>MaxDam</th>\n",
       "      <th>LowerDam</th>\n",
       "      <th>UpperDam</th>\n",
       "      <th>UpperDam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>W4_5</th>\n",
       "      <td>Class 4, age 0</td>\n",
       "      <td>Salman and Li, 2016</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2800.665206</td>\n",
       "      <td>2100.498904</td>\n",
       "      <td>3500.831507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_6</th>\n",
       "      <td>Steel, age 0</td>\n",
       "      <td>Salman and Li, 2016</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>3019.333691</td>\n",
       "      <td>2264.500268</td>\n",
       "      <td>3774.167114</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_7</th>\n",
       "      <td>Class 4, age 20 + CPD</td>\n",
       "      <td>Salman and Li, 2016</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2800.665206</td>\n",
       "      <td>2100.498904</td>\n",
       "      <td>3500.831507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_8</th>\n",
       "      <td>Class 4, age 20 no CPD</td>\n",
       "      <td>Salman and Li, 2016</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2800.665206</td>\n",
       "      <td>2100.498904</td>\n",
       "      <td>3500.831507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_9</th>\n",
       "      <td>Steel, age 20</td>\n",
       "      <td>Salman and Li, 2016</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>3019.333691</td>\n",
       "      <td>2264.500268</td>\n",
       "      <td>3774.167114</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_10</th>\n",
       "      <td>Class 4, age 40 + CPD</td>\n",
       "      <td>Salman and Li, 2016</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2800.665206</td>\n",
       "      <td>2100.498904</td>\n",
       "      <td>3500.831507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_11</th>\n",
       "      <td>Class 4, age 40 no CPD</td>\n",
       "      <td>Salman and Li, 2016</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2800.665206</td>\n",
       "      <td>2100.498904</td>\n",
       "      <td>3500.831507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_12</th>\n",
       "      <td>Steel, age 40</td>\n",
       "      <td>Salman and Li, 2016</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>3019.333691</td>\n",
       "      <td>2264.500268</td>\n",
       "      <td>3774.167114</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_13</th>\n",
       "      <td>Class 4, age 60 + CPD</td>\n",
       "      <td>Salman and Li, 2016</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2800.665206</td>\n",
       "      <td>2100.498904</td>\n",
       "      <td>3500.831507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_14</th>\n",
       "      <td>Class 4, age 60 no CPD</td>\n",
       "      <td>Salman and Li, 2016</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2800.665206</td>\n",
       "      <td>2100.498904</td>\n",
       "      <td>3500.831507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_15</th>\n",
       "      <td>Steel, age 60</td>\n",
       "      <td>Salman and Li, 2016</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>3019.333691</td>\n",
       "      <td>2264.500268</td>\n",
       "      <td>3774.167114</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_16</th>\n",
       "      <td>Class 2, age 0</td>\n",
       "      <td>Yuan et al., 2018</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_17</th>\n",
       "      <td>Class 2, age 30</td>\n",
       "      <td>Yuan et al., 2018</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_18</th>\n",
       "      <td>Class 2, age 60</td>\n",
       "      <td>Yuan et al., 2018</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_19</th>\n",
       "      <td>Class 2, age 90</td>\n",
       "      <td>Yuan et al., 2018</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_20</th>\n",
       "      <td>Class 3, age 0</td>\n",
       "      <td>Yuan et al., 2018</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_21</th>\n",
       "      <td>Class 3, age 30</td>\n",
       "      <td>Yuan et al., 2018</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_22</th>\n",
       "      <td>Class 3, age 60</td>\n",
       "      <td>Yuan et al., 2018</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_23</th>\n",
       "      <td>Class 3, age 90</td>\n",
       "      <td>Yuan et al., 2018</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_24</th>\n",
       "      <td>Class 5, age 0, angle 0</td>\n",
       "      <td>Lee and Ham, 2021</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_25</th>\n",
       "      <td>Class 5, age 0, angle 10</td>\n",
       "      <td>Lee and Ham, 2021</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_26</th>\n",
       "      <td>Class 5, age 0, angle 20</td>\n",
       "      <td>Lee and Ham, 2021</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_27</th>\n",
       "      <td>Class 5, age 0, angle 30</td>\n",
       "      <td>Lee and Ham, 2021</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_28</th>\n",
       "      <td>Class 5, age 50, angle 0</td>\n",
       "      <td>Lee and Ham, 2021</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_29</th>\n",
       "      <td>Class 5, age 50, angle 10</td>\n",
       "      <td>Lee and Ham, 2021</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_30</th>\n",
       "      <td>Class 5, age 50, angle 20</td>\n",
       "      <td>Lee and Ham, 2021</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_31</th>\n",
       "      <td>Class 5, age 0, angle 30</td>\n",
       "      <td>Lee and Ham, 2021</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_48</th>\n",
       "      <td>Class 3</td>\n",
       "      <td>Darestani and Shafieezadeh, 2019</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_49</th>\n",
       "      <td>Class 4</td>\n",
       "      <td>Darestani and Shafieezadeh, 2019</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2800.665206</td>\n",
       "      <td>2100.498904</td>\n",
       "      <td>3500.831507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_50</th>\n",
       "      <td>Class 5</td>\n",
       "      <td>Darestani and Shafieezadeh, 2019</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_32</th>\n",
       "      <td>Class 3-power model-new</td>\n",
       "      <td>Shafieezadeh et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_33</th>\n",
       "      <td>Class 3-power model-25 yr</td>\n",
       "      <td>Shafieezadeh et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_34</th>\n",
       "      <td>Class 3-power model-50 yr</td>\n",
       "      <td>Shafieezadeh et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_35</th>\n",
       "      <td>Class 3-power model-75 yr</td>\n",
       "      <td>Shafieezadeh et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_36</th>\n",
       "      <td>Class 3-power model-100 yr</td>\n",
       "      <td>Shafieezadeh et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_37</th>\n",
       "      <td>Class 5-power model-new</td>\n",
       "      <td>Shafieezadeh et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_38</th>\n",
       "      <td>Class 5-power model-25 yr</td>\n",
       "      <td>Shafieezadeh et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_39</th>\n",
       "      <td>Class 5-power model-50 yr</td>\n",
       "      <td>Shafieezadeh et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_40</th>\n",
       "      <td>Class 5-power model-75 yr</td>\n",
       "      <td>Shafieezadeh et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_41</th>\n",
       "      <td>Class 5-power model-100 yr</td>\n",
       "      <td>Shafieezadeh et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_42</th>\n",
       "      <td>Southern Pine, 12.47kV</td>\n",
       "      <td>Han et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_43</th>\n",
       "      <td>Southern Pine, 34.5kV</td>\n",
       "      <td>Han et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_44</th>\n",
       "      <td>Douglas Fir, 12.47kV</td>\n",
       "      <td>Han et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_45</th>\n",
       "      <td>Douglas Fir, 34.5kV</td>\n",
       "      <td>Han et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_46</th>\n",
       "      <td>Western Red Cedar, 12.47kV</td>\n",
       "      <td>Han et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_47</th>\n",
       "      <td>Western Red Cedar, 34.5kV</td>\n",
       "      <td>Han et al., 2014</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_1</th>\n",
       "      <td>Design case 1 (class 5)</td>\n",
       "      <td>Bjarnadottir et al., 2013</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_2</th>\n",
       "      <td>Design case 2 (class 2)</td>\n",
       "      <td>Bjarnadottir et al., 2013</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_3</th>\n",
       "      <td>Design case 3 (class 3)</td>\n",
       "      <td>Bjarnadottir et al., 2013</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2837.985192</td>\n",
       "      <td>2128.488894</td>\n",
       "      <td>3547.48149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4_4</th>\n",
       "      <td>Design case 4 (class 5)</td>\n",
       "      <td>Bjarnadottir et al., 2013</td>\n",
       "      <td>curve</td>\n",
       "      <td>euro/facility</td>\n",
       "      <td>2778.847368</td>\n",
       "      <td>2084.135526</td>\n",
       "      <td>3473.55921</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Specific occupancy                         Reference  \\\n",
       "Code                                                                  \n",
       "W4_5               Class 4, age 0               Salman and Li, 2016   \n",
       "W4_6                 Steel, age 0               Salman and Li, 2016   \n",
       "W4_7        Class 4, age 20 + CPD               Salman and Li, 2016   \n",
       "W4_8       Class 4, age 20 no CPD               Salman and Li, 2016   \n",
       "W4_9                Steel, age 20               Salman and Li, 2016   \n",
       "W4_10       Class 4, age 40 + CPD               Salman and Li, 2016   \n",
       "W4_11      Class 4, age 40 no CPD               Salman and Li, 2016   \n",
       "W4_12               Steel, age 40               Salman and Li, 2016   \n",
       "W4_13       Class 4, age 60 + CPD               Salman and Li, 2016   \n",
       "W4_14      Class 4, age 60 no CPD               Salman and Li, 2016   \n",
       "W4_15               Steel, age 60               Salman and Li, 2016   \n",
       "W4_16              Class 2, age 0                 Yuan et al., 2018   \n",
       "W4_17             Class 2, age 30                 Yuan et al., 2018   \n",
       "W4_18             Class 2, age 60                 Yuan et al., 2018   \n",
       "W4_19             Class 2, age 90                 Yuan et al., 2018   \n",
       "W4_20              Class 3, age 0                 Yuan et al., 2018   \n",
       "W4_21             Class 3, age 30                 Yuan et al., 2018   \n",
       "W4_22             Class 3, age 60                 Yuan et al., 2018   \n",
       "W4_23             Class 3, age 90                 Yuan et al., 2018   \n",
       "W4_24     Class 5, age 0, angle 0                 Lee and Ham, 2021   \n",
       "W4_25    Class 5, age 0, angle 10                 Lee and Ham, 2021   \n",
       "W4_26    Class 5, age 0, angle 20                 Lee and Ham, 2021   \n",
       "W4_27    Class 5, age 0, angle 30                 Lee and Ham, 2021   \n",
       "W4_28    Class 5, age 50, angle 0                 Lee and Ham, 2021   \n",
       "W4_29   Class 5, age 50, angle 10                 Lee and Ham, 2021   \n",
       "W4_30   Class 5, age 50, angle 20                 Lee and Ham, 2021   \n",
       "W4_31    Class 5, age 0, angle 30                 Lee and Ham, 2021   \n",
       "W4_48                     Class 3  Darestani and Shafieezadeh, 2019   \n",
       "W4_49                     Class 4  Darestani and Shafieezadeh, 2019   \n",
       "W4_50                     Class 5  Darestani and Shafieezadeh, 2019   \n",
       "W4_32     Class 3-power model-new         Shafieezadeh et al., 2014   \n",
       "W4_33   Class 3-power model-25 yr         Shafieezadeh et al., 2014   \n",
       "W4_34   Class 3-power model-50 yr         Shafieezadeh et al., 2014   \n",
       "W4_35   Class 3-power model-75 yr         Shafieezadeh et al., 2014   \n",
       "W4_36  Class 3-power model-100 yr         Shafieezadeh et al., 2014   \n",
       "W4_37     Class 5-power model-new         Shafieezadeh et al., 2014   \n",
       "W4_38   Class 5-power model-25 yr         Shafieezadeh et al., 2014   \n",
       "W4_39   Class 5-power model-50 yr         Shafieezadeh et al., 2014   \n",
       "W4_40   Class 5-power model-75 yr         Shafieezadeh et al., 2014   \n",
       "W4_41  Class 5-power model-100 yr         Shafieezadeh et al., 2014   \n",
       "W4_42      Southern Pine, 12.47kV                  Han et al., 2014   \n",
       "W4_43       Southern Pine, 34.5kV                  Han et al., 2014   \n",
       "W4_44        Douglas Fir, 12.47kV                  Han et al., 2014   \n",
       "W4_45         Douglas Fir, 34.5kV                  Han et al., 2014   \n",
       "W4_46  Western Red Cedar, 12.47kV                  Han et al., 2014   \n",
       "W4_47   Western Red Cedar, 34.5kV                  Han et al., 2014   \n",
       "W4_1      Design case 1 (class 5)         Bjarnadottir et al., 2013   \n",
       "W4_2      Design case 2 (class 2)         Bjarnadottir et al., 2013   \n",
       "W4_3      Design case 3 (class 3)         Bjarnadottir et al., 2013   \n",
       "W4_4      Design case 4 (class 5)         Bjarnadottir et al., 2013   \n",
       "\n",
       "      Type vulnerability data           Unit       MaxDam     LowerDam  \\\n",
       "Code                                                                     \n",
       "W4_5                    curve  euro/facility  2800.665206  2100.498904   \n",
       "W4_6                    curve  euro/facility  3019.333691  2264.500268   \n",
       "W4_7                    curve  euro/facility  2800.665206  2100.498904   \n",
       "W4_8                    curve  euro/facility  2800.665206  2100.498904   \n",
       "W4_9                    curve  euro/facility  3019.333691  2264.500268   \n",
       "W4_10                   curve  euro/facility  2800.665206  2100.498904   \n",
       "W4_11                   curve  euro/facility  2800.665206  2100.498904   \n",
       "W4_12                   curve  euro/facility  3019.333691  2264.500268   \n",
       "W4_13                   curve  euro/facility  2800.665206  2100.498904   \n",
       "W4_14                   curve  euro/facility  2800.665206  2100.498904   \n",
       "W4_15                   curve  euro/facility  3019.333691  2264.500268   \n",
       "W4_16                   curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_17                   curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_18                   curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_19                   curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_20                   curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_21                   curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_22                   curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_23                   curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_24                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_25                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_26                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_27                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_28                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_29                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_30                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_31                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_48                   curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_49                   curve  euro/facility  2800.665206  2100.498904   \n",
       "W4_50                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_32                   curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_33                   curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_34                   curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_35                   curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_36                   curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_37                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_38                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_39                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_40                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_41                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_42                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_43                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_44                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_45                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_46                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_47                   curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_1                    curve  euro/facility  2778.847368  2084.135526   \n",
       "W4_2                    curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_3                    curve  euro/facility  2837.985192  2128.488894   \n",
       "W4_4                    curve  euro/facility  2778.847368  2084.135526   \n",
       "\n",
       "          UpperDam UpperDam  \n",
       "Code                         \n",
       "W4_5   3500.831507      NaN  \n",
       "W4_6   3774.167114      NaN  \n",
       "W4_7   3500.831507      NaN  \n",
       "W4_8   3500.831507      NaN  \n",
       "W4_9   3774.167114      NaN  \n",
       "W4_10  3500.831507      NaN  \n",
       "W4_11  3500.831507      NaN  \n",
       "W4_12  3774.167114      NaN  \n",
       "W4_13  3500.831507      NaN  \n",
       "W4_14  3500.831507      NaN  \n",
       "W4_15  3774.167114      NaN  \n",
       "W4_16   3547.48149      NaN  \n",
       "W4_17   3547.48149      NaN  \n",
       "W4_18   3547.48149      NaN  \n",
       "W4_19   3547.48149      NaN  \n",
       "W4_20   3547.48149      NaN  \n",
       "W4_21   3547.48149      NaN  \n",
       "W4_22   3547.48149      NaN  \n",
       "W4_23   3547.48149      NaN  \n",
       "W4_24   3473.55921      NaN  \n",
       "W4_25   3473.55921      NaN  \n",
       "W4_26   3473.55921      NaN  \n",
       "W4_27   3473.55921      NaN  \n",
       "W4_28   3473.55921      NaN  \n",
       "W4_29   3473.55921      NaN  \n",
       "W4_30   3473.55921      NaN  \n",
       "W4_31   3473.55921      NaN  \n",
       "W4_48   3547.48149      NaN  \n",
       "W4_49  3500.831507      NaN  \n",
       "W4_50   3473.55921      NaN  \n",
       "W4_32   3547.48149      NaN  \n",
       "W4_33   3547.48149      NaN  \n",
       "W4_34   3547.48149      NaN  \n",
       "W4_35   3547.48149      NaN  \n",
       "W4_36   3547.48149      NaN  \n",
       "W4_37   3473.55921      NaN  \n",
       "W4_38   3473.55921      NaN  \n",
       "W4_39   3473.55921      NaN  \n",
       "W4_40   3473.55921      NaN  \n",
       "W4_41   3473.55921      NaN  \n",
       "W4_42   3473.55921      NaN  \n",
       "W4_43   3473.55921      NaN  \n",
       "W4_44   3473.55921      NaN  \n",
       "W4_45   3473.55921      NaN  \n",
       "W4_46   3473.55921      NaN  \n",
       "W4_47   3473.55921      NaN  \n",
       "W4_1    3473.55921      NaN  \n",
       "W4_2    3547.48149      NaN  \n",
       "W4_3    3547.48149      NaN  \n",
       "W4_4    3473.55921      NaN  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_curves_maxdam(vul_curve_path,'tc')[1].loc['power_pole']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b55262-ea71-4f39-8bb3-ced4f1917af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9896d9f-4592-4552-8876-c1340f3e09f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_storm_data(climate_model,basin,bbox,ne_crs):\n",
    "    \n",
    "    with xr.open_dataset(os.path.join(tc_path,'STORM_FIXED_RETURN_PERIODS{}_{}.nc'.format(climate_model,basin))) as ds:\n",
    "        \n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        ds = ds.rio.clip_box(minx=bbox[0],miny=bbox[1],maxx=bbox[2],maxy=bbox[3])\n",
    "        \n",
    "        # get the mean values\n",
    "        df_ds = ds['mean'].to_dataframe().unstack(level=2).reset_index()\n",
    "\n",
    "        # create geometry values and drop lat lon columns\n",
    "        df_ds['geometry'] = [pygeos.points(x) for x in list(zip(df_ds['lon'],df_ds['lat']))]\n",
    "        df_ds = df_ds.drop(['lat','lon'],axis=1,level=0)\n",
    "        \n",
    "        # interpolate wind speeds of 1, 2, and 5-yr return period\n",
    "        ## rename columns to return periods (must be integer for interpolating)\n",
    "        df_ds_geometry = pd.DataFrame()\n",
    "        df_ds_geometry['geometry'] = df_ds['geometry']\n",
    "        df_ds = df_ds.drop(['geometry'],axis=1,level=0)\n",
    "        df_ds = df_ds['mean']\n",
    "        df_ds.columns = [int(x) for x in ds['mean']['rp']]\n",
    "        df_ds[1] = np.nan\n",
    "        df_ds[2] = np.nan\n",
    "        df_ds[5] = np.nan\n",
    "        df_ds[25] = np.nan\n",
    "        df_ds[250] = np.nan\n",
    "        df_ds = df_ds.reindex(sorted(df_ds.columns), axis=1)\n",
    "        df_ds = df_ds.interpolate(method='linear',axis=1,limit_direction='both')\n",
    "        df_ds['geometry'] = df_ds_geometry['geometry']\n",
    "        #df_ds = df_ds[['1','2','5','10','25','50','100','250','500','1000','geometry']]\n",
    "        df_ds = df_ds[[1,2,5,10,25,50,100,250,500,1000,'geometry']]\n",
    "        \n",
    "        #rename columns to return periods\n",
    "        #return_periods = ['1_{}{}'.format(int(x),climate_model) for x in ds['rp']]\n",
    "        df_ds.columns = ['1_{}{}'.format(int(x),climate_model) for x in [1,2,5,10,25,50,100,250,500,1000]] +['geometry']     \n",
    "        df_ds['geometry'] = pygeos.buffer(df_ds.geometry,radius=0.1/2,cap_style='square').values\n",
    "        df_ds['geometry'] = reproject(df_ds)\n",
    "            \n",
    "        # drop all non values to reduce size\n",
    "        #df_ds = df_ds.loc[~df_ds['1_10000{}'.format(climate_model)].isna()].reset_index(drop=True)\n",
    "        df_ds = df_ds.fillna(0)\n",
    "        #df_ds = df_ds[['1','2','5','10','25','50','100','250','500','1000']]\n",
    "        #df_ds = df_ds[['1_{}{}'.format(int(x),climate_model) for x in list(df_ds.columns.get_level_values(0))[:-1]]+['geometry']]\n",
    "\n",
    "    return df_ds\n",
    "\n",
    "def open_storm_data(country_code):\n",
    "    climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "    df_ds = {}\n",
    "    \n",
    "    country_basin = {\n",
    "    \"BRN\": [\"WP\"],\n",
    "    \"KHM\": [\"WP\"],\n",
    "    \"CHN\": [\"WP\", \"NI\"],\n",
    "    \"IDN\": [\"SI\", \"SP\", \"NI\", \"WP\"],\n",
    "    \"JPN\": [\"WP\"],\n",
    "    \"LAO\": [\"WP\"],\n",
    "    \"MYS\": [\"WP\", \"NI\"],\n",
    "    \"MNG\": [\"WP\", \"NI\"],\n",
    "    \"MMR\": [\"NI\", \"WP\"],\n",
    "    \"PRK\": [\"WP\"],\n",
    "    \"PHL\": [\"WP\"],\n",
    "    \"SGP\": [\"WP\"],\n",
    "    \"KOR\": [\"WP\"],\n",
    "    \"TWN\": [\"WP\"],\n",
    "    \"THA\": [\"WP\", \"NI\"],\n",
    "    \"VNM\": [\"WP\"] }\n",
    "    \n",
    "    # load country geometry file and create geometry to clip\n",
    "    ne_countries = gpd.read_file('C:\\\\Data\\\\natural_earth\\\\ne_10m_admin_0_countries.shp') \n",
    "    ne_crs = ne_countries.crs\n",
    "    bbox = ne_countries.loc[ne_countries['ISO_A3']==country_code].geometry.buffer(1).values[0].bounds\n",
    "\n",
    "    for climate_model in climate_models:\n",
    "        concat_prep = []\n",
    "        #combine STORM data from different basins\n",
    "        if \"WP\" in country_basin[country_code]:\n",
    "            WP = load_storm_data(climate_model,'WP',bbox,ne_crs)\n",
    "            concat_prep.append(WP)\n",
    "        if \"SP\" in country_basin[country_code]:\n",
    "            SP = load_storm_data(climate_model,'SP',bbox,ne_crs)\n",
    "            concat_prep.append(SP)\n",
    "        if \"NI\" in country_basin[country_code]:            \n",
    "            NI = load_storm_data(climate_model,'NI',bbox,ne_crs)\n",
    "            concat_prep.append(NI)            \n",
    "        if \"SI\" in country_basin[country_code]:       \n",
    "            SI = load_storm_data(climate_model,'SI',bbox,ne_crs)\n",
    "            concat_prep.append(SI)            \n",
    "                   \n",
    "        df_ds_cl = pd.concat(concat_prep,keys=country_basin[country_code])#,sp,ni,si,'sp','ni','si'])\n",
    "\n",
    "        df_ds_cl = df_ds_cl.reset_index(drop=True)\n",
    "        \n",
    "        df_ds[climate_model] = df_ds_cl\n",
    "    \n",
    "    return df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "7059f5f8-fff4-4827-a187-bebcf957230d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.73 s\n",
      "Wall time: 1.72 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_1</th>\n",
       "      <th>1_2</th>\n",
       "      <th>1_5</th>\n",
       "      <th>1_10</th>\n",
       "      <th>1_25</th>\n",
       "      <th>1_50</th>\n",
       "      <th>1_100</th>\n",
       "      <th>1_250</th>\n",
       "      <th>1_500</th>\n",
       "      <th>1_1000</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31.484970</td>\n",
       "      <td>31.484970</td>\n",
       "      <td>31.484970</td>\n",
       "      <td>31.484970</td>\n",
       "      <td>34.882271</td>\n",
       "      <td>37.150791</td>\n",
       "      <td>38.786932</td>\n",
       "      <td>41.029938</td>\n",
       "      <td>42.933264</td>\n",
       "      <td>43.897242</td>\n",
       "      <td>POLYGON ((13057776.27 2391878.588, 13057776.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.561684</td>\n",
       "      <td>31.561684</td>\n",
       "      <td>31.561684</td>\n",
       "      <td>31.561684</td>\n",
       "      <td>34.941517</td>\n",
       "      <td>37.417290</td>\n",
       "      <td>38.952814</td>\n",
       "      <td>40.987155</td>\n",
       "      <td>42.433489</td>\n",
       "      <td>43.846372</td>\n",
       "      <td>POLYGON ((13068908.219 2391878.588, 13068908.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.559660</td>\n",
       "      <td>31.559660</td>\n",
       "      <td>31.559660</td>\n",
       "      <td>31.559660</td>\n",
       "      <td>34.989208</td>\n",
       "      <td>37.441990</td>\n",
       "      <td>39.295692</td>\n",
       "      <td>41.087890</td>\n",
       "      <td>42.636647</td>\n",
       "      <td>43.937314</td>\n",
       "      <td>POLYGON ((13080040.168 2391878.588, 13080040.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.649051</td>\n",
       "      <td>31.649051</td>\n",
       "      <td>31.649051</td>\n",
       "      <td>31.649051</td>\n",
       "      <td>35.093386</td>\n",
       "      <td>37.464786</td>\n",
       "      <td>39.427878</td>\n",
       "      <td>41.296611</td>\n",
       "      <td>42.939086</td>\n",
       "      <td>44.289626</td>\n",
       "      <td>POLYGON ((13091172.117 2391878.588, 13091172.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.749391</td>\n",
       "      <td>31.749391</td>\n",
       "      <td>31.749391</td>\n",
       "      <td>31.749391</td>\n",
       "      <td>35.266336</td>\n",
       "      <td>37.634156</td>\n",
       "      <td>39.579647</td>\n",
       "      <td>41.514516</td>\n",
       "      <td>42.741313</td>\n",
       "      <td>44.298240</td>\n",
       "      <td>POLYGON ((13102304.066 2391878.588, 13102304.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3181</th>\n",
       "      <td>35.069860</td>\n",
       "      <td>35.069860</td>\n",
       "      <td>35.069860</td>\n",
       "      <td>35.069860</td>\n",
       "      <td>39.243768</td>\n",
       "      <td>42.045388</td>\n",
       "      <td>44.128518</td>\n",
       "      <td>46.371252</td>\n",
       "      <td>48.097373</td>\n",
       "      <td>49.527067</td>\n",
       "      <td>POLYGON ((13658901.52 3036284.923, 13658901.52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3182</th>\n",
       "      <td>35.159120</td>\n",
       "      <td>35.159120</td>\n",
       "      <td>35.159120</td>\n",
       "      <td>35.159120</td>\n",
       "      <td>39.261418</td>\n",
       "      <td>41.821312</td>\n",
       "      <td>44.056902</td>\n",
       "      <td>46.617187</td>\n",
       "      <td>48.180678</td>\n",
       "      <td>49.415934</td>\n",
       "      <td>POLYGON ((13670033.469 3036284.923, 13670033.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3183</th>\n",
       "      <td>35.238116</td>\n",
       "      <td>35.238116</td>\n",
       "      <td>35.238116</td>\n",
       "      <td>35.238116</td>\n",
       "      <td>39.250911</td>\n",
       "      <td>41.894078</td>\n",
       "      <td>44.101615</td>\n",
       "      <td>46.562302</td>\n",
       "      <td>47.930412</td>\n",
       "      <td>48.967779</td>\n",
       "      <td>POLYGON ((13681165.418 3036284.923, 13681165.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3184</th>\n",
       "      <td>35.309186</td>\n",
       "      <td>35.309186</td>\n",
       "      <td>35.309186</td>\n",
       "      <td>35.309186</td>\n",
       "      <td>39.248515</td>\n",
       "      <td>41.935010</td>\n",
       "      <td>44.177309</td>\n",
       "      <td>46.750433</td>\n",
       "      <td>47.926717</td>\n",
       "      <td>48.968906</td>\n",
       "      <td>POLYGON ((13692297.368 3036284.923, 13692297.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3185</th>\n",
       "      <td>35.334279</td>\n",
       "      <td>35.334279</td>\n",
       "      <td>35.334279</td>\n",
       "      <td>35.334279</td>\n",
       "      <td>39.291144</td>\n",
       "      <td>41.815739</td>\n",
       "      <td>44.186469</td>\n",
       "      <td>46.671538</td>\n",
       "      <td>47.922262</td>\n",
       "      <td>49.403801</td>\n",
       "      <td>POLYGON ((13703429.317 3036284.923, 13703429.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3186 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1_1        1_2        1_5       1_10       1_25       1_50  \\\n",
       "0     31.484970  31.484970  31.484970  31.484970  34.882271  37.150791   \n",
       "1     31.561684  31.561684  31.561684  31.561684  34.941517  37.417290   \n",
       "2     31.559660  31.559660  31.559660  31.559660  34.989208  37.441990   \n",
       "3     31.649051  31.649051  31.649051  31.649051  35.093386  37.464786   \n",
       "4     31.749391  31.749391  31.749391  31.749391  35.266336  37.634156   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "3181  35.069860  35.069860  35.069860  35.069860  39.243768  42.045388   \n",
       "3182  35.159120  35.159120  35.159120  35.159120  39.261418  41.821312   \n",
       "3183  35.238116  35.238116  35.238116  35.238116  39.250911  41.894078   \n",
       "3184  35.309186  35.309186  35.309186  35.309186  39.248515  41.935010   \n",
       "3185  35.334279  35.334279  35.334279  35.334279  39.291144  41.815739   \n",
       "\n",
       "          1_100      1_250      1_500     1_1000  \\\n",
       "0     38.786932  41.029938  42.933264  43.897242   \n",
       "1     38.952814  40.987155  42.433489  43.846372   \n",
       "2     39.295692  41.087890  42.636647  43.937314   \n",
       "3     39.427878  41.296611  42.939086  44.289626   \n",
       "4     39.579647  41.514516  42.741313  44.298240   \n",
       "...         ...        ...        ...        ...   \n",
       "3181  44.128518  46.371252  48.097373  49.527067   \n",
       "3182  44.056902  46.617187  48.180678  49.415934   \n",
       "3183  44.101615  46.562302  47.930412  48.967779   \n",
       "3184  44.177309  46.750433  47.926717  48.968906   \n",
       "3185  44.186469  46.671538  47.922262  49.403801   \n",
       "\n",
       "                                               geometry  \n",
       "0     POLYGON ((13057776.27 2391878.588, 13057776.27...  \n",
       "1     POLYGON ((13068908.219 2391878.588, 13068908.2...  \n",
       "2     POLYGON ((13080040.168 2391878.588, 13080040.1...  \n",
       "3     POLYGON ((13091172.117 2391878.588, 13091172.1...  \n",
       "4     POLYGON ((13102304.066 2391878.588, 13102304.0...  \n",
       "...                                                 ...  \n",
       "3181  POLYGON ((13658901.52 3036284.923, 13658901.52...  \n",
       "3182  POLYGON ((13670033.469 3036284.923, 13670033.4...  \n",
       "3183  POLYGON ((13681165.418 3036284.923, 13681165.4...  \n",
       "3184  POLYGON ((13692297.368 3036284.923, 13692297.3...  \n",
       "3185  POLYGON ((13703429.317 3036284.923, 13703429.3...  \n",
       "\n",
       "[3186 rows x 11 columns]"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "open_storm_data('TWN')['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c7ff6dd-a3f2-4e2a-becd-e5cffa38edde",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clip_flood_data(country_code):\n",
    "\n",
    "    # load country geometry file and create geometry to clip\n",
    "    ne_countries = gpd.read_file('C:\\\\Data\\\\natural_earth\\\\ne_10m_admin_0_countries.shp') \n",
    "    geometry = ne_countries.loc[ne_countries['ISO_A3']==country_code].geometry.values[0]\n",
    "    geoms = [mapping(geometry)]\n",
    "    \n",
    "    #climate_model: historical, rcp4p5, rcp8p5; time_period: hist, 2030, 2050, 2080\n",
    "    rps = ['0001','0002','0005','0010','0025','0050','0100','0250','0500','1000']\n",
    "    climate_models = ['historical','rcp8p5']\n",
    "    \n",
    "    \n",
    "    for rp in rps:\n",
    "        #global input_file\n",
    "        for climate_model in climate_models:\n",
    "            if climate_model=='historical':\n",
    "                #f rps=='0001':\n",
    "                    input_file = os.path.join(fl_path,'global',\n",
    "                                              'inuncoast_{}_nosub_hist_rp{}_0.tif'.format(climate_model,rp)) \n",
    "                #elif rps==['0002','0005','0010','0025','0050','0100','0250','0500','1000']:\n",
    "                #    input_file = os.path.join(fl_path,'global',\n",
    "                #                              'inuncoast_{}_nosub_hist_rp{}_0.tif'.format(climate_model,rp)) \n",
    "            elif climate_model=='rcp8p5':\n",
    "                #f rps=='0001':\n",
    "                    input_file = os.path.join(fl_path,'global',\n",
    "                                              'inuncoast_{}_nosub_2030_rp{}_0.tif'.format(climate_model,rp))\n",
    "                #elif rps==['0002','0005','0010','0025','0050','0100','0250','0500','1000']:\n",
    "                #    input_file = os.path.join(fl_path,'global',\n",
    "                #                              'inuncoast_{}_nosub_2030_rp{}_0.tif'.format(climate_model,rp))\n",
    "            \n",
    "            # load raster file and save clipped version\n",
    "            with rasterio.open(input_file) as src:\n",
    "                out_image, out_transform = mask(src, geoms, crop=True)\n",
    "                out_meta = src.meta\n",
    "\n",
    "                out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "\n",
    "                file_path = os.path.join(fl_path,'country','_'.join([country_code]+input_file.split('_')[3:]))\n",
    "\n",
    "                with rasterio.open(file_path, \"w\", **out_meta) as dest:\n",
    "                    dest.write(out_image)\n",
    "\n",
    "def load_flood_data(country_code,scenario_type):\n",
    "    files = [x for x in os.listdir(os.path.join(fl_path,'country'))  if country_code in x ]\n",
    "    \n",
    "    rps = ['0001','0002','0005','0010','0025','0050','0100','0250','0500','1000']\n",
    "    collect_df_ds = []\n",
    "    \n",
    "    if scenario_type=='historical':\n",
    "        print('Loading historical coastal flood data ...')\n",
    "        for rp in rps:\n",
    "            #for file in files:\n",
    "            file_path = os.path.join(fl_path,'country','{}_{}_nosub_hist_rp{}_0.tif'.format(country_code,scenario_type,rp))\n",
    "            with xr.open_dataset(file_path) as ds: #, engine=\"rasterio\"\n",
    "                df_ds = ds.to_dataframe().reset_index()\n",
    "                df_ds['geometry'] = pygeos.points(df_ds.x,y=df_ds.y)\n",
    "                df_ds = df_ds.rename(columns={'band_data': 'rp'+rp}) #rename to return period\n",
    "                df_ds['rp'+rp] = (df_ds['rp'+rp]*100)         \n",
    "                df_ds = df_ds.drop(['band','x', 'y','spatial_ref'], axis=1)\n",
    "                df_ds = df_ds.dropna()\n",
    "                df_ds = df_ds.reset_index(drop=True)\n",
    "                df_ds.geometry= pygeos.buffer(df_ds.geometry,radius=100/2,cap_style='square').values  #?????????????????????????\n",
    "                df_ds['geometry'] = reproject(df_ds)\n",
    "                collect_df_ds.append(df_ds)\n",
    "\n",
    "        df_all = collect_df_ds[0].merge(collect_df_ds[1]).merge(collect_df_ds[2]).merge(collect_df_ds[3]).merge(collect_df_ds[4])\\\n",
    "                 .merge(collect_df_ds[5]).merge(collect_df_ds[6]).merge(collect_df_ds[7]).merge(collect_df_ds[8]).merge(collect_df_ds[9])\n",
    "        df_all = df_all.loc[df_all['rp1000']>0].reset_index(drop=True)\n",
    "\n",
    "    elif scenario_type=='rcp8p5':\n",
    "        print('Loading future coastal flood data ...')\n",
    "        for rp in rps:\n",
    "            #for file in files:\n",
    "            file_path = os.path.join(fl_path,'country','{}_{}_nosub_2030_rp{}_0.tif'.format(country_code,scenario_type,rp))\n",
    "            with xr.open_dataset(file_path) as ds: #, engine=\"rasterio\"\n",
    "                df_ds = ds.to_dataframe().reset_index()\n",
    "                df_ds['geometry'] = pygeos.points(df_ds.x,y=df_ds.y)\n",
    "                df_ds = df_ds.rename(columns={'band_data': 'rp'+rp}) #rename to return period\n",
    "                df_ds['rp'+rp] = (df_ds['rp'+rp]*100)\n",
    "                df_ds = df_ds.drop(['band','x', 'y','spatial_ref'], axis=1)\n",
    "                df_ds = df_ds.dropna()\n",
    "                df_ds = df_ds.reset_index(drop=True)\n",
    "                df_ds.geometry= pygeos.buffer(df_ds.geometry,radius=100/2,cap_style='square').values\n",
    "                df_ds['geometry'] = reproject(df_ds)\n",
    "                collect_df_ds.append(df_ds)\n",
    "\n",
    "        df_all = collect_df_ds[0].merge(collect_df_ds[1]).merge(collect_df_ds[2]).merge(collect_df_ds[3]).merge(collect_df_ds[4])\\\n",
    "                 .merge(collect_df_ds[5]).merge(collect_df_ds[6]).merge(collect_df_ds[7]).merge(collect_df_ds[8]).merge(collect_df_ds[9])\n",
    "\n",
    "        df_all = df_all.loc[df_all['rp1000']>0].reset_index(drop=True)\n",
    "    return df_all\n",
    "\n",
    "def open_flood_data(country_code):\n",
    "    scenario_types = ['historical','rcp8p5']\n",
    "    df_ds = {}\n",
    "    for scenario_type in scenario_types:\n",
    "        #hist = load_flood_data(country_code,'historical')\n",
    "        #rcp8p5 = load_flood_data(country_code,'rcp8p5')\n",
    "        #df_ds_sc = pd.concat([hist,rcp8p5],keys=['historical','rcp8p5'])\n",
    "        df_ds_sc = load_flood_data(country_code,scenario_type)\n",
    "\n",
    "        df_ds[scenario_type] = df_ds_sc\n",
    "    \n",
    "    return df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebbb408-29e0-4128-ad1b-2a7eae99b0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5ac9f48-e434-4893-8c04-07e663921d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n",
      "CPU times: total: 17.9 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "twn_flood = open_flood_data('TWN')\n",
    "#print(type(lao_flood))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3de688-606c-4dd2-abe3-f37015c7442e",
   "metadata": {},
   "source": [
    "# OSM data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934161b6-5f01-4315-87b3-9e4a7bd70876",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_osm_infrastructure(country_code,osm_data_path):\n",
    "\n",
    "    # lines\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    power_lines_country = power_polyline(osm_path)\n",
    "    power_lines_country['geometry'] = reproject(power_lines_country)\n",
    "    power_lines_country = buffer_assets(power_lines_country.loc[power_lines_country.asset.isin(\n",
    "        ['cable','minor_cable','line','minor_line'])],buffer_size=100).reset_index(drop=True)\n",
    "    \n",
    "    # polygons\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    power_poly_country = electricity(osm_path)\n",
    "    power_poly_country['geometry'] = reproject(power_poly_country)\n",
    "    \n",
    "    # points\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    power_points_country = power_point(osm_path)\n",
    "    power_points_country['geometry'] = reproject(power_points_country)\n",
    "    power_points_country = buffer_assets(power_points_country.loc[power_points_country.asset.isin(\n",
    "        ['power_tower','power_pole'])],buffer_size=100).reset_index(drop=True)\n",
    "    #print(power_points_country)\n",
    "    #print(type(power_points_country))\n",
    "\n",
    "    return power_lines_country,power_poly_country,power_points_country\n",
    "\n",
    "\n",
    "#print(type(osm_power_infra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b16fbb7a-d740-4b39-bfdd-127321fb4cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|██████████████████████████████████████████████████████████████████████| 412/412 [00:01<00:00, 258.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|█████████████████████████████████████████████████████████████████████████| 14/14 [00:04<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|████████████████████████████████████████████████████████████████| 42729/42729 [00:02<00:00, 16033.53it/s]\n"
     ]
    }
   ],
   "source": [
    "osm_power_infra = extract_osm_infrastructure('LAO',osm_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "abb7a943-1950-445c-8328-f6db95b7f163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_damage_osm(country_code,osm_power_infra,hazard_type):\n",
    "    \n",
    "    # load curves and maxdam\n",
    "    curves,maxdam = load_curves_maxdam(vul_curve_path,hazard_type)\n",
    "    \n",
    "    # read infrastructure data:\n",
    "    power_lines,power_poly,power_points = osm_power_infra\n",
    "    #print(type(power_lines))\n",
    "    #print(type(osm_power_infra))\n",
    "    \n",
    "    if hazard_type=='tc':\n",
    "        # read wind data\n",
    "        climate_models = ['']#,'_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "        df_ds = open_storm_data(country_code)\n",
    "        \n",
    "        # remove assets that will not have any damage\n",
    "        power_lines = power_lines.loc[power_lines.asset != 'cable'].reset_index(drop=True)\n",
    "        power_poly = power_poly.loc[power_poly.asset != 'plant'].reset_index(drop=True)\n",
    "\n",
    "    elif hazard_type=='fl':\n",
    "        # read flood data\n",
    "        climate_models = ['historical']#,'rcp8p5']\n",
    "        df_ds = open_flood_data(country_code) \n",
    "        \n",
    "    #calculate damaged lines in loop by climate_model\n",
    "    damaged_lines = {}\n",
    "    for climate_model in climate_models:\n",
    "        \n",
    "        if hazard_type == 'tc':\n",
    "            return_periods = ['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),'1_10{}'.format(climate_model),\n",
    "                              '1_25{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),\n",
    "                              '1_250{}'.format(climate_model),'1_500{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "        elif hazard_type == 'fl':\n",
    "            return_periods = ['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000']  \n",
    "\n",
    "        overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],power_lines).T,\n",
    "                                     columns=['asset','hazard_point'])\n",
    "        collect_line_damages = []\n",
    "        for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                          desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "            for return_period in return_periods:\n",
    "                collect_line_damages.append([return_period,get_damage_per_asset_per_rp(asset,\n",
    "                                                                                       df_ds[climate_model],\n",
    "                                                                                       power_lines,\n",
    "                                                                                       curves,\n",
    "                                                                                       maxdam,\n",
    "                                                                                       return_period,\n",
    "                                                                                       country_code)])\n",
    "\n",
    "        get_asset_type_line = dict(zip(power_lines.index,power_lines.asset))\n",
    "\n",
    "        if hazard_type == 'tc':\n",
    "            results = pd.concat([pd.DataFrame(line[1],columns=pd.MultiIndex.from_product([[line[0]],\n",
    "                                                                                ['asset','curve','damage']])).stack(level=0) \n",
    "                       for line in collect_line_damages]).reset_index(level=1).reset_index(drop=True)\n",
    "        elif hazard_type == 'fl':\n",
    "            results = pd.concat([pd.DataFrame([line[1]],columns=pd.MultiIndex.from_product([[line[0]],\n",
    "                                                                                        ['asset','curve','damage']])).stack(level=0) \n",
    "                               for line in collect_line_damages]).reset_index(level=1).reset_index(drop=True) \n",
    "            \n",
    "        results.columns = ['rp','asset','curve','damage']\n",
    "        results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "        damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum()['damage'].reset_index()\n",
    "\n",
    "    # calculate damaged polygons in loop by country_code and climate_model\n",
    "    damaged_poly = {}\n",
    "    for climate_model in climate_models:\n",
    "        if hazard_type == 'tc':\n",
    "            return_periods = ['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),'1_10{}'.format(climate_model),\n",
    "                              '1_25{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),\n",
    "                              '1_250{}'.format(climate_model),'1_500{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "        elif hazard_type == 'fl':\n",
    "            return_periods = ['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000']  \n",
    "\n",
    "        overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],power_poly).T,\n",
    "                                    columns=['asset','hazard_point'])\n",
    "        collect_poly_damages = []\n",
    "        for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),\n",
    "                          desc='polygon damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "            for return_period in return_periods:\n",
    "                collect_poly_damages.append([return_period,get_damage_per_asset_per_rp(asset,\n",
    "                                                                                       df_ds[climate_model],\n",
    "                                                                                       power_poly,\n",
    "                                                                                       curves,\n",
    "                                                                                       maxdam,\n",
    "                                                                                       return_period,\n",
    "                                                                                       country_code)])\n",
    "        get_asset_type_poly = dict(zip(power_poly.index,power_poly.asset))\n",
    "\n",
    "        if hazard_type == 'tc':\n",
    "            results = pd.concat([pd.DataFrame(line[1],columns=pd.MultiIndex.from_product([[line[0]],\n",
    "                                                                                ['asset','curve','damage']])).stack(level=0) \n",
    "                       for line in collect_poly_damages]).reset_index(level=1).reset_index(drop=True)\n",
    "        elif hazard_type == 'fl':\n",
    "            results = pd.concat([pd.DataFrame([line[1]],columns=pd.MultiIndex.from_product([[line[0]],\n",
    "                                                                                        ['asset','curve','damage']])).stack(level=0) \n",
    "                               for line in collect_poly_damages]).reset_index(level=1).reset_index(drop=True) \n",
    "\n",
    "        results.columns = ['rp','asset','curve','damage']\n",
    "        results['asset_type'] = results.asset.apply(lambda x : get_asset_type_poly[x])\n",
    "\n",
    "        damaged_poly[climate_model] = results.groupby(['rp','curve','asset_type']).sum()['damage'].reset_index()\n",
    "\n",
    "    # calculate damaged points in loop by country_code and climate_model\n",
    "    damaged_points = {}\n",
    "    for climate_model in climate_models:\n",
    "        if hazard_type == 'tc':\n",
    "            return_periods = ['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),'1_10{}'.format(climate_model),\n",
    "                              '1_25{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),\n",
    "                              '1_250{}'.format(climate_model),'1_500{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "        elif hazard_type == 'fl':\n",
    "            return_periods = ['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000']  \n",
    "\n",
    "        overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],power_points).T,\n",
    "                                      columns=['asset','hazard_point'])\n",
    "        collect_point_damages = []\n",
    "        for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                          desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "            for return_period in return_periods:\n",
    "                collect_point_damages.append([return_period,get_damage_per_asset_per_rp(asset,\n",
    "                                                                                        df_ds[climate_model],\n",
    "                                                                                        power_points,\n",
    "                                                                                        curves,\n",
    "                                                                                        maxdam,\n",
    "                                                                                        return_period,\n",
    "                                                                                        country_code)])\n",
    "\n",
    "        get_asset_type_point = dict(zip(power_points.index,power_points.asset))\n",
    "\n",
    "        if hazard_type == 'tc':\n",
    "            results = pd.concat([pd.DataFrame(line[1],columns=pd.MultiIndex.from_product([[line[0]],\n",
    "                                                                                ['asset','curve','damage']])).stack(level=0) \n",
    "                       for line in collect_point_damages]).reset_index(level=1).reset_index(drop=True)\n",
    "        elif hazard_type == 'fl':\n",
    "            results = pd.concat([pd.DataFrame([line[1]],columns=pd.MultiIndex.from_product([[line[0]],\n",
    "                                                                                        ['asset','curve','damage']])).stack(level=0) \n",
    "                               for line in collect_point_damages]).reset_index(level=1).reset_index(drop=True) \n",
    "\n",
    "        results.columns = ['rp','asset','curve','damage']\n",
    "        results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])\n",
    "\n",
    "        damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum()['damage'].reset_index()\n",
    "\n",
    "    return damaged_lines,damaged_poly,damaged_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ac71f06a-d796-456c-a7b5-4564ba01f806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for LAO fl (historical): 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [141]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m osm_damage_infra \u001b[38;5;241m=\u001b[39m \u001b[43massess_damage_osm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLAO\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mosm_power_infra\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [137]\u001b[0m, in \u001b[0;36massess_damage_osm\u001b[1;34m(country_code, osm_power_infra, hazard_type)\u001b[0m\n\u001b[0;32m     53\u001b[0m     results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mDataFrame(line[\u001b[38;5;241m1\u001b[39m],columns\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mMultiIndex\u001b[38;5;241m.\u001b[39mfrom_product([[line[\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m     54\u001b[0m                                                                         [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masset\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurve\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdamage\u001b[39m\u001b[38;5;124m'\u001b[39m]]))\u001b[38;5;241m.\u001b[39mstack(level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \n\u001b[0;32m     55\u001b[0m                \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m collect_line_damages])\u001b[38;5;241m.\u001b[39mreset_index(level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hazard_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfl\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 57\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_product\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43masset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcurve\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdamage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollect_line_damages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index(level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[0;32m     61\u001b[0m results\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrp\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masset\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurve\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdamage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     62\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masset_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39masset\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x : get_asset_type_line[x])\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[0;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    along the other axes.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:404\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    401\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "osm_damage_infra = assess_damage_osm('LAO',osm_power_infra,'tc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "457c186a-018f-4a87-b4e0-c0a7e2927aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_analysis_osm(country_code,hazard_type): #\n",
    "    \n",
    "    # extract infrastructure data from OSM\n",
    "    osm_power_infra = extract_osm_infrastructure(country_code,osm_data_path)\n",
    "    \n",
    "    # assess damage to hazard_type\n",
    "    osm_damage_infra = assess_damage_osm(country_code,osm_power_infra,hazard_type)\n",
    "\n",
    "    return osm_damage_infra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "b08ddf81-6571-4e42-bbdd-b6b6e948186a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|████████████████████████████████████████████████████████████████████| 2643/2643 [00:04<00:00, 628.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "Input \u001b[1;32mIn [175]\u001b[0m, in \u001b[0;36mcountry_analysis_osm\u001b[1;34m(country_code, hazard_type)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcountry_analysis_osm\u001b[39m(country_code,hazard_type): \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# extract infrastructure data from OSM\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     osm_power_infra \u001b[38;5;241m=\u001b[39m \u001b[43mextract_osm_infrastructure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountry_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43mosm_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# assess damage to hazard_type\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     osm_damage_infra \u001b[38;5;241m=\u001b[39m assess_damage_osm(country_code,osm_power_infra,hazard_type)\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mextract_osm_infrastructure\u001b[1;34m(country_code, osm_data_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# polygons\u001b[39;00m\n\u001b[0;32m     11\u001b[0m osm_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(osm_data_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.osm.pbf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(country_code))\n\u001b[1;32m---> 12\u001b[0m power_poly_country \u001b[38;5;241m=\u001b[39m \u001b[43melectricity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mosm_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m power_poly_country[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m reproject(power_poly_country)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# points\u001b[39;00m\n",
      "Input \u001b[1;32mIn [231]\u001b[0m, in \u001b[0;36melectricity\u001b[1;34m(osm_path)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melectricity\u001b[39m(osm_path):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    Function to extract building polygons from OpenStreetMap    \u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m        *GeoDataFrame* : a geopandas GeoDataFrame with all unique building polygons.    \u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mosm_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmultipolygons\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpower\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpower\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124masset\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m#df = df[df.asset!='generator']\u001b[39;00m\n",
      "Input \u001b[1;32mIn [231]\u001b[0m, in \u001b[0;36mretrieve\u001b[1;34m(osm_path, geoType, keyCol, **valConstraint)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery is finished, lets start the loop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_lyr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mextract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mGetField(keyCol[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m             geom1 \u001b[38;5;241m=\u001b[39m (feature\u001b[38;5;241m.\u001b[39mgeometry()\u001b[38;5;241m.\u001b[39mExportToWkt())\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\tqdm\\std.py:989\u001b[0m, in \u001b[0;36mtqdm.__init__\u001b[1;34m(self, iterable, desc, total, leave, file, ncols, mininterval, maxinterval, miniters, ascii, disable, unit, unit_scale, dynamic_ncols, smoothing, bar_format, initial, position, postfix, unit_divisor, write_bytes, lock_args, nrows, colour, delay, gui, **kwargs)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m         total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[0;32m    991\u001b[0m         total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\osgeo\\ogr.py:2720\u001b[0m, in \u001b[0;36mLayer.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2718\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   2719\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the number of features in the layer\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetFeatureCount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\osgeo\\ogr.py:1606\u001b[0m, in \u001b[0;36mLayer.GetFeatureCount\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGetFeatureCount\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1573\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \u001b[38;5;124;03m    GetFeatureCount(Layer self, int force=1) -> GIntBig\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;124;03m    GIntBig\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;124;03m    feature count, -1 if count not known. \u001b[39;00m\n\u001b[0;32m   1605\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ogr\u001b[38;5;241m.\u001b[39mLayer_GetFeatureCount(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "osm_damage_infra = country_analysis_osm('TWN','tc') #,'line','PG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093416a-4b36-4f15-affe-2b78c4942c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#osm_damage_infra[1]['_CNRM-CM6-1-HR']\n",
    "osm_damage_infra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ba717-0015-4527-bf1a-a9556f87a541",
   "metadata": {},
   "source": [
    "# Government data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef53a27c-f539-43db-9870-1ee933ddf7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load collected power grid data\n",
    "def extract_pg_data(country_code,pg_type):\n",
    "    files = [x for x in os.listdir(pg_data_path)  if country_code in x ]\n",
    "    \n",
    "    if pg_type=='line':\n",
    "        for file in files: \n",
    "            file_path = os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))\n",
    "\n",
    "            pg_data_country = gpd.read_file(file_path)\n",
    "            pg_data_country = pd.DataFrame(pg_data_country.copy())\n",
    "            #print(pg_data_country.head())\n",
    "            pg_data_country.geometry = pygeos.from_shapely(pg_data_country.geometry)\n",
    "            pg_data_country['geometry'] = reproject(pg_data_country)\n",
    "\n",
    "        pg_data_country = buffer_assets(pg_data_country.loc[pg_data_country.asset.isin(['line'])],buffer_size=100).reset_index(drop=True)\n",
    "\n",
    "    elif pg_type=='point':\n",
    "        for file in files:\n",
    "            file_path = os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))\n",
    "                \n",
    "            pg_data_country = gpd.read_file(file_path)\n",
    "            pg_data_country = pd.DataFrame(pg_data_country.copy())\n",
    "            #print(pg_data_country.head())\n",
    "            pg_data_country.geometry = pygeos.from_shapely(pg_data_country.geometry)\n",
    "            pg_data_country['geometry'] = reproject(pg_data_country)\n",
    "            #print(pg_data_country)\n",
    "\n",
    "        pg_data_country = buffer_assets(pg_data_country.loc[pg_data_country.asset.isin(['plant_point','substation_point','power_tower','power_pole'])],buffer_size=100).reset_index(drop=True)\n",
    "\n",
    "    return pg_data_country\n",
    "\n",
    "def open_pg_data(country_code):\n",
    "    pg_lines = extract_pg_data(country_code,'line')\n",
    "    pg_points = extract_pg_data(country_code,'point')\n",
    "    #print(pg_points)\n",
    "    return pg_lines,pg_points\n",
    "\n",
    "pg_infra = open_pg_data('LAO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24824f71-092f-4509-97cf-4e292da71126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_damage_pg(country_code,pg_infra,hazard_type):\n",
    "    \n",
    "    # load curves and maxdam\n",
    "    curves,maxdam = load_curves_maxdam(os.path.join(vul_curve_path,'infra_vulnerability_data.xlsx'))\n",
    "    #curves['line'] = 1 # remove this when things work!\n",
    "    \n",
    "    # read infrastructure data:\n",
    "    pg_lines,pg_points = pg_infra\n",
    "    #print(type(pg_points))\n",
    "    #print(type(pg_infra))\n",
    "    \n",
    "    pg_lines.head(5)\n",
    "    pg_points.head(5)\n",
    "    \n",
    "    if hazard_type=='tc':\n",
    "        # read wind data\n",
    "        climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM'] # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        df_ds = open_storm_data()\n",
    "        \n",
    "        # calculate damaged lines in loop by country_code and climate_model\n",
    "        damaged_lines = {}\n",
    "        for climate_model in climate_models:\n",
    "            return_periods = ['1_10{}'.format(climate_model),'1_50{}'.format(climate_model),\n",
    "                              '1_100{}'.format(climate_model),'1_500{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "\n",
    "            overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],\n",
    "                                                               pg_lines).T,columns=['asset','hazard_point'])\n",
    "            collect_line_damages = []\n",
    "            for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                              desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_line_damages.append([return_period,get_damage_per_asset_per_rp(asset,\n",
    "                                                                                           df_ds[climate_model],\n",
    "                                                                                           pg_lines,\n",
    "                                                                                           curves,\n",
    "                                                                                           maxdam,\n",
    "                                                                                           return_period,\n",
    "                                                                                           country_code)])\n",
    "\n",
    "        collect_line_damages = [(line[0],line[1][0],line[1][1]) for line in collect_line_damages]\n",
    "        damaged_lines_country = pg_lines.merge(pd.DataFrame(collect_line_damages,columns=['return_period','index','damage']),\n",
    "                                                  left_index=True,right_on='index')\n",
    "        damaged_lines_country = damaged_lines_country.drop(['buffered'],axis=1)\n",
    "        damaged_lines[climate_model] = damaged_lines_country\n",
    "        \n",
    "        # calculate damaged points in loop by country_code and climate_model\n",
    "        damaged_points = {}\n",
    "        for climate_model in climate_models:\n",
    "            return_periods = ['1_10{}'.format(climate_model),'1_50{}'.format(climate_model),\n",
    "                              '1_100{}'.format(climate_model),'1_500{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "\n",
    "            overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],pg_points).T,\n",
    "                                          columns=['asset','hazard_point'])\n",
    "            collect_point_damages = []\n",
    "            for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                              desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_point_damages.append([return_period,get_damage_per_asset_per_rp(asset,\n",
    "                                                                                            df_ds[climate_model],\n",
    "                                                                                            pg_points,\n",
    "                                                                                            curves,\n",
    "                                                                                            maxdam,\n",
    "                                                                                            return_period,\n",
    "                                                                                            country_code)])\n",
    "\n",
    "        collect_point_damages = [(line[0],line[1][0],line[1][1]) for line in collect_point_damages]\n",
    "        damaged_points_country = pg_points.merge(pd.DataFrame(collect_point_damages,columns=['return_period','index','damage']),\n",
    "                                                    left_index=True,right_on='index')\n",
    "        damaged_points_country = damaged_points_country.drop(['buffered'],axis=1)\n",
    "        damaged_points[climate_model] = damaged_points_country\n",
    " \n",
    "    elif hazard_type=='fl':\n",
    "        # read flood data\n",
    "        scenario_types = ('historical','rcp8p5')\n",
    "        df_ds = open_flood_data(country_code) #['historical'].head(30) # REMOVE .HEAD(30)\n",
    "        #time_periods = []\n",
    "        \n",
    "        #for time_period in time_periods:\n",
    "        return_periods = ['rp0010','rp0050','rp0100','rp0500','rp1000']\n",
    "        \n",
    "        # calculate damaged lines in loop by country_code and climate_model\n",
    "        damaged_lines = {}\n",
    "        for scenario_type in scenario_types:\n",
    "            overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[scenario_type],pg_lines).T,columns=['asset','hazard_point'])\n",
    "            collect_line_damages = []\n",
    "            for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                              desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,scenario_type)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_line_damages.append([return_period,get_damage_per_asset_per_rp(asset,\n",
    "                                                                                           df_ds[scenario_type],\n",
    "                                                                                           pg_lines,\n",
    "                                                                                           curves,\n",
    "                                                                                           maxdam,\n",
    "                                                                                           return_period,\n",
    "                                                                                           country_code)])\n",
    "                    \n",
    "        collect_line_damages = [(line[0],line[1][0],line[1][1]) for line in collect_line_damages]\n",
    "        damaged_lines_country = pg_lines.merge(pd.DataFrame(collect_line_damages,columns=['return_period','index','damage']),\n",
    "                                                  left_index=True,right_on='index')\n",
    "        damaged_lines = damaged_lines_country.drop(['buffered'],axis=1)\n",
    "        \n",
    "        # calculate damaged points in loop by country_code and climate_model\n",
    "        damaged_points = {}\n",
    "        for scenario_type in scenario_types:\n",
    "            overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[scenario_type],pg_points).T,\n",
    "                                          columns=['asset','hazard_point'])\n",
    "            collect_point_damages = []\n",
    "            for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                              desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,scenario_type)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_point_damages.append([return_period,get_damage_per_asset_per_rp(asset,\n",
    "                                                                                            df_ds[scenario_type],\n",
    "                                                                                            pg_points,\n",
    "                                                                                            curves,\n",
    "                                                                                            maxdam,\n",
    "                                                                                            return_period,\n",
    "                                                                                            country_code)])\n",
    "\n",
    "        collect_point_damages = [(line[0],line[1][0],line[1][1]) for line in collect_point_damages]\n",
    "        damaged_points_country = pg_points.merge(pd.DataFrame(collect_point_damages,columns=['return_period','index','damage']),\n",
    "                                                    left_index=True,right_on='index')\n",
    "        damaged_points = damaged_points_country.drop(['buffered'],axis=1)\n",
    "        \n",
    "    return damaged_lines,damaged_points\n",
    "\n",
    "pg_damage_infra = assess_damage_pg('LAO',pg_infra,'tc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24da424b-cbc9-4359-a557-7900720bbef8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pg_damage_infra' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpg_damage_infra\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pg_damage_infra' is not defined"
     ]
    }
   ],
   "source": [
    "pg_damage_infra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fac49c0-8408-4d6a-b063-ef6180b6c315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'tuple'>\n",
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for LAO fl (historical):  74%|█████████████████      | 317/427 [3:20:12<1:09:28, 37.89s/it]\n",
      "Exception ignored in: <function ZipFile.__del__ at 0x0000012B1188B040>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mye500\\Miniconda3\\envs\\pgrisk\\lib\\zipfile.py\", line 1816, in __del__\n",
      "    self.close()\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def country_analysis_pg(country_code,hazard_type): #\n",
    "    \n",
    "    # extract infrastructure data from OSM\n",
    "    pg_infra = open_pg_data(country_code)\n",
    "\n",
    "    # assess damage to wind storms\n",
    "    pg_damage_infra = assess_damage_pg(country_code,pg_infra,hazard_type)\n",
    "\n",
    "    return pg_damage_infra\n",
    "    \n",
    "    \n",
    "pg_damage_infra = country_analysis_pg('LAO','fl') #,'line','PG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db9c5e8-9e66-40ed-9f76-3049cfa21b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'_CMCC-CM2-VHR4':         status  capacity_kV              value   id      source country  \\\n",
       "  0     Existing          230  transmission_line  0.0  World Bank    Laos   \n",
       "  1     Existing          230  transmission_line  0.0  World Bank    Laos   \n",
       "  2     Existing          230  transmission_line  0.0  World Bank    Laos   \n",
       "  3     Existing          230  transmission_line  0.0  World Bank    Laos   \n",
       "  4     Existing          230  transmission_line  0.0  World Bank    Laos   \n",
       "  ...        ...          ...                ...  ...         ...     ...   \n",
       "  2130  Existing           22               None  NaN  World Bank    None   \n",
       "  2131  Existing           22               None  NaN  World Bank    None   \n",
       "  2132  Existing           22               None  NaN  World Bank    None   \n",
       "  2133  Existing           22               None  NaN  World Bank    None   \n",
       "  2134  Existing           22               None  NaN  World Bank    None   \n",
       "  \n",
       "       operator undergrnd phases cables  year asset  \\\n",
       "  0        None      None   None   None  None  line   \n",
       "  1        None      None   None   None  None  line   \n",
       "  2        None      None   None   None  None  line   \n",
       "  3        None      None   None   None  None  line   \n",
       "  4        None      None   None   None  None  line   \n",
       "  ...       ...       ...    ...    ...   ...   ...   \n",
       "  2130     None      None   None   None  None  line   \n",
       "  2131     None      None   None   None  None  line   \n",
       "  2132     None      None   None   None  None  line   \n",
       "  2133     None      None   None   None  None  line   \n",
       "  2134     None      None   None   None  None  line   \n",
       "  \n",
       "                                                 geometry         return_period  \\\n",
       "  0     LINESTRING (11642041.532 2064458.971, 11641491...    1_10_CMCC-CM2-VHR4   \n",
       "  1     LINESTRING (11642041.532 2064458.971, 11641491...    1_50_CMCC-CM2-VHR4   \n",
       "  2     LINESTRING (11642041.532 2064458.971, 11641491...   1_100_CMCC-CM2-VHR4   \n",
       "  3     LINESTRING (11642041.532 2064458.971, 11641491...   1_500_CMCC-CM2-VHR4   \n",
       "  4     LINESTRING (11642041.532 2064458.971, 11641491...  1_1000_CMCC-CM2-VHR4   \n",
       "  ...                                                 ...                   ...   \n",
       "  2130  LINESTRING (11369167.465 2255664.932, 11361366...    1_10_CMCC-CM2-VHR4   \n",
       "  2131  LINESTRING (11369167.465 2255664.932, 11361366...    1_50_CMCC-CM2-VHR4   \n",
       "  2132  LINESTRING (11369167.465 2255664.932, 11361366...   1_100_CMCC-CM2-VHR4   \n",
       "  2133  LINESTRING (11369167.465 2255664.932, 11361366...   1_500_CMCC-CM2-VHR4   \n",
       "  2134  LINESTRING (11369167.465 2255664.932, 11361366...  1_1000_CMCC-CM2-VHR4   \n",
       "  \n",
       "        index        damage  \n",
       "  0         0  2.790178e+08  \n",
       "  1         0  2.790178e+08  \n",
       "  2         0  2.790178e+08  \n",
       "  3         0  2.790178e+08  \n",
       "  4         0  2.790178e+08  \n",
       "  ...     ...           ...  \n",
       "  2130    426  2.658654e+07  \n",
       "  2131    426  2.658654e+07  \n",
       "  2132    426  2.658654e+07  \n",
       "  2133    426  2.658654e+07  \n",
       "  2134    426  2.658654e+07  \n",
       "  \n",
       "  [2135 rows x 16 columns]},\n",
       " {'_CMCC-CM2-VHR4':        status      source country   type capacit_MW  involt_kV outvolt_kV  \\\n",
       "  0    Existing  World Bank    Laos  Hydro       None        NaN       None   \n",
       "  1    Existing  World Bank    Laos  Hydro       None        NaN       None   \n",
       "  2    Existing  World Bank    Laos  Hydro       None        NaN       None   \n",
       "  3    Existing  World Bank    Laos  Hydro       None        NaN       None   \n",
       "  4    Existing  World Bank    Laos  Hydro       None        NaN       None   \n",
       "  ..        ...         ...     ...    ...        ...        ...        ...   \n",
       "  180   Planned  World Bank    Laos   None       None       22.0       None   \n",
       "  181   Planned  World Bank    Laos   None       None       22.0       None   \n",
       "  182   Planned  World Bank    Laos   None       None       22.0       None   \n",
       "  183   Planned  World Bank    Laos   None       None       22.0       None   \n",
       "  184   Planned  World Bank    Laos   None       None       22.0       None   \n",
       "  \n",
       "      capaci_kVA units                  layer  \\\n",
       "  0         None  None  lao33762powerstations   \n",
       "  1         None  None  lao33762powerstations   \n",
       "  2         None  None  lao33762powerstations   \n",
       "  3         None  None  lao33762powerstations   \n",
       "  4         None  None  lao33762powerstations   \n",
       "  ..         ...   ...                    ...   \n",
       "  180       None  None    lao33762substations   \n",
       "  181       None  None    lao33762substations   \n",
       "  182       None  None    lao33762substations   \n",
       "  183       None  None    lao33762substations   \n",
       "  184       None  None    lao33762substations   \n",
       "  \n",
       "                                                    path             asset  \\\n",
       "  0    C:/Users/mye500/OneDrive - Vrije Universiteit ...       plant_point   \n",
       "  1    C:/Users/mye500/OneDrive - Vrije Universiteit ...       plant_point   \n",
       "  2    C:/Users/mye500/OneDrive - Vrije Universiteit ...       plant_point   \n",
       "  3    C:/Users/mye500/OneDrive - Vrije Universiteit ...       plant_point   \n",
       "  4    C:/Users/mye500/OneDrive - Vrije Universiteit ...       plant_point   \n",
       "  ..                                                 ...               ...   \n",
       "  180  C:/Users/mye500/OneDrive - Vrije Universiteit ...  substation_point   \n",
       "  181  C:/Users/mye500/OneDrive - Vrije Universiteit ...  substation_point   \n",
       "  182  C:/Users/mye500/OneDrive - Vrije Universiteit ...  substation_point   \n",
       "  183  C:/Users/mye500/OneDrive - Vrije Universiteit ...  substation_point   \n",
       "  184  C:/Users/mye500/OneDrive - Vrije Universiteit ...  substation_point   \n",
       "  \n",
       "                               geometry         return_period  index  \\\n",
       "  0    POINT (11361572.355 2244469.157)    1_10_CMCC-CM2-VHR4      0   \n",
       "  1    POINT (11361572.355 2244469.157)    1_50_CMCC-CM2-VHR4      0   \n",
       "  2    POINT (11361572.355 2244469.157)   1_100_CMCC-CM2-VHR4      0   \n",
       "  3    POINT (11361572.355 2244469.157)   1_500_CMCC-CM2-VHR4      0   \n",
       "  4    POINT (11361572.355 2244469.157)  1_1000_CMCC-CM2-VHR4      0   \n",
       "  ..                                ...                   ...    ...   \n",
       "  180  POINT (11668538.497 1965410.054)    1_10_CMCC-CM2-VHR4     36   \n",
       "  181  POINT (11668538.497 1965410.054)    1_50_CMCC-CM2-VHR4     36   \n",
       "  182  POINT (11668538.497 1965410.054)   1_100_CMCC-CM2-VHR4     36   \n",
       "  183  POINT (11668538.497 1965410.054)   1_500_CMCC-CM2-VHR4     36   \n",
       "  184  POINT (11668538.497 1965410.054)  1_1000_CMCC-CM2-VHR4     36   \n",
       "  \n",
       "             damage  \n",
       "  0    1.723896e+07  \n",
       "  1    1.965907e+07  \n",
       "  2    2.060631e+07  \n",
       "  3    2.362159e+07  \n",
       "  4    2.478939e+07  \n",
       "  ..            ...  \n",
       "  180  9.672118e+05  \n",
       "  181  1.260258e+06  \n",
       "  182  1.351720e+06  \n",
       "  183  1.495723e+06  \n",
       "  184  1.539769e+06  \n",
       "  \n",
       "  [185 rows x 16 columns]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_damage_infra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188bac94-5a72-447a-ae22-42da57a6b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def country_analysis_pg(country_code,hazard_type): #\n",
    "    \n",
    "    # extract infrastructure data from gov data\n",
    "    osm_power_infra = extract_pg_infra(country_code,pg_data_path)\n",
    "    osm_damage_infra = assess_damage_pg(country_code,pg_data_country,hazard_type)\n",
    "    \n",
    "    return osm_damage_infra\n",
    "    \n",
    "    \n",
    "osm_damage_infra = country_analysis_pg('LAO','fl') #,'line','PG'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769479e-4a85-4931-af47-fb1ff09b73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gridfinder(country_code):\n",
    "    base_map_path = os.path.join(data_path,'base_map')\n",
    "\n",
    "    cty_boundary_path = os.path.join(base_map_path,'gadm41_{}.gpkg'.format(country_code))\n",
    "    cty_boundary = gpd.read_file(cty_boundary_path)\n",
    "    #mask = pd.DataFrame(mask.copy())\n",
    "    #mask.geometry = pygeos.from_shapely(mask.geometry)\n",
    "    #mask['geometry'] = reproject(mask)\n",
    "\n",
    "    gridfinder_path = r'C:\\Users\\mye500\\OneDrive - Vrije Universiteit Amsterdam\\01_Research-Projects\\01_risk_assessment\\PG_data\\gridfinder\\grid.gpkg'\n",
    "    gridfinder = gpd.read_file(gridfinder_path)\n",
    "    #gridfinder = pd.DataFrame(gridfinder.copy())\n",
    "    #gridfinder.geometry = pygeos.from_shapely(gridfinder.geometry)\n",
    "    #gridfinder['geometry'] = reproject(gridfinder)\n",
    "\n",
    "    clipped = gpd.clip(gridfinder,cty_boundary)\n",
    "\n",
    "    return clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fde70-513a-4898-bbc2-ac8fa3912bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_gridfinder('TWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d93b7-74b5-4ff6-b812-ee1f3bc78aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
