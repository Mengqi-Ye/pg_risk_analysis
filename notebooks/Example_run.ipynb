{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5a5263e9-087d-4863-a4e5-839dc33017bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from osgeo import ogr,gdal\n",
    "import os\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pyproj\n",
    "from pygeos import from_wkb,from_wkt\n",
    "import pygeos\n",
    "from tqdm import tqdm\n",
    "from shapely.wkb import loads\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from shapely.geometry import mapping\n",
    "pd.options.mode.chained_assignment = None\n",
    "from rasterio.mask import mask\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f11a902-f512-4fd3-b035-af38041ee083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdal.SetConfigOption(\"OSM_CONFIG_FILE\", os.path.join('..',\"osmconf.ini\"))\n",
    "\n",
    "# change paths to make it work on your own machine\n",
    "data_path = os.path.join('C:\\\\','Data','pg_risk_analysis')\n",
    "tc_path = os.path.join(data_path,'tc_netcdf')\n",
    "fl_path = os.path.join(data_path,'GLOFRIS')\n",
    "osm_data_path = os.path.join('C:\\\\','Data','country_osm')\n",
    "pg_data_path = os.path.join(data_path,'pg_data')\n",
    "vul_curve_path = os.path.join(data_path,'vulnerability_curves','input_vulnerability_data.xlsx')\n",
    "output_path = os.path.join('C:\\\\','projects','pg_risk_analysis','output')\n",
    "ne_path = os.path.join(data_path,'..',\"natural_earth\",\"ne_10m_admin_0_countries.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fc83d355-8c98-4ae6-97b1-3203a29a4fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten(xs):\n",
    "    for x in xs:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            yield from flatten(x)\n",
    "        else:\n",
    "            yield x\n",
    "\n",
    "def query_b(geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    This function builds an SQL query from the values passed to the retrieve() function.\n",
    "    Arguments:\n",
    "         *geoType* : Type of geometry (osm layer) to search for.\n",
    "         *keyCol* : A list of keys/columns that should be selected from the layer.\n",
    "         ***valConstraint* : A dictionary of constraints for the values. e.g. WHERE 'value'>20 or 'value'='constraint'\n",
    "    Returns:\n",
    "        *string: : a SQL query string.\n",
    "    \"\"\"\n",
    "    query = \"SELECT \" + \"osm_id\"\n",
    "    for a in keyCol: query+= \",\"+ a  \n",
    "    query += \" FROM \" + geoType + \" WHERE \"\n",
    "    # If there are values in the dictionary, add constraint clauses\n",
    "    if valConstraint: \n",
    "        for a in [*valConstraint]:\n",
    "            # For each value of the key, add the constraint\n",
    "            for b in valConstraint[a]: query += a + b\n",
    "        query+= \" AND \"\n",
    "    # Always ensures the first key/col provided is not Null.\n",
    "    query+= \"\"+str(keyCol[0]) +\" IS NOT NULL\" \n",
    "    return query \n",
    "\n",
    "\n",
    "def retrieve(osm_path,geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    Function to extract specified geometry and keys/values from OpenStreetMap\n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.     \n",
    "        *geoType* : Type of Geometry to retrieve. e.g. lines, multipolygons, etc.\n",
    "        *keyCol* : These keys will be returned as columns in the dataframe.\n",
    "        ***valConstraint: A dictionary specifiying the value constraints.  \n",
    "        A key can have multiple values (as a list) for more than one constraint for key/value.  \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with all columns, geometries, and constraints specified.    \n",
    "    \"\"\"\n",
    "    driver=ogr.GetDriverByName('OSM')\n",
    "    data = driver.Open(osm_path)\n",
    "    query = query_b(geoType,keyCol,**valConstraint)\n",
    "    sql_lyr = data.ExecuteSQL(query)\n",
    "    features =[]\n",
    "    # cl = columns \n",
    "    cl = ['osm_id'] \n",
    "    for a in keyCol: cl.append(a)\n",
    "    if data is not None:\n",
    "        print('query is finished, lets start the loop')\n",
    "        for feature in tqdm(sql_lyr,desc='extract'):\n",
    "            #try:\n",
    "            if feature.GetField(keyCol[0]) is not None:\n",
    "                geom1 = (feature.geometry().ExportToWkt())\n",
    "                #print(geom1)\n",
    "                geom = from_wkt(feature.geometry().ExportToWkt()) \n",
    "                if geom is None:\n",
    "                    continue\n",
    "                # field will become a row in the dataframe.\n",
    "                field = []\n",
    "                for i in cl: field.append(feature.GetField(i))\n",
    "                field.append(geom)   \n",
    "                features.append(field)\n",
    "            #except:\n",
    "            #    print(\"WARNING: skipped OSM feature\")   \n",
    "    else:\n",
    "        print(\"ERROR: Nonetype error when requesting SQL. Check required.\")    \n",
    "    cl.append('geometry')                   \n",
    "    if len(features) > 0:\n",
    "        return pd.DataFrame(features,columns=cl)\n",
    "    else:\n",
    "        print(\"WARNING: No features or No Memory. returning empty GeoDataFrame\") \n",
    "        return pd.DataFrame(columns=['osm_id','geometry'])\n",
    "\n",
    "def power_polyline(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract all energy linestrings from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'lines',['power','voltage'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).rename(columns={'power': 'asset'})\n",
    "    \n",
    "    #print(df) #check infra keys\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def power_polygon(osm_path): # check with joel, something was wrong here with extracting substations\n",
    "    \"\"\"\n",
    "    Function to extract energy polygons from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['other_tags']) \n",
    "    \n",
    "    df = df.loc[(df.other_tags.str.contains('power'))]   #keep rows containing power data         \n",
    "    df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})     \n",
    "    \n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"substation\"', case=False)]  = 'substation' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"plant\"', case=False)] = 'plant' #specify row\n",
    "    \n",
    "    df = df.loc[(df.asset == 'substation') | (df.asset == 'plant')]\n",
    "            \n",
    "    return df.reset_index(drop=True) \n",
    "\n",
    "def electricity(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract building polygons from OpenStreetMap    \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with all unique building polygons.    \n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['power'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).rename(columns={'power': 'asset'})\n",
    "    \n",
    "    #df = df[df.asset!='generator']\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"substation\"', case=False)]  = 'substation' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"plant\"', case=False)] = 'plant' #specify row\n",
    "    \n",
    "    #print(df)  #check infra keys\n",
    "    \n",
    "    df = df.loc[(df.asset == 'substation') | (df.asset == 'plant')]\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def retrieve_poly_subs(osm_path, w_list, b_list):\n",
    "    \"\"\"\n",
    "    Function to extract electricity substation polygons from OpenStreetMap\n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region\n",
    "        for which we want to do the analysis.\n",
    "        *w_list* :  white list of keywords to search in the other_tags columns\n",
    "        *b_list* :  black list of keywords of rows that should not be selected\n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique substation.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['other_tags'])\n",
    "    df = df[df.other_tags.str.contains('substation', case=False, na=False)]\n",
    "    #df = df.loc[(df.other_tags.str.contains('substation'))]\n",
    "    df = df[~df.other_tags.str.contains('|'.join(b_list))]\n",
    "    #df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})\n",
    "    df['asset']  = 'substation' #specify row\n",
    "    #df = df.loc[(df.asset == 'substation')] #specify row\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def power_point(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract energy points from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"   \n",
    "    df = retrieve(osm_path,'points',['other_tags']) \n",
    "    df = df.loc[(df.other_tags.str.contains('power'))]  #keep rows containing power data       \n",
    "    df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})     \n",
    "        \n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"tower\"', case=False)]  = 'power_tower' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"pole\"', case=False)] = 'power_pole' #specify row\n",
    "    #df['asset'].loc[df['asset'].str.contains('\"utility\"=>\"power\"', case=False)] = 'power_tower' #specify row\n",
    "    \n",
    "    df = df.loc[(df.asset == 'power_tower') | (df.asset == 'power_pole')]\n",
    "            \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "36fe9f2e-0be0-47fd-83ca-67f3a272279d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reproject(df_ds, current_crs=\"epsg:4326\", approximate_crs=\"epsg:3857\"):\n",
    "\n",
    "    # Extract the input geometries as a numpy array of coordinates\n",
    "    geometries = df_ds['geometry']\n",
    "    coords = pygeos.get_coordinates(geometries)\n",
    "\n",
    "    # Transform the coordinates using pyproj\n",
    "    transformer = pyproj.Transformer.from_crs(current_crs, approximate_crs, always_xy=True)\n",
    "    new_coords = transformer.transform(coords[:, 0], coords[:, 1])\n",
    "\n",
    "    # Create a new GeoSeries with the reprojected coordinates\n",
    "    return pygeos.set_coordinates(geometries.copy(), np.array(new_coords).T)\n",
    "\n",
    "def buffer_assets(assets, buffer_size=100):\n",
    "    \"\"\"\n",
    "    Create a buffer of a specified size around the geometries in a GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        assets (GeoDataFrame): A GeoDataFrame containing geometries to be buffered.\n",
    "        buffer_size (int, optional): The distance in the units of the GeoDataFrame's CRS to buffer the geometries.\n",
    "            Defaults to 100.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A new GeoDataFrame with an additional column named 'buffered' containing the buffered\n",
    "            geometries.\n",
    "    \"\"\"\n",
    "    # Create a buffer of the specified size around the geometries\n",
    "    assets['buffered'] = pygeos.buffer(assets.geometry.values, buffer_size)\n",
    "    \n",
    "    return assets\n",
    "\n",
    "def load_curves_maxdam(vul_curve_path,hazard_type):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        data_path ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "\n",
    "    if hazard_type == 'tc':\n",
    "        sheet_name = 'wind_curves'\n",
    "    \n",
    "    elif hazard_type == 'fl':\n",
    "        sheet_name = 'flooding_curves'\n",
    "    \n",
    "    # load curves and maximum damages as separate inputs\n",
    "    curves = pd.read_excel(vul_curve_path,sheet_name=sheet_name,skiprows=10,index_col=[0])\n",
    "    \n",
    "    #if hazard_type == 'fl':\n",
    "    #    maxdam = pd.read_excel(vul_curve_path,sheet_name=sheet_name,index_col=[0]).iloc[:8]\n",
    "    #elif hazard_type == 'tc':\n",
    "    maxdam = pd.read_excel(vul_curve_path,sheet_name=sheet_name,index_col=[0],header=[0,1]).iloc[:7]\n",
    "    maxdam = maxdam.rename({'substation_point':'substation'},level=0,axis=1)\n",
    "\n",
    "    curves.columns = maxdam.columns\n",
    "        \n",
    "    #transpose maxdam so its easier work with the dataframe\n",
    "    maxdam = maxdam.T\n",
    "\n",
    "    #interpolate the curves to fill missing values\n",
    "    curves = curves.interpolate()\n",
    "    \n",
    "    #print(curves)\n",
    "   \n",
    "    return curves,maxdam\n",
    "\n",
    "\n",
    "def overlay_hazard_assets(df_ds, assets):\n",
    "    \"\"\"\n",
    "    Overlay a set of assets with a hazard dataset and return the subset of assets that intersect with\n",
    "    one or more hazard polygons or lines.\n",
    "    \n",
    "    Args:\n",
    "        df_ds (GeoDataFrame): A GeoDataFrame containing the hazard dataset.\n",
    "        assets (GeoDataFrame): A GeoDataFrame containing the assets to be overlaid with the hazard dataset.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: A numpy array of integers representing the indices of the hazard geometries that intersect with\n",
    "            the assets. If the assets have a 'buffered' column, the buffered geometries are used for the overlay.\n",
    "    \"\"\"\n",
    "    hazard_tree = pygeos.STRtree(df_ds.geometry.values)\n",
    "    if (pygeos.get_type_id(assets.iloc[0].geometry) == 3) | (pygeos.get_type_id(assets.iloc[0].geometry) == 6):\n",
    "        return  hazard_tree.query_bulk(assets.geometry,predicate='intersects')    \n",
    "    else:\n",
    "        return  hazard_tree.query_bulk(assets.buffered,predicate='intersects')\n",
    "    \n",
    "def get_damage_per_asset_per_rp(asset,df_ds,assets,curves,maxdam,return_period,country):\n",
    "    \"\"\"\n",
    "    Calculates the damage per asset per return period based on asset type, hazard curves and maximum damage\n",
    "\n",
    "    Args:\n",
    "        asset (tuple): Tuple with two dictionaries, containing the asset index and the hazard point index of the asset\n",
    "        df_ds (pandas.DataFrame): A pandas DataFrame containing hazard points with a 'geometry' column\n",
    "        assets (geopandas.GeoDataFrame): A GeoDataFrame containing asset geometries and asset type information\n",
    "        curves (dict): A dictionary with the asset types as keys and their corresponding hazard curves as values\n",
    "        maxdam (pandas.DataFrame): A pandas DataFrame containing the maximum damage for each asset type\n",
    "        return_period (str): The return period for which the damage should be calculated\n",
    "        country (str): The country for which the damage should be calculated\n",
    "\n",
    "    Returns:\n",
    "        list or tuple: Depending on the input, the function either returns a list of tuples with the asset index, the curve name and the calculated damage, or a tuple with None, None, None if no hazard points are found\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = df_ds.iloc[asset[1]['hazard_point'].values].reset_index()\n",
    "    get_hazard_points = get_hazard_points.loc[pygeos.intersects(get_hazard_points.geometry.values,assets.iloc[asset[0]].geometry)]\n",
    "\n",
    "    \n",
    "    asset_type = assets.iloc[asset[0]].asset\n",
    "    asset_geom = assets.iloc[asset[0]].geometry\n",
    "\n",
    "    if asset_type in ['plant','substation','generator']:\n",
    "        #if plant,substation are points, do not calculate the area\n",
    "        if pygeos.area(asset_geom) == 0:\n",
    "            maxdam_asset = maxdam.loc[asset_type].MaxDam\n",
    "            lowerdam_asset = maxdam.loc[asset_type].LowerDam\n",
    "            upperdam_asset = maxdam.loc[asset_type].UpperDam\n",
    "        else:\n",
    "            maxdam_asset = maxdam.loc[asset_type].MaxDam/pygeos.area(asset_geom)\n",
    "            lowerdam_asset = maxdam.loc[asset_type].LowerDam/pygeos.area(asset_geom)\n",
    "            upperdam_asset = maxdam.loc[asset_type].UpperDam/pygeos.area(asset_geom)\n",
    "    else:\n",
    "        maxdam_asset = maxdam.loc[asset_type].MaxDam\n",
    "        lowerdam_asset = maxdam.loc[asset_type].LowerDam\n",
    "        upperdam_asset = maxdam.loc[asset_type].UpperDam\n",
    "\n",
    "\n",
    "    hazard_intensity = curves[asset_type].index.values\n",
    "    \n",
    "    if isinstance(curves[asset_type],pd.core.series.Series):\n",
    "        fragility_values = curves[asset_type].values.flatten()\n",
    "        only_one = True\n",
    "        curve_name = curves[asset_type].name\n",
    "    elif len(curves[asset_type].columns) == 1:\n",
    "        fragility_values = curves[asset_type].values.flatten()      \n",
    "        only_one = True   \n",
    "        curve_name = curves[asset_type].columns[0]\n",
    "    else:\n",
    "        fragility_values = curves[asset_type].values#.T[0]\n",
    "        maxdam_asset = maxdam_asset.values#[0]\n",
    "        only_one = False\n",
    "\n",
    "    #print(asset[0],asset_type)\n",
    "    if len(get_hazard_points) == 0:\n",
    "        if only_one:\n",
    "            return [return_period,asset[0],curve_name,0,0,0]\n",
    "        else:\n",
    "            return [return_period,asset[0],curves[asset_type].columns[0],0,0,0]\n",
    "            \n",
    "    else:\n",
    "        if only_one:    \n",
    "            # run the calculation as normal when the asset just has a single curve\n",
    "            if pygeos.get_type_id(asset_geom) == 1:            \n",
    "                get_hazard_points['overlay_meters'] = pygeos.length(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "                return [return_period,asset[0],curve_name,np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,\n",
    "                                                             fragility_values))*get_hazard_points.overlay_meters*maxdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,\n",
    "                                                             fragility_values))*get_hazard_points.overlay_meters*lowerdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,\n",
    "                                                             fragility_values))*get_hazard_points.overlay_meters*upperdam_asset)]\n",
    "\n",
    "            elif (pygeos.get_type_id(asset_geom) == 3) | (pygeos.get_type_id(asset_geom) == 6) :\n",
    "                get_hazard_points['overlay_m2'] = pygeos.area(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "                return [return_period,asset[0],curve_name,get_hazard_points.apply(lambda x: np.interp(x[return_period],hazard_intensity, \n",
    "                                                                  fragility_values)*maxdam_asset*x.overlay_m2,axis=1).sum(),\n",
    "                                                          get_hazard_points.apply(lambda x: np.interp(x[return_period],hazard_intensity, \n",
    "                                                                  fragility_values)*lowerdam_asset*x.overlay_m2,axis=1).sum(),\n",
    "                                                          get_hazard_points.apply(lambda x: np.interp(x[return_period],hazard_intensity, \n",
    "                                                                  fragility_values)*upperdam_asset*x.overlay_m2,axis=1).sum()]  \n",
    "\n",
    "            else:\n",
    "                return [return_period,asset[0],curve_name,np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                             hazard_intensity,fragility_values))*maxdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                             hazard_intensity,fragility_values))*lowerdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                             hazard_intensity,fragility_values))*upperdam_asset)]\n",
    "        else:\n",
    "            # run the calculation when the asset has multiple curves\n",
    "            if pygeos.get_type_id(asset_geom) == 1:            \n",
    "                get_hazard_points['overlay_meters'] = pygeos.length(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            elif (pygeos.get_type_id(asset_geom) == 3) | (pygeos.get_type_id(asset_geom) == 6) :\n",
    "                get_hazard_points['overlay_m2'] = pygeos.area(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            \n",
    "            collect_all = []\n",
    "            for iter_,curve_ids in enumerate(curves[asset_type].columns):\n",
    "                if pygeos.get_type_id(asset_geom) == 1:\n",
    "                    collect_all.append([return_period,asset[0],curves[asset_type].columns[iter_],\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*get_hazard_points.overlay_meters*maxdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*get_hazard_points.overlay_meters*lowerdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*get_hazard_points.overlay_meters*upperdam_asset[iter_])])\n",
    "                                   \n",
    "                elif (pygeos.get_type_id(asset_geom) == 3) | (pygeos.get_type_id(asset_geom) == 6) :\n",
    "                    collect_all.append([return_period,asset[0],curves[asset_type].columns[iter_],\n",
    "                                        get_hazard_points.apply(lambda x: np.interp(x[return_period], hazard_intensity,\n",
    "                                                                                    fragility_values.T[iter_])*maxdam_asset[iter_]*x.overlay_m2,axis=1).sum(),\n",
    "                                        get_hazard_points.apply(lambda x: np.interp(x[return_period], hazard_intensity,\n",
    "                                                                                    fragility_values.T[iter_])*lowerdam_asset[iter_]*x.overlay_m2,axis=1).sum(),\n",
    "                                        get_hazard_points.apply(lambda x: np.interp(x[return_period], hazard_intensity,\n",
    "                                                                                    fragility_values.T[iter_])*upperdam_asset[iter_]*x.overlay_m2,axis=1).sum()])\n",
    "\n",
    "                else:\n",
    "                    collect_all.append([return_period,asset[0],curves[asset_type].columns[iter_],\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*maxdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*lowerdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*upperdam_asset[iter_])])\n",
    "            return collect_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1bf0e992-b4d1-46b0-a952-488ac8c5b2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Infrastructure type</th>\n",
       "      <th colspan=\"3\" halign=\"left\">plant</th>\n",
       "      <th colspan=\"3\" halign=\"left\">substation</th>\n",
       "      <th>power_tower</th>\n",
       "      <th colspan=\"3\" halign=\"left\">power_pole</th>\n",
       "      <th>line</th>\n",
       "      <th>minor_line</th>\n",
       "      <th>cable</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Code</th>\n",
       "      <th>F1_1_1</th>\n",
       "      <th>F1_1_2</th>\n",
       "      <th>F1_1_3</th>\n",
       "      <th>F2_1_1</th>\n",
       "      <th>F2_1_2</th>\n",
       "      <th>F2_1_3</th>\n",
       "      <th>F3_1</th>\n",
       "      <th>F4_1_1</th>\n",
       "      <th>F4_1_2</th>\n",
       "      <th>F4_1_3</th>\n",
       "      <th>F5_1</th>\n",
       "      <th>F5_2</th>\n",
       "      <th>F5_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depth (cm)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290.0</th>\n",
       "      <td>0.276</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295.0</th>\n",
       "      <td>0.284</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300.0</th>\n",
       "      <td>0.292</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305.0</th>\n",
       "      <td>0.300</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305.0</th>\n",
       "      <td>0.300</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Infrastructure type  plant               substation               power_tower  \\\n",
       "Code                F1_1_1 F1_1_2 F1_1_3     F2_1_1 F2_1_2 F2_1_3        F3_1   \n",
       "Depth (cm)                                                                      \n",
       "0.0                  0.000  0.000  0.000      0.000  0.000  0.000        0.00   \n",
       "5.0                  0.004  0.004  0.004      0.003  0.003  0.003        0.00   \n",
       "10.0                 0.008  0.008  0.008      0.007  0.007  0.007        0.00   \n",
       "15.0                 0.012  0.012  0.012      0.010  0.010  0.010        0.00   \n",
       "20.0                 0.016  0.016  0.016      0.013  0.013  0.013        0.00   \n",
       "...                    ...    ...    ...        ...    ...    ...         ...   \n",
       "290.0                0.276  0.276  0.276      0.150  0.150  0.150        0.02   \n",
       "295.0                0.284  0.284  0.284      0.154  0.154  0.154        0.02   \n",
       "300.0                0.292  0.292  0.292      0.157  0.157  0.157        0.02   \n",
       "305.0                0.300  0.300  0.300      0.150  0.150  0.150        0.02   \n",
       "305.0                0.300  0.300  0.300      0.150  0.150  0.150        0.02   \n",
       "\n",
       "Infrastructure type power_pole                line minor_line cable  \n",
       "Code                    F4_1_1 F4_1_2 F4_1_3  F5_1       F5_2  F5_3  \n",
       "Depth (cm)                                                           \n",
       "0.0                       0.00   0.00   0.00  0.00       0.00  0.00  \n",
       "5.0                       0.00   0.00   0.00  0.00       0.00  0.00  \n",
       "10.0                      0.00   0.00   0.00  0.00       0.00  0.00  \n",
       "15.0                      0.00   0.00   0.00  0.00       0.00  0.00  \n",
       "20.0                      0.00   0.00   0.00  0.00       0.00  0.00  \n",
       "...                        ...    ...    ...   ...        ...   ...  \n",
       "290.0                     0.02   0.02   0.02  0.02       0.02  0.02  \n",
       "295.0                     0.02   0.02   0.02  0.02       0.02  0.02  \n",
       "300.0                     0.02   0.02   0.02  0.02       0.02  0.02  \n",
       "305.0                     0.02   0.02   0.02  0.02       0.02  0.02  \n",
       "305.0                     0.02   0.02   0.02  0.02       0.02  0.02  \n",
       "\n",
       "[71 rows x 13 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_curves_maxdam(vul_curve_path,'fl')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9896d9f-4592-4552-8876-c1340f3e09f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def open_storm_data(country_code):\n",
    "    \"\"\"\n",
    "    This function loads STORM data for a given country code, clips it based on the country geometry,\n",
    "    and combines data from different basins and climate models.\n",
    "\n",
    "    Args:\n",
    "    - country_code (str): a 3-letter ISO code of the country of interest\n",
    "\n",
    "    Returns:\n",
    "    - df_ds (dict): a dictionary containing STORM data for different climate models, organized by basin\n",
    "    \"\"\"\n",
    "    # set paths\n",
    "    # data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "    \n",
    "    # list of available climate models\n",
    "    climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "\n",
    "    # dictionary of basins for each country\n",
    "    country_basin = {\n",
    "        \"BRN\": [\"WP\"],\n",
    "        \"KHM\": [\"WP\"],\n",
    "        \"CHN\": [\"WP\", \"NI\"],\n",
    "        \"IDN\": [\"SI\", \"SP\", \"NI\", \"WP\"],\n",
    "        \"JPN\": [\"WP\"],\n",
    "        \"LAO\": [\"WP\"],\n",
    "        \"MYS\": [\"WP\", \"NI\"],\n",
    "        \"MNG\": [\"WP\", \"NI\"],\n",
    "        \"MMR\": [\"NI\", \"WP\"],\n",
    "        \"PRK\": [\"WP\"],\n",
    "        \"PHL\": [\"WP\"],\n",
    "        \"SGP\": [\"WP\"],\n",
    "        \"KOR\": [\"WP\"],\n",
    "        \"TWN\": [\"WP\"],\n",
    "        \"THA\": [\"WP\", \"NI\"],\n",
    "        \"VNM\": [\"WP\"]\n",
    "    }\n",
    "\n",
    "    # load country geometry file and create geometry to clip\n",
    "    ne_countries = gpd.read_file(os.path.join(data_path,'..',\"natural_earth\",\"ne_10m_admin_0_countries.shp\"))\n",
    "    bbox = ne_countries.loc[ne_countries['ISO_A3']==country_code].geometry.buffer(1).values[0].bounds\n",
    "\n",
    "    df_ds = {}\n",
    "    for climate_model in climate_models:\n",
    "        concat_prep = []\n",
    "\n",
    "        #combine STORM data from different basins\n",
    "        if \"WP\" in country_basin[country_code]:\n",
    "            WP = load_storm_data(climate_model,'WP',bbox)\n",
    "            concat_prep.append(WP)\n",
    "        if \"SP\" in country_basin[country_code]:\n",
    "            SP = load_storm_data(climate_model,'SP',bbox)\n",
    "            concat_prep.append(SP)\n",
    "        if \"NI\" in country_basin[country_code]:            \n",
    "            NI = load_storm_data(climate_model,'NI',bbox)\n",
    "            concat_prep.append(NI)            \n",
    "        if \"SI\" in country_basin[country_code]:       \n",
    "            SI = load_storm_data(climate_model,'SI',bbox)\n",
    "            concat_prep.append(SI)            \n",
    "                   \n",
    "        df_ds_cl = pd.concat(concat_prep, keys=country_basin[country_code])\n",
    "        df_ds_cl = df_ds_cl.reset_index(drop=True)\n",
    "        df_ds[climate_model] = df_ds_cl\n",
    "\n",
    "    return df_ds\n",
    "\n",
    "def load_storm_data(climate_model,basin,bbox):\n",
    "    \"\"\"\n",
    "    Load storm data from a NetCDF file and process it to return a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - climate_model (str): name of the climate model\n",
    "    - basin (str): name of the basin\n",
    "    - bbox (tuple): bounding box coordinates in the format (minx, miny, maxx, maxy)\n",
    "    - ne_crs (str): CRS string of the North-East projection\n",
    "\n",
    "    Returns:\n",
    "    - df_ds (pd.DataFrame): pandas DataFrame with interpolated wind speeds for different return periods and geometry column\n",
    "    \"\"\"\n",
    "    # set paths\n",
    "    # data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "\n",
    "    filename = os.path.join(tc_path, f'STORM_FIXED_RETURN_PERIODS{climate_model}_{basin}.nc')\n",
    "    \n",
    "    # load data from NetCDF file\n",
    "    with xr.open_dataset(filename) as ds:\n",
    "        \n",
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        ds = ds.rio.clip_box(minx=bbox[0], miny=bbox[1], maxx=bbox[2], maxy=bbox[3])\n",
    "        ds['mean_3s'] = ds['mean']/0.88*1.11 #convert 10-min sustained wind speed to 3-s gust wind speed\n",
    "        #print(ds.head())\n",
    "\n",
    "        # get the mean values\n",
    "        df_ds = ds['mean_3s'].to_dataframe().unstack(level=2).reset_index()\n",
    "\n",
    "        # create geometry values and drop lat lon columns\n",
    "        df_ds['geometry'] = [pygeos.points(x) for x in list(zip(df_ds['lon'], df_ds['lat']))]\n",
    "        df_ds = df_ds.drop(['lat', 'lon'], axis=1, level=0)\n",
    "        \n",
    "        # interpolate wind speeds of 1,2,5,25,and 250-yr return period\n",
    "        ## rename columns to return periods (must be integer for interpolating)\n",
    "        df_ds_geometry = pd.DataFrame()\n",
    "        df_ds_geometry['geometry'] = df_ds['geometry']\n",
    "        df_ds = df_ds.drop(['geometry'], axis=1, level=0)\n",
    "        df_ds = df_ds['mean_3s']\n",
    "        df_ds.columns = [int(x) for x in ds['mean_3s']['rp']]\n",
    "        df_ds[1] = np.nan\n",
    "        df_ds[2] = np.nan\n",
    "        df_ds[5] = np.nan\n",
    "        df_ds[25] = np.nan\n",
    "        df_ds[250] = np.nan\n",
    "        df_ds = df_ds.reindex(sorted(df_ds.columns), axis=1)\n",
    "        df_ds = df_ds.interpolate(method='linear', axis=1, limit_direction='both')\n",
    "        df_ds['geometry'] = df_ds_geometry['geometry']\n",
    "        df_ds = df_ds[[1, 2, 5, 10, 25, 50, 100, 250, 500, 1000, 'geometry']]\n",
    "        \n",
    "        \n",
    "        # rename columns to return periods\n",
    "        df_ds.columns = ['1_{}{}'.format(int(x), climate_model) for x in [1, 2, 5, 10, 25, 50, 100, 250, 500, 1000]] +['geometry']     \n",
    "        df_ds['geometry'] = pygeos.buffer(df_ds.geometry, radius=0.1/2, cap_style='square').values\n",
    "        #print(type(df_ds['geometry'][0]))\n",
    "        # reproject the geometry column to the specified CRS\n",
    "        df_ds['geometry'] = reproject(df_ds)\n",
    "        #print(type(df_ds['geometry'][0]))\n",
    "        #print(df_ds.head())\n",
    "            \n",
    "        # drop all non values to reduce size\n",
    "        #df_ds = df_ds.loc[~df_ds['1_10000{}'.format(climate_model)].isna()].reset_index(drop=True)\n",
    "        df_ds = df_ds.fillna(0)\n",
    "        #print(type(df_ds['geometry'][0]))\n",
    "\n",
    "    return df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6b43b71-49ee-4aee-aca7-db18ecbd8b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brn_wind=open_storm_data('BRN')\n",
    "#print(type(twn_wind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7059f5f8-fff4-4827-a187-bebcf957230d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_1_HadGEM3-GC31-HM</th>\n",
       "      <th>1_2_HadGEM3-GC31-HM</th>\n",
       "      <th>1_5_HadGEM3-GC31-HM</th>\n",
       "      <th>1_10_HadGEM3-GC31-HM</th>\n",
       "      <th>1_25_HadGEM3-GC31-HM</th>\n",
       "      <th>1_50_HadGEM3-GC31-HM</th>\n",
       "      <th>1_100_HadGEM3-GC31-HM</th>\n",
       "      <th>1_250_HadGEM3-GC31-HM</th>\n",
       "      <th>1_500_HadGEM3-GC31-HM</th>\n",
       "      <th>1_1000_HadGEM3-GC31-HM</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.469543</td>\n",
       "      <td>40.469543</td>\n",
       "      <td>40.469543</td>\n",
       "      <td>40.469543</td>\n",
       "      <td>47.006968</td>\n",
       "      <td>51.136926</td>\n",
       "      <td>54.000687</td>\n",
       "      <td>57.706195</td>\n",
       "      <td>59.990146</td>\n",
       "      <td>61.671156</td>\n",
       "      <td>POLYGON ((13725693.215 4397372.744, 13725693.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.559481</td>\n",
       "      <td>40.559481</td>\n",
       "      <td>40.559481</td>\n",
       "      <td>40.559481</td>\n",
       "      <td>46.998784</td>\n",
       "      <td>51.011116</td>\n",
       "      <td>54.276254</td>\n",
       "      <td>57.470437</td>\n",
       "      <td>59.796552</td>\n",
       "      <td>61.367880</td>\n",
       "      <td>POLYGON ((13736825.164 4397372.744, 13736825.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.576730</td>\n",
       "      <td>40.576730</td>\n",
       "      <td>40.576730</td>\n",
       "      <td>40.576730</td>\n",
       "      <td>47.074321</td>\n",
       "      <td>50.719132</td>\n",
       "      <td>54.085564</td>\n",
       "      <td>57.368295</td>\n",
       "      <td>59.887502</td>\n",
       "      <td>61.455106</td>\n",
       "      <td>POLYGON ((13747957.113 4397372.744, 13747957.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.619581</td>\n",
       "      <td>40.619581</td>\n",
       "      <td>40.619581</td>\n",
       "      <td>40.619581</td>\n",
       "      <td>47.139382</td>\n",
       "      <td>50.878822</td>\n",
       "      <td>53.868798</td>\n",
       "      <td>57.394859</td>\n",
       "      <td>59.747586</td>\n",
       "      <td>61.142801</td>\n",
       "      <td>POLYGON ((13759089.062 4397372.744, 13759089.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.767273</td>\n",
       "      <td>40.767273</td>\n",
       "      <td>40.767273</td>\n",
       "      <td>40.767273</td>\n",
       "      <td>47.236443</td>\n",
       "      <td>50.914861</td>\n",
       "      <td>53.647720</td>\n",
       "      <td>57.101056</td>\n",
       "      <td>59.263981</td>\n",
       "      <td>61.107820</td>\n",
       "      <td>POLYGON ((13770221.011 4397372.744, 13770221.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370</th>\n",
       "      <td>28.165473</td>\n",
       "      <td>28.165473</td>\n",
       "      <td>28.165473</td>\n",
       "      <td>28.165473</td>\n",
       "      <td>36.016688</td>\n",
       "      <td>41.022344</td>\n",
       "      <td>44.180265</td>\n",
       "      <td>48.861651</td>\n",
       "      <td>51.562568</td>\n",
       "      <td>54.256075</td>\n",
       "      <td>POLYGON ((14616249.141 5480930.477, 14616249.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6371</th>\n",
       "      <td>28.399414</td>\n",
       "      <td>28.399414</td>\n",
       "      <td>28.399414</td>\n",
       "      <td>28.399414</td>\n",
       "      <td>36.329825</td>\n",
       "      <td>41.091320</td>\n",
       "      <td>44.535876</td>\n",
       "      <td>48.959105</td>\n",
       "      <td>51.567495</td>\n",
       "      <td>54.432705</td>\n",
       "      <td>POLYGON ((14627381.09 5480930.477, 14627381.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6372</th>\n",
       "      <td>28.670507</td>\n",
       "      <td>28.670507</td>\n",
       "      <td>28.670507</td>\n",
       "      <td>28.670507</td>\n",
       "      <td>36.401315</td>\n",
       "      <td>40.999775</td>\n",
       "      <td>45.117046</td>\n",
       "      <td>49.259744</td>\n",
       "      <td>51.713517</td>\n",
       "      <td>54.310172</td>\n",
       "      <td>POLYGON ((14638513.039 5480930.477, 14638513.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6373</th>\n",
       "      <td>28.952827</td>\n",
       "      <td>28.952827</td>\n",
       "      <td>28.952827</td>\n",
       "      <td>28.952827</td>\n",
       "      <td>36.496510</td>\n",
       "      <td>41.032414</td>\n",
       "      <td>45.355875</td>\n",
       "      <td>49.466124</td>\n",
       "      <td>52.108267</td>\n",
       "      <td>54.097208</td>\n",
       "      <td>POLYGON ((14649644.988 5480930.477, 14649644.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6374</th>\n",
       "      <td>29.290191</td>\n",
       "      <td>29.290191</td>\n",
       "      <td>29.290191</td>\n",
       "      <td>29.290191</td>\n",
       "      <td>36.657309</td>\n",
       "      <td>41.175934</td>\n",
       "      <td>45.435669</td>\n",
       "      <td>49.921387</td>\n",
       "      <td>52.398132</td>\n",
       "      <td>54.091844</td>\n",
       "      <td>POLYGON ((14660776.937 5480930.477, 14660776.9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6375 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1_1_HadGEM3-GC31-HM  1_2_HadGEM3-GC31-HM  1_5_HadGEM3-GC31-HM  \\\n",
       "0               40.469543            40.469543            40.469543   \n",
       "1               40.559481            40.559481            40.559481   \n",
       "2               40.576730            40.576730            40.576730   \n",
       "3               40.619581            40.619581            40.619581   \n",
       "4               40.767273            40.767273            40.767273   \n",
       "...                   ...                  ...                  ...   \n",
       "6370            28.165473            28.165473            28.165473   \n",
       "6371            28.399414            28.399414            28.399414   \n",
       "6372            28.670507            28.670507            28.670507   \n",
       "6373            28.952827            28.952827            28.952827   \n",
       "6374            29.290191            29.290191            29.290191   \n",
       "\n",
       "      1_10_HadGEM3-GC31-HM  1_25_HadGEM3-GC31-HM  1_50_HadGEM3-GC31-HM  \\\n",
       "0                40.469543             47.006968             51.136926   \n",
       "1                40.559481             46.998784             51.011116   \n",
       "2                40.576730             47.074321             50.719132   \n",
       "3                40.619581             47.139382             50.878822   \n",
       "4                40.767273             47.236443             50.914861   \n",
       "...                    ...                   ...                   ...   \n",
       "6370             28.165473             36.016688             41.022344   \n",
       "6371             28.399414             36.329825             41.091320   \n",
       "6372             28.670507             36.401315             40.999775   \n",
       "6373             28.952827             36.496510             41.032414   \n",
       "6374             29.290191             36.657309             41.175934   \n",
       "\n",
       "      1_100_HadGEM3-GC31-HM  1_250_HadGEM3-GC31-HM  1_500_HadGEM3-GC31-HM  \\\n",
       "0                 54.000687              57.706195              59.990146   \n",
       "1                 54.276254              57.470437              59.796552   \n",
       "2                 54.085564              57.368295              59.887502   \n",
       "3                 53.868798              57.394859              59.747586   \n",
       "4                 53.647720              57.101056              59.263981   \n",
       "...                     ...                    ...                    ...   \n",
       "6370              44.180265              48.861651              51.562568   \n",
       "6371              44.535876              48.959105              51.567495   \n",
       "6372              45.117046              49.259744              51.713517   \n",
       "6373              45.355875              49.466124              52.108267   \n",
       "6374              45.435669              49.921387              52.398132   \n",
       "\n",
       "      1_1000_HadGEM3-GC31-HM  \\\n",
       "0                  61.671156   \n",
       "1                  61.367880   \n",
       "2                  61.455106   \n",
       "3                  61.142801   \n",
       "4                  61.107820   \n",
       "...                      ...   \n",
       "6370               54.256075   \n",
       "6371               54.432705   \n",
       "6372               54.310172   \n",
       "6373               54.097208   \n",
       "6374               54.091844   \n",
       "\n",
       "                                               geometry  \n",
       "0     POLYGON ((13725693.215 4397372.744, 13725693.2...  \n",
       "1     POLYGON ((13736825.164 4397372.744, 13736825.1...  \n",
       "2     POLYGON ((13747957.113 4397372.744, 13747957.1...  \n",
       "3     POLYGON ((13759089.062 4397372.744, 13759089.0...  \n",
       "4     POLYGON ((13770221.011 4397372.744, 13770221.0...  \n",
       "...                                                 ...  \n",
       "6370  POLYGON ((14616249.141 5480930.477, 14616249.1...  \n",
       "6371  POLYGON ((14627381.09 5480930.477, 14627381.09...  \n",
       "6372  POLYGON ((14638513.039 5480930.477, 14638513.0...  \n",
       "6373  POLYGON ((14649644.988 5480930.477, 14649644.9...  \n",
       "6374  POLYGON ((14660776.937 5480930.477, 14660776.9...  \n",
       "\n",
       "[6375 rows x 11 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prk_wind['_HadGEM3-GC31-HM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c7ff6dd-a3f2-4e2a-becd-e5cffa38edde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clip_flood_data(country_code):\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "\n",
    "    # load country geometry file and create geometry to clip\n",
    "    ne_countries = gpd.read_file(ne_path)\n",
    "    geometry = ne_countries.loc[ne_countries['ISO_A3']==country_code].geometry.values[0]\n",
    "    geoms = [mapping(geometry)]\n",
    "    \n",
    "    #climate_model: historical, rcp4p5, rcp8p5; time_period: hist, 2030, 2050, 2080\n",
    "    rps = ['0001','0002','0005','0010','0025','0050','0100','0250','0500','1000']\n",
    "    climate_models = ['historical','rcp8p5']\n",
    "    \n",
    "    #\"/scistor/ivm/data_catalogue/open_street_map/pg_risk_analysis/GLOFRIS/global/inuncoast_historical_nosub_hist_rp0001_0.tif\"\n",
    "\n",
    "    for rp in rps:\n",
    "        #global input_file\n",
    "        for climate_model in climate_models:\n",
    "            if climate_model=='historical':\n",
    "                input_file = os.path.join(fl_path,'global',\n",
    "                                          'inuncoast_{}_nosub_hist_rp{}_0.tif'.format(climate_model,rp)) \n",
    " \n",
    "            elif climate_model=='rcp8p5':\n",
    "                input_file = os.path.join(fl_path,'global',\n",
    "                                          'inuncoast_{}_nosub_2030_rp{}_0.tif'.format(climate_model,rp))\n",
    "\n",
    "            # load raster file and save clipped version\n",
    "            with rasterio.open(input_file) as src:\n",
    "                out_image, out_transform = mask(src, geoms, crop=True)\n",
    "                out_meta = src.meta\n",
    "\n",
    "                out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "\n",
    "                if 'scistor' in fl_path:\n",
    "                    file_path = os.path.join(fl_path,'country','_'.join([country_code]+input_file.split('_')[6:]))\n",
    "                else:\n",
    "                    file_path = os.path.join(fl_path,'country','_'.join([country_code]+input_file.split('_')[3:]))\n",
    "\n",
    "                with rasterio.open(file_path, \"w\", **out_meta) as dest:\n",
    "                    dest.write(out_image)\n",
    "\n",
    "def load_flood_data(country_code,climate_model):\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "     \n",
    "    rps = ['0001','0002','0005','0010','0025','0050','0100','0250','0500','1000']\n",
    "    collect_df_ds = []\n",
    "    \n",
    "    if climate_model=='historical':\n",
    "        print('Loading historical coastal flood data ...')\n",
    "        for rp in rps:\n",
    "            #for file in files:\n",
    "            file_path = os.path.join(fl_path,'country','{}_{}_nosub_hist_rp{}_0.tif'.format(country_code,climate_model,rp))\n",
    "            with xr.open_dataset(file_path) as ds: #, engine=\"rasterio\"\n",
    "                df_ds = ds.to_dataframe().reset_index()\n",
    "                df_ds['geometry'] = pygeos.points(df_ds.x,y=df_ds.y)\n",
    "                df_ds = df_ds.rename(columns={'band_data': 'rp'+rp}) #rename to return period\n",
    "                \n",
    "                # move from meters to centimeters\n",
    "                df_ds['rp'+rp] = (df_ds['rp'+rp]*100)         \n",
    "                df_ds = df_ds.drop(['band','x', 'y','spatial_ref'], axis=1)\n",
    "                df_ds = df_ds.dropna()\n",
    "                df_ds = df_ds.reset_index(drop=True)\n",
    "                df_ds.geometry= pygeos.buffer(df_ds.geometry,radius=0.00833/2,cap_style='square').values  #?????????????????????????\n",
    "                df_ds['geometry'] = reproject(df_ds)\n",
    "                collect_df_ds.append(df_ds)\n",
    "\n",
    "        df_all = collect_df_ds[0].merge(collect_df_ds[1]).merge(collect_df_ds[2]).merge(collect_df_ds[3]).merge(collect_df_ds[4])\\\n",
    "                 .merge(collect_df_ds[5]).merge(collect_df_ds[6]).merge(collect_df_ds[7]).merge(collect_df_ds[8]).merge(collect_df_ds[9])\n",
    "        df_all = df_all.loc[df_all['rp1000']>0].reset_index(drop=True)\n",
    "\n",
    "    elif climate_model=='rcp8p5':\n",
    "        print('Loading future coastal flood data ...')\n",
    "        for rp in rps:\n",
    "            #for file in files:\n",
    "            file_path = os.path.join(fl_path,'country','{}_{}_nosub_2030_rp{}_0.tif'.format(country_code,climate_model,rp))\n",
    "            with xr.open_dataset(file_path) as ds: #, engine=\"rasterio\"\n",
    "                df_ds = ds.to_dataframe().reset_index()\n",
    "                df_ds['geometry'] = pygeos.points(df_ds.x,y=df_ds.y)\n",
    "                df_ds = df_ds.rename(columns={'band_data': 'rp'+rp}) #rename to return period\n",
    "                df_ds['rp'+rp] = (df_ds['rp'+rp]*100)\n",
    "                df_ds = df_ds.drop(['band','x', 'y','spatial_ref'], axis=1)\n",
    "                df_ds = df_ds.dropna()\n",
    "                df_ds = df_ds.reset_index(drop=True)\n",
    "                df_ds.geometry= pygeos.buffer(df_ds.geometry,radius=0.00833/2,cap_style='square').values\n",
    "                df_ds['geometry'] = reproject(df_ds)\n",
    "                collect_df_ds.append(df_ds)\n",
    "\n",
    "        df_all = collect_df_ds[0].merge(collect_df_ds[1]).merge(collect_df_ds[2]).merge(collect_df_ds[3]).merge(collect_df_ds[4])\\\n",
    "                 .merge(collect_df_ds[5]).merge(collect_df_ds[6]).merge(collect_df_ds[7]).merge(collect_df_ds[8]).merge(collect_df_ds[9])\n",
    "\n",
    "        df_all = df_all.loc[df_all['rp1000']>0].reset_index(drop=True)\n",
    "    return df_all\n",
    "\n",
    "def open_flood_data(country_code):\n",
    "    climate_models = ['historical','rcp8p5']\n",
    "    df_ds = {}\n",
    "    for climate_model in climate_models:\n",
    "        df_ds_sc = load_flood_data(country_code,climate_model)\n",
    "        df_ds[climate_model] = df_ds_sc\n",
    "    \n",
    "    return df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2160cb1f-75ac-48f0-898e-6fdcb8bb3b59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clip_flood_data('JPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5ac9f48-e434-4893-8c04-07e663921d94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n",
      "CPU times: total: 1min 54s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prk_flood = open_flood_data('PRK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3de688-606c-4dd2-abe3-f37015c7442e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OSM data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "934161b6-5f01-4315-87b3-9e4a7bd70876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_osm_infrastructure(country_code,osm_data_path):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        country_code (_type_): _description_\n",
    "        osm_data_path (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "    \n",
    "    # lines\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    osm_lines = power_polyline(osm_path)\n",
    "    osm_lines['geometry'] = reproject(osm_lines)\n",
    "    osm_lines = buffer_assets(osm_lines.loc[osm_lines.asset.isin(\n",
    "        ['cable','minor_cable','line','minor_line'])],buffer_size=100).reset_index(drop=True)\n",
    "    \n",
    "    # polygons\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    osm_polygons = electricity(osm_path)\n",
    "    osm_polygons['geometry'] = reproject(osm_polygons)\n",
    "    \n",
    "    # points\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    osm_points = power_point(osm_path)\n",
    "    osm_points['geometry'] = reproject(osm_points)\n",
    "    osm_points = buffer_assets(osm_points.loc[osm_points.asset.isin(\n",
    "        ['power_tower','power_pole'])],buffer_size=100).reset_index(drop=True)\n",
    "    \n",
    "    return osm_lines,osm_polygons,osm_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16fbb7a-d740-4b39-bfdd-127321fb4cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract:   0%|                                                                                | 0/2502 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "osm_power_infra = extract_osm_infrastructure('TWN',osm_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7a943-1950-445c-8328-f6db95b7f163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_damage_osm(country_code,osm_power_infra,hazard_type):\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "\n",
    "    # load curves and maxdam\n",
    "    curves,maxdam = load_curves_maxdam(vul_curve_path,hazard_type)\n",
    "    \n",
    "    # read infrastructure data:\n",
    "    osm_lines,osm_poly,osm_points = osm_power_infra\n",
    "    #print(osm_lines['asset'].unique())\n",
    "    \n",
    "    #calculate damaged lines/polygons/points in loop by climate_model\n",
    "    damaged_lines = {}\n",
    "    damaged_poly = {}\n",
    "    damaged_points = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        # read wind data\n",
    "        climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "        df_ds = open_storm_data(country_code)\n",
    "\n",
    "        # remove assets that will not have any damage\n",
    "        osm_lines = osm_lines.loc[osm_lines.asset != 'cable'].reset_index(drop=True)\n",
    "        osm_lines['asset'] = osm_lines['asset'].replace(['minor_line'], 'line')\n",
    "        osm_poly = osm_poly.loc[osm_poly.asset != 'plant'].reset_index(drop=True)\n",
    "        \n",
    "        for climate_model in climate_models:\n",
    "            return_periods = ['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),'1_10{}'.format(climate_model),\n",
    "                              '1_25{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),\n",
    "                              '1_250{}'.format(climate_model),'1_500{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "            \n",
    "            # assess damage for lines\n",
    "            overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_lines).T,\n",
    "                                         columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_lines) == 0:\n",
    "                damaged_lines[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_line_damages = []\n",
    "                for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                                  desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_line_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_lines,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_line = dict(zip(osm_lines.index,osm_lines.asset))\n",
    "                results = pd.DataFrame([item for sublist in collect_line_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            # assess damage for polygons\n",
    "            if len(osm_poly) > 0:\n",
    "                overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_poly).T,\n",
    "                                        columns=['asset','hazard_point'])\n",
    "            else:\n",
    "                overlay_poly = pd.DataFrame()\n",
    "\n",
    "            if len(overlay_poly) == 0:\n",
    "                damaged_poly[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_poly_damages = []\n",
    "                for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),\n",
    "                                  desc='polygon damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_poly_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_poly,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_poly = dict(zip(osm_poly.index,osm_poly.asset))\n",
    "                \n",
    "                results = pd.DataFrame([item for sublist in collect_poly_damages \n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_poly[x])    \n",
    "\n",
    "                damaged_poly[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            # assess damage for points\n",
    "            overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_points).T,\n",
    "                                          columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_points) == 0:\n",
    "                damaged_points[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_point_damages = []\n",
    "                for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                                  desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_point_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_points,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_point = dict(zip(osm_points.index,osm_points.asset))\n",
    "                \n",
    "                results = pd.DataFrame([item for sublist in collect_point_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])    \n",
    "\n",
    "                damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "\n",
    "    elif hazard_type=='fl':\n",
    "        # read flood data\n",
    "        climate_models = ['historical','rcp8p5'] #\n",
    "        df_ds = open_flood_data(country_code)\n",
    "    \n",
    "        for climate_model in climate_models:\n",
    "            return_periods = ['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'] \n",
    "\n",
    "            # assess damage for lines\n",
    "            overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_lines).T,\n",
    "                                         columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_lines) == 0:\n",
    "                damaged_lines[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_line_damages = []\n",
    "                for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                                  desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_line_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_lines,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_line = dict(zip(osm_lines.index,osm_lines.asset))\n",
    "                results = pd.DataFrame(collect_line_damages,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                #sum damage of line, cable, and minor_line\n",
    "                results['curve'] = results['curve'].replace(['cable', 'minor_line'], 'line')\n",
    "                results['asset_type'] = results['asset_type'].replace(['cable', 'minor_line'], 'line')\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            # assess damage for polygons\n",
    "            if len(osm_poly) > 0:\n",
    "                overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_poly).T,\n",
    "                                        columns=['asset','hazard_point'])\n",
    "            else:\n",
    "                overlay_poly = pd.DataFrame()\n",
    "\n",
    "            if len(overlay_poly) == 0:\n",
    "                damaged_poly[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_poly_damages = []\n",
    "                for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),\n",
    "                                  desc='polygon damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_poly_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_poly,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_poly = dict(zip(osm_poly.index,osm_poly.asset))\n",
    "                               \n",
    "#                results = pd.DataFrame(collect_poly_damages ,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                results = pd.DataFrame([item for sublist in collect_poly_damages \n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_poly[x])    \n",
    "\n",
    "                damaged_poly[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            # assess damage for points\n",
    "            overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_points).T,\n",
    "                                          columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_points) == 0:\n",
    "                damaged_points[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_point_damages = []\n",
    "                for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                                  desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_point_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_points,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_point = dict(zip(osm_points.index,osm_points.asset))\n",
    "                \n",
    "               \n",
    "                #results = pd.DataFrame(collect_point_damages ,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results = pd.DataFrame(np.array(list(flatten(collect_point_damages))).reshape(\n",
    "                    int(len(list(flatten(collect_point_damages)))/6), 6),\n",
    "                                       columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results['asset'] = results['asset'].astype(int)\n",
    "                results[['meandam','lowerdam','upperdam']] = results[['meandam','lowerdam','upperdam']].astype(float)\n",
    "                \n",
    "                #return collect_point_damages,get_asset_type_point\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])    \n",
    "\n",
    "                damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "    return damaged_lines,damaged_poly,damaged_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d355b5f-1ad4-4d91-b4a3-1506667bb7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_damage_infra = assess_damage_osm('TWN',osm_power_infra,'tc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "313d91ab-0996-4aa6-8de6-49b0cb2bf424",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rp</th>\n",
       "      <th>curve</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>meandam</th>\n",
       "      <th>lowerdam</th>\n",
       "      <th>upperdam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1000_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_1_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>0.018276</td>\n",
       "      <td>0.013707</td>\n",
       "      <td>0.022845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1000_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_1_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>0.036551</td>\n",
       "      <td>0.027413</td>\n",
       "      <td>0.045689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1000_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_1_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>0.091378</td>\n",
       "      <td>0.068534</td>\n",
       "      <td>0.114223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_1000_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_2_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1000_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_2_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1_5_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_6_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>84270.363554</td>\n",
       "      <td>63202.772666</td>\n",
       "      <td>105337.954443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>1_5_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_6_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>821.411243</td>\n",
       "      <td>616.058432</td>\n",
       "      <td>1026.764053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1_5_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_7_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>29009.357101</td>\n",
       "      <td>21757.017826</td>\n",
       "      <td>36261.696376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1_5_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_7_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>59462.163961</td>\n",
       "      <td>44596.622971</td>\n",
       "      <td>74327.704951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1_5_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_7_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>169661.197440</td>\n",
       "      <td>127245.898080</td>\n",
       "      <td>212076.496801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       rp   curve  asset_type        meandam       lowerdam  \\\n",
       "0    1_1000_CMCC-CM2-VHR4  W2_1_1  substation       0.018276       0.013707   \n",
       "1    1_1000_CMCC-CM2-VHR4  W2_1_2  substation       0.036551       0.027413   \n",
       "2    1_1000_CMCC-CM2-VHR4  W2_1_3  substation       0.091378       0.068534   \n",
       "3    1_1000_CMCC-CM2-VHR4  W2_2_1  substation       0.000002       0.000002   \n",
       "4    1_1000_CMCC-CM2-VHR4  W2_2_2  substation       0.000004       0.000003   \n",
       "..                    ...     ...         ...            ...            ...   \n",
       "205     1_5_CMCC-CM2-VHR4  W2_6_2  substation   84270.363554   63202.772666   \n",
       "206     1_5_CMCC-CM2-VHR4  W2_6_3  substation     821.411243     616.058432   \n",
       "207     1_5_CMCC-CM2-VHR4  W2_7_1  substation   29009.357101   21757.017826   \n",
       "208     1_5_CMCC-CM2-VHR4  W2_7_2  substation   59462.163961   44596.622971   \n",
       "209     1_5_CMCC-CM2-VHR4  W2_7_3  substation  169661.197440  127245.898080   \n",
       "\n",
       "          upperdam  \n",
       "0         0.022845  \n",
       "1         0.045689  \n",
       "2         0.114223  \n",
       "3         0.000003  \n",
       "4         0.000005  \n",
       "..             ...  \n",
       "205  105337.954443  \n",
       "206    1026.764053  \n",
       "207   36261.696376  \n",
       "208   74327.704951  \n",
       "209  212076.496801  \n",
       "\n",
       "[210 rows x 6 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract nested dict by key\n",
    "osm_damage_infra[1]['_CMCC-CM2-VHR4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e56c1d61-40b5-4892-ac70-c20c522eb661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def country_analysis_osm(country_code,hazard_type):\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "    \n",
    "    # extract infrastructure data from OSM\n",
    "    osm_power_infra = extract_osm_infrastructure(country_code,osm_data_path)\n",
    "    \n",
    "    # assess damage to hazard_type\n",
    "    osm_damage_infra = assess_damage_osm(country_code,osm_power_infra,hazard_type)\n",
    "    \n",
    "    line_risk = {}\n",
    "    plant_risk = {}\n",
    "    substation_risk = {}\n",
    "    tower_risk = {}\n",
    "    pole_risk = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "\n",
    "        for i in range(len(osm_damage_infra)):\n",
    "            for climate_model in climate_models:\n",
    "                df = osm_damage_infra[i][climate_model]\n",
    "\n",
    "                if len(df) == 0:\n",
    "                    print(\"No {}_{} risk of infra_type {} in {}\".format(hazard_type,climate_model,i,country_code))\n",
    "\n",
    "                else:\n",
    "                    with pd.ExcelWriter(os.path.join(output_path,'damage','{}_osm_{}{}_damage_{}'.format(country_code,hazard_type,climate_model,i)+'.xlsx')) as writer:\n",
    "                        df.to_excel(writer)\n",
    "\n",
    "                    df['rp'] = df['rp'].replace(['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),\n",
    "                                                '1_10{}'.format(climate_model),'1_25{}'.format(climate_model),'1_50{}'.format(climate_model),\n",
    "                                                '1_100{}'.format(climate_model),'1_250{}'.format(climate_model),'1_500{}'.format(climate_model),\n",
    "                                                '1_1000{}'.format(climate_model)],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "                    \n",
    "                    curve_code_substation = ['W2_1_1','W2_1_2','W2_1_3','W2_2_1','W2_2_2','W2_2_3','W2_3_1','W2_3_2','W2_3_3',\n",
    "                                            'W2_4_1','W2_4_2','W2_4_3','W2_5_1','W2_5_2','W2_5_3','W2_6_1','W2_6_2','W2_6_3',\n",
    "                                            'W2_7_1','W2_7_2','W2_7_3']\n",
    "                    \n",
    "                    curve_code_tower = ['W3_1','W3_2','W3_3','W3_4','W3_5','W3_6','W3_7','W3_8','W3_9','W3_10','W3_11','W3_12',\n",
    "                                        'W3_13','W3_14','W3_15','W3_16','W3_17','W3_18','W3_19','W3_20','W3_21','W3_22','W3_23',\n",
    "                                        'W3_24','W3_25','W3_26','W3_27','W3_28','W3_29','W3_30']\n",
    "                    \n",
    "                    curve_code_pole = ['W4_1','W4_2','W4_3','W4_4','W4_5','W4_6','W4_7','W4_8','W4_9','W4_10','W4_11','W4_12',\n",
    "                                    'W4_13','W4_14','W4_15','W4_16','W4_17','W4_18','W4_19','W4_20','W4_21','W4_22','W4_23',\n",
    "                                    'W4_24','W4_25','W4_26','W4_27','W4_28','W4_29','W4_30','W4_31','W4_32','W4_33','W4_34',\n",
    "                                    'W4_35','W4_36','W4_37','W4_38','W4_39','W4_40','W4_41','W4_42','W4_43','W4_44','W4_45',\n",
    "                                    'W4_46','W4_47','W4_48','W4_49','W4_50','W4_51','W4_52','W4_53','W4_54','W4_55']\n",
    "                    \n",
    "                    curve_code_line = ['W5_1','W5_2','W5_3']\n",
    "\n",
    "                    #assess risk for power lines\n",
    "                    if i == 0:\n",
    "                        for curve_code in curve_code_line:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                            loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                            loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                            RPS = loss_list.rp.values.tolist()\n",
    "                            line_risk[climate_model,curve_code] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                            }\n",
    "                            \n",
    "                    #assess risk for power substations                \n",
    "                    elif i == 1:                        \n",
    "                        for curve_code in curve_code_substation:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                            loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                            loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                            RPS = loss_list.rp.values.tolist()\n",
    "                            substation_risk[climate_model,curve_code] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                            }\n",
    "\n",
    "                    #assess risk for power towers and power poles\n",
    "                    elif i == 2:\n",
    "                        for curve_code in curve_code_tower:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power towers ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = df.loc[df['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                tower_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "                        \n",
    "                        for curve_code in curve_code_pole:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power poles ...\")\n",
    "                            \n",
    "                            else:                    \n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = df.loc[df['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                pole_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "    \n",
    "    elif hazard_type=='fl':\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "    \n",
    "        for i in range(len(osm_damage_infra)):\n",
    "            for climate_model in climate_models:\n",
    "                df = osm_damage_infra[i][climate_model]\n",
    "                    \n",
    "                if len(df) == 0:\n",
    "                    print(\"No {}_{} risk of infra_type {} in {}\".format(hazard_type,climate_model,i,country_code))\n",
    "\n",
    "                else:\n",
    "                    with pd.ExcelWriter(os.path.join(output_path,'damage','{}_osm_{}_{}_damage_{}'.format(country_code,hazard_type,climate_model,i)+'.xlsx')) as writer:\n",
    "                        df.to_excel(writer)\n",
    "\n",
    "                    df['rp'] = df['rp'].replace(['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "                    \n",
    "                    curve_code_plant = ['F1_1_1','F1_1_2','F1_1_3']\n",
    "                    curve_code_substation = ['F2_1_1','F2_1_2','F2_1_3']\n",
    "                    curve_code_tower = ['F3_1']\n",
    "                    curve_code_pole = ['F4_1_1','F4_1_2','F4_1_3']\n",
    "                    curve_code_line = ['F5_1']\n",
    "                    curve_code_minor_line = ['F5_2']\n",
    "                    curve_code_cable = ['F5_3']\n",
    "\n",
    "                    #assess risk for power lines\n",
    "                    if i == 0:\n",
    "                        for curve_code in curve_code_line:\n",
    "                            loss_list_mean = df.meandam.values.tolist()\n",
    "                            loss_list_lower = df.lowerdam.values.tolist()\n",
    "                            loss_list_upper = df.upperdam.values.tolist()\n",
    "                            RPS = df.rp.values.tolist()\n",
    "                            line_risk[climate_model,curve_code] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                            }\n",
    "\n",
    "                    #assess risk for power plants and substations                \n",
    "                    elif i == 1:\n",
    "                        for curve_code in curve_code_plant:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of plants ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = df.loc[df['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                plant_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "\n",
    "                        for curve_code in curve_code_substation:    \n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of substations ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = df.loc[df['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                substation_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                    }\n",
    "\n",
    "                    #assess risk for power towers and power poles\n",
    "                    elif i == 2:\n",
    "                        for curve_code in curve_code_tower:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power towers ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = df.loc[df['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                tower_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "                            \n",
    "                        for curve_code in curve_code_pole:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power poles ...\")\n",
    "                            \n",
    "                            else:                    \n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = df.loc[df['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                pole_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "                                \n",
    "    return pd.DataFrame(line_risk),pd.DataFrame(plant_risk),pd.DataFrame(substation_risk),pd.DataFrame(tower_risk),pd.DataFrame(pole_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b08ddf81-6571-4e42-bbdd-b6b6e948186a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2502/2502 [00:04<00:00, 615.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 393/393 [00:09<00:00, 40.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1709591/1709591 [01:33<00:00, 18322.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for TWN fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:02<00:00, 24.37it/s]\n",
      "polygon damage calculation for TWN fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66/66 [00:05<00:00, 12.58it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mcountry_analysis_osm\u001b[1;34m(country_code, hazard_type)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(osm_damage_infra)):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m climate_model \u001b[38;5;129;01min\u001b[39;00m climate_models:\n\u001b[1;32m--> 124\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mosm_damage_infra\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclimate_model\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    127\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m risk of infra_type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(hazard_type,climate_model,i,country_code))\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = country_analysis_osm('TWN','fl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "815b5c0d-739f-4a7a-bd5e-71959ce9dd05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>_CMCC-CM2-VHR4</th>\n",
       "      <th>_CNRM-CM6-1-HR</th>\n",
       "      <th>_EC-Earth3P-HR</th>\n",
       "      <th>_HadGEM3-GC31-HM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_risk</th>\n",
       "      <td>170556.628078</td>\n",
       "      <td>169519.571782</td>\n",
       "      <td>173071.127219</td>\n",
       "      <td>168483.870815</td>\n",
       "      <td>169359.116259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower_risk</th>\n",
       "      <td>127917.471058</td>\n",
       "      <td>127139.678837</td>\n",
       "      <td>129803.345414</td>\n",
       "      <td>126362.903111</td>\n",
       "      <td>127019.337194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_risk</th>\n",
       "      <td>213195.785097</td>\n",
       "      <td>211899.464728</td>\n",
       "      <td>216338.909023</td>\n",
       "      <td>210604.838519</td>\n",
       "      <td>211698.895324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _CMCC-CM2-VHR4  _CNRM-CM6-1-HR  _EC-Earth3P-HR  \\\n",
       "mean_risk   170556.628078   169519.571782   173071.127219   168483.870815   \n",
       "lower_risk  127917.471058   127139.678837   129803.345414   126362.903111   \n",
       "upper_risk  213195.785097   211899.464728   216338.909023   210604.838519   \n",
       "\n",
       "            _HadGEM3-GC31-HM  \n",
       "mean_risk      169359.116259  \n",
       "lower_risk     127019.337194  \n",
       "upper_risk     211698.895324  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_damage_infra[2] #'','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6525b38-e84d-4dd4-94f4-1d676dc36183",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'historical': {'mean': 220072615.93097857,\n",
       "   'lower': 165054461.94823387,\n",
       "   'upper': 275090769.91372323},\n",
       "  'rcp8p5': {'mean': 289554076.10272974,\n",
       "   'lower': 217165557.07704726,\n",
       "   'upper': 361942595.1284121}},\n",
       " {},\n",
       " {},\n",
       " {'historical': 132315.8115881768, 'rcp8p5': 179535.9734914216},\n",
       " {'historical': 0.0, 'rcp8p5': 0.48358291963805916})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_damage_infra#[0]['historical']['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ba717-0015-4527-bf1a-a9556f87a541",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Government data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef53a27c-f539-43db-9870-1ee933ddf7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_pg_infrastructure(country_code):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        country_code (_type_): _description_\n",
    "        pg_type (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "\n",
    "    files = [x for x in os.listdir(pg_data_path)  if country_code in x ]\n",
    "    pg_types = ['line','point']\n",
    "    \n",
    "    for pg_type in pg_types:\n",
    "        #print(os.path.isfile(os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))))\n",
    "        if os.path.isfile(os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))):\n",
    "            if pg_type=='line':\n",
    "                for file in files: \n",
    "                    file_path = os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))\n",
    "\n",
    "                    pg_data_country = gpd.read_file(file_path)\n",
    "                    pg_data_country = pd.DataFrame(pg_data_country.copy())\n",
    "                    pg_data_country.geometry = pygeos.from_shapely(pg_data_country.geometry)\n",
    "                    pg_data_country['geometry'] = reproject(pg_data_country)\n",
    "\n",
    "                pg_lines = buffer_assets(pg_data_country.loc[pg_data_country.asset.isin(['line'])],buffer_size=100).reset_index(drop=True)\n",
    "\n",
    "            elif pg_type=='point':\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))\n",
    "\n",
    "                    pg_data_country = gpd.read_file(file_path)\n",
    "                    pg_data_country = pd.DataFrame(pg_data_country.copy())\n",
    "                    pg_data_country.geometry = pygeos.from_shapely(pg_data_country.geometry)\n",
    "                    pg_data_country['geometry'] = reproject(pg_data_country)\n",
    "\n",
    "                pg_points = buffer_assets(pg_data_country.loc[pg_data_country.asset.isin(['plant','substation','power_tower','power_pole'])],buffer_size=100).reset_index(drop=True)\n",
    "\n",
    "    return pg_lines,pg_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6a7cdb0-f9a6-4d0c-9dd9-c9d88d7e98e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "pg_infra = extract_pg_infrastructure('JPN')\n",
    "print(type(pg_infra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c78d9c1-f773-42ea-ad91-0acb4aad54bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>asset</th>\n",
       "      <th>geometry</th>\n",
       "      <th>buffered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (15911096.018 5305017.559)</td>\n",
       "      <td>POLYGON ((15911196.018 5305017.559, 15911194.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (15830095.626 5281047.141)</td>\n",
       "      <td>POLYGON ((15830195.626 5281047.141, 15830193.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (15784471.912 5372262.28)</td>\n",
       "      <td>POLYGON ((15784571.912 5372262.28, 15784569.99...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (15725429.458 5296354.502)</td>\n",
       "      <td>POLYGON ((15725529.458 5296354.502, 15725527.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (15678341.881 5273735.047)</td>\n",
       "      <td>POLYGON ((15678441.881 5273735.047, 15678439.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (14654065.093 3892423.909)</td>\n",
       "      <td>POLYGON ((14654165.093 3892423.909, 14654163.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (14567941.182 3869470.184)</td>\n",
       "      <td>POLYGON ((14568041.182 3869470.184, 14568039.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (14554522.443 3832084.632)</td>\n",
       "      <td>POLYGON ((14554622.443 3832084.632, 14554620.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (14530856.666 3761834.532)</td>\n",
       "      <td>POLYGON ((14530956.666 3761834.532, 14530954.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (14591119.005 3740566.664)</td>\n",
       "      <td>POLYGON ((14591219.005 3740566.664, 14591217.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id       asset                          geometry  \\\n",
       "0    1  substation  POINT (15911096.018 5305017.559)   \n",
       "1    2  substation  POINT (15830095.626 5281047.141)   \n",
       "2    3  substation   POINT (15784471.912 5372262.28)   \n",
       "3    4  substation  POINT (15725429.458 5296354.502)   \n",
       "4    5  substation  POINT (15678341.881 5273735.047)   \n",
       "..  ..         ...                               ...   \n",
       "63  64  substation  POINT (14654065.093 3892423.909)   \n",
       "64  65  substation  POINT (14567941.182 3869470.184)   \n",
       "65  66  substation  POINT (14554522.443 3832084.632)   \n",
       "66  67  substation  POINT (14530856.666 3761834.532)   \n",
       "67  68  substation  POINT (14591119.005 3740566.664)   \n",
       "\n",
       "                                             buffered  \n",
       "0   POLYGON ((15911196.018 5305017.559, 15911194.0...  \n",
       "1   POLYGON ((15830195.626 5281047.141, 15830193.7...  \n",
       "2   POLYGON ((15784571.912 5372262.28, 15784569.99...  \n",
       "3   POLYGON ((15725529.458 5296354.502, 15725527.5...  \n",
       "4   POLYGON ((15678441.881 5273735.047, 15678439.9...  \n",
       "..                                                ...  \n",
       "63  POLYGON ((14654165.093 3892423.909, 14654163.1...  \n",
       "64  POLYGON ((14568041.182 3869470.184, 14568039.2...  \n",
       "65  POLYGON ((14554622.443 3832084.632, 14554620.5...  \n",
       "66  POLYGON ((14530956.666 3761834.532, 14530954.7...  \n",
       "67  POLYGON ((14591219.005 3740566.664, 14591217.0...  \n",
       "\n",
       "[68 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_infra[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c0797c42-fd24-45f7-a0c0-51783767aa2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_damage_pg(country_code,pg_infra,hazard_type):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        country_code (_type_): _description_\n",
    "        pg_data_country (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "\n",
    "    # load curves and maxdam\n",
    "    curves,maxdam = load_curves_maxdam(vul_curve_path,hazard_type)\n",
    "    \n",
    "    # read infrastructure data:\n",
    "    pg_lines,pg_points = pg_infra\n",
    "    \n",
    "    #calculate damaged lines/polygons/points in loop by climate_model\n",
    "    damaged_lines = {}\n",
    "    damaged_points = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        # read wind data\n",
    "        climate_models = ['_CMCC-CM2-VHR4','_CNRM-CM6-1-HR'] #'','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM'\n",
    "        df_ds = open_storm_data(country_code)\n",
    "        \n",
    "        # remove assets that will not have any damage\n",
    "        pg_points = pg_points.loc[pg_points.asset != 'plant'].reset_index(drop=True)\n",
    "    \n",
    "    elif hazard_type == 'fl':\n",
    "        # read flood data\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "        df_ds = open_flood_data(country_code)\n",
    "        \n",
    "    for climate_model in climate_models:\n",
    "        if hazard_type=='tc':\n",
    "            return_periods = ['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),'1_10{}'.format(climate_model),\n",
    "                              '1_25{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),\n",
    "                              '1_250{}'.format(climate_model),'1_500{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "        elif hazard_type == 'fl':\n",
    "            return_periods = ['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'] \n",
    "            \n",
    "        # assess damage for lines\n",
    "        overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],pg_lines).T,\n",
    "                                     columns=['asset','hazard_point'])\n",
    "\n",
    "        if len(overlay_lines) == 0:\n",
    "            damaged_lines[climate_model] = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            collect_line_damages = []\n",
    "            for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                              desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_line_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                            df_ds[climate_model],\n",
    "                                                                            pg_lines,\n",
    "                                                                            curves,\n",
    "                                                                            maxdam,\n",
    "                                                                            return_period,\n",
    "                                                                            country_code))\n",
    "\n",
    "            get_asset_type_line = dict(zip(pg_lines.index,pg_lines.asset))\n",
    "            \n",
    "            if hazard_type=='tc':\n",
    "                results = pd.DataFrame([item for sublist in collect_line_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "                \n",
    "            elif hazard_type == 'fl':\n",
    "                results = pd.DataFrame(collect_line_damages,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                #sum damage of line, cable, and minor_line\n",
    "                results['curve'] = results['curve'].replace(['cable', 'minor_line'], 'line')\n",
    "                results['asset_type'] = results['asset_type'].replace(['cable', 'minor_line'], 'line')\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index() \n",
    "\n",
    "        # assess damage for points\n",
    "        overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],pg_points).T,\n",
    "                                      columns=['asset','hazard_point'])\n",
    "\n",
    "        if len(overlay_points) == 0:\n",
    "            damaged_points[climate_model] = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            collect_point_damages = []\n",
    "            for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                              desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_point_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                            df_ds[climate_model],\n",
    "                                                                            pg_points,\n",
    "                                                                            curves,\n",
    "                                                                            maxdam,\n",
    "                                                                            return_period,\n",
    "                                                                            country_code))\n",
    "\n",
    "            get_asset_type_point = dict(zip(pg_points.index,pg_points.asset))\n",
    "            \n",
    "            if hazard_type == 'tc':\n",
    "                results = pd.DataFrame([item for sublist in collect_point_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "            elif hazard_type == 'fl':\n",
    "                results = pd.DataFrame(collect_point_damages ,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "            results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])    \n",
    "\n",
    "            damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "                \n",
    "    return damaged_lines,damaged_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5680499-fc0f-454f-ab9b-4452a2c120ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for JPN tc (): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.78it/s]\n",
      "point damage calculation for JPN tc (): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:08<00:00,  7.87it/s]\n",
      "polyline damage calculation for JPN tc (_CMCC-CM2-VHR4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.75it/s]\n",
      "point damage calculation for JPN tc (_CMCC-CM2-VHR4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:08<00:00,  7.79it/s]\n",
      "polyline damage calculation for JPN tc (_CNRM-CM6-1-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.72it/s]\n",
      "point damage calculation for JPN tc (_CNRM-CM6-1-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:08<00:00,  7.76it/s]\n",
      "polyline damage calculation for JPN tc (_EC-Earth3P-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.77it/s]\n",
      "point damage calculation for JPN tc (_EC-Earth3P-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:08<00:00,  7.71it/s]\n",
      "polyline damage calculation for JPN tc (_HadGEM3-GC31-HM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.77it/s]\n",
      "point damage calculation for JPN tc (_HadGEM3-GC31-HM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:08<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 50s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg_damage_infra_tc = assess_damage_pg('JPN',pg_infra,'tc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64cfcf4a-3381-4a83-9f58-6b254ce75336",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rp</th>\n",
       "      <th>curve</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>meandam</th>\n",
       "      <th>lowerdam</th>\n",
       "      <th>upperdam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1</td>\n",
       "      <td>W2_1_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>2.638182e+05</td>\n",
       "      <td>1.978637e+05</td>\n",
       "      <td>3.297728e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1</td>\n",
       "      <td>W2_1_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>5.276365e+05</td>\n",
       "      <td>3.957274e+05</td>\n",
       "      <td>6.595456e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1</td>\n",
       "      <td>W2_1_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>1.319091e+06</td>\n",
       "      <td>9.893184e+05</td>\n",
       "      <td>1.648864e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_1</td>\n",
       "      <td>W2_2_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>1.826395e+04</td>\n",
       "      <td>1.369796e+04</td>\n",
       "      <td>2.282994e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1</td>\n",
       "      <td>W2_2_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>3.652790e+04</td>\n",
       "      <td>2.739592e+04</td>\n",
       "      <td>4.565987e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1_500</td>\n",
       "      <td>W2_6_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>2.243239e+08</td>\n",
       "      <td>1.682430e+08</td>\n",
       "      <td>2.804049e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>1_500</td>\n",
       "      <td>W2_6_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>5.608099e+08</td>\n",
       "      <td>4.206074e+08</td>\n",
       "      <td>7.010123e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1_500</td>\n",
       "      <td>W2_7_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>4.358600e+07</td>\n",
       "      <td>3.268950e+07</td>\n",
       "      <td>5.448250e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1_500</td>\n",
       "      <td>W2_7_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>8.717200e+07</td>\n",
       "      <td>6.537900e+07</td>\n",
       "      <td>1.089650e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1_500</td>\n",
       "      <td>W2_7_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>2.179300e+08</td>\n",
       "      <td>1.634475e+08</td>\n",
       "      <td>2.724125e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rp   curve  asset_type       meandam      lowerdam      upperdam\n",
       "0      1_1  W2_1_1  substation  2.638182e+05  1.978637e+05  3.297728e+05\n",
       "1      1_1  W2_1_2  substation  5.276365e+05  3.957274e+05  6.595456e+05\n",
       "2      1_1  W2_1_3  substation  1.319091e+06  9.893184e+05  1.648864e+06\n",
       "3      1_1  W2_2_1  substation  1.826395e+04  1.369796e+04  2.282994e+04\n",
       "4      1_1  W2_2_2  substation  3.652790e+04  2.739592e+04  4.565987e+04\n",
       "..     ...     ...         ...           ...           ...           ...\n",
       "205  1_500  W2_6_2  substation  2.243239e+08  1.682430e+08  2.804049e+08\n",
       "206  1_500  W2_6_3  substation  5.608099e+08  4.206074e+08  7.010123e+08\n",
       "207  1_500  W2_7_1  substation  4.358600e+07  3.268950e+07  5.448250e+07\n",
       "208  1_500  W2_7_2  substation  8.717200e+07  6.537900e+07  1.089650e+08\n",
       "209  1_500  W2_7_3  substation  2.179300e+08  1.634475e+08  2.724125e+08\n",
       "\n",
       "[210 rows x 6 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_damage_infra_tc[1]['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f7d5fb3-c1c9-4e1e-a73c-fea26042357f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for JPN fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 25.00it/s]\n",
      "point damage calculation for JPN fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 45.45it/s]\n",
      "polyline damage calculation for JPN fl (rcp8p5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 24.04it/s]\n",
      "point damage calculation for JPN fl (rcp8p5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 50.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8min 7s\n",
      "Wall time: 8min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg_damage_infra_fl = assess_damage_pg('JPN',pg_infra,'fl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc6d1154-a26e-4dfe-9ee0-86f6bb80c10d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'historical':        rp       curve  asset_type        meandam       lowerdam       upperdam\n",
       "  0  rp0001  substation  substation  490588.160384  367941.120288  613235.200480\n",
       "  1  rp0002  substation  substation  517498.328609  388123.746457  646872.910762\n",
       "  2  rp0005  substation  substation  559647.208366  419735.406274  699559.010457\n",
       "  3  rp0010  substation  substation  579966.806294  434975.104721  724958.507868\n",
       "  4  rp0025  substation  substation  608736.040167  456552.030125  760920.050208\n",
       "  5  rp0050  substation  substation  632107.357808  474080.518356  790134.197260\n",
       "  6  rp0100  substation  substation  652340.431603  489255.323702  815425.539504\n",
       "  7  rp0250  substation  substation  682990.145845  512242.609384  853737.682306\n",
       "  8  rp0500  substation  substation  699857.025260  524892.768945  874821.281575\n",
       "  9  rp1000  substation  substation  721527.021566  541145.266174  901908.776957,\n",
       "  'rcp8p5':        rp       curve  asset_type        meandam       lowerdam       upperdam\n",
       "  0  rp0001  substation  substation  563081.188997  422310.891747  703851.486246\n",
       "  1  rp0002  substation  substation  576128.595727  432096.446795  720160.744658\n",
       "  2  rp0005  substation  substation  610238.220801  457678.665601  762797.776001\n",
       "  3  rp0010  substation  substation  636848.474749  477636.356061  796060.593436\n",
       "  4  rp0025  substation  substation  663008.857279  497256.642959  828761.071599\n",
       "  5  rp0050  substation  substation  688989.026262  516741.769697  861236.282828\n",
       "  6  rp0100  substation  substation  707410.163024  530557.622268  884262.703781\n",
       "  7  rp0250  substation  substation  731011.743016  548258.807262  913764.678770\n",
       "  8  rp0500  substation  substation  756650.033516  567487.525137  945812.541895\n",
       "  9  rp1000  substation  substation  773409.866355  580057.399766  966762.332944},\n",
       " {})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_damage_infra_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "98d174ad-3c24-44ea-88cb-0b88ee406e40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def country_analysis_pg(country_code,hazard_type):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        country_code (_type_): _description_\n",
    "        hazard_type (str, optional): _description_. Defaults to 'OSM'.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "    \n",
    "    # extract infrastructure data from gov data\n",
    "    pg_power_infra = extract_pg_infrastructure(country_code)\n",
    "    \n",
    "    # assess damage to hazard_type\n",
    "    pg_damage_infra = assess_damage_pg(country_code,pg_power_infra,hazard_type)\n",
    "    \n",
    "    line_risk = {}\n",
    "    plant_risk = {}\n",
    "    substation_risk = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        climate_models = ['_CMCC-CM2-VHR4','_CNRM-CM6-1-HR'] #'','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM'\n",
    "\n",
    "        for i in range(len(pg_damage_infra)):\n",
    "            for climate_model in climate_models:\n",
    "                df = pg_damage_infra[i][climate_model]\n",
    "                    \n",
    "                if len(df) == 0:\n",
    "                    print(\"No {}_{} risk of infra_type {} in {}\".format(hazard_type,climate_model,i,country_code))\n",
    "\n",
    "                else:\n",
    "                    with pd.ExcelWriter(os.path.join(output_path,'damage','{}_pg_{}{}_damage_{}'.format(country_code,hazard_type,climate_model,i)+'.xlsx')) as writer:\n",
    "                        df.to_excel(writer)\n",
    "\n",
    "                    df['rp'] = df['rp'].replace(['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),\n",
    "                                                '1_10{}'.format(climate_model),'1_25{}'.format(climate_model),'1_50{}'.format(climate_model),\n",
    "                                                '1_100{}'.format(climate_model),'1_250{}'.format(climate_model),'1_500{}'.format(climate_model),\n",
    "                                                '1_1000{}'.format(climate_model)],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "                    \n",
    "                    curve_code_substation = ['W2_1_1','W2_1_2','W2_1_3','W2_2_1','W2_2_2','W2_2_3','W2_3_1','W2_3_2','W2_3_3',\n",
    "                                            'W2_4_1','W2_4_2','W2_4_3','W2_5_1','W2_5_2','W2_5_3','W2_6_1','W2_6_2','W2_6_3',\n",
    "                                            'W2_7_1','W2_7_2','W2_7_3']\n",
    "                    \n",
    "                    curve_code_line = ['W5_1','W5_2','W5_3']\n",
    "\n",
    "                    #assess risk for power lines\n",
    "                    if i == 0:\n",
    "                        for curve_code in curve_code_line:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                            loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                            loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                            RPS = loss_list.rp.values.tolist()\n",
    "                            \n",
    "                            line_risk[climate_model,curve_code] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                            }\n",
    "                            #print(line_risk_curve)\n",
    "                    \n",
    "                    #assess risk for power substations                \n",
    "                    elif i == 1:                        \n",
    "                        for curve_code in curve_code_substation:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                            loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                            loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                            RPS = loss_list.rp.values.tolist()\n",
    "                            \n",
    "                            substation_risk[climate_model,curve_code] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                            }\n",
    "\n",
    "\n",
    "    elif hazard_type=='fl':\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "    \n",
    "        for i in range(len(pg_damage_infra)):\n",
    "            for climate_model in climate_models:\n",
    "                df = pg_damage_infra[i][climate_model]\n",
    "                    \n",
    "                if len(df) == 0:\n",
    "                    print(\"No {}_{} risk of infra_type {} in {}\".format(hazard_type,climate_model,i,country_code))\n",
    "\n",
    "                else:\n",
    "                    with pd.ExcelWriter(os.path.join(output_path,'damage','{}_pg_{}_{}_damage_{}'.format(country_code,hazard_type,climate_model,i)+'.xlsx')) as writer:\n",
    "                        df.to_excel(writer)\n",
    "\n",
    "                    df['rp'] = df['rp'].replace(['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "                    \n",
    "                    curve_code_plant = ['F1_1_1','F1_1_2','F1_1_3']\n",
    "                    curve_code_substation = ['F2_1_1','F2_1_2','F2_1_3']\n",
    "                    curve_code_line = ['F5_1']\n",
    "\n",
    "                    #assess risk for power lines\n",
    "                    if i == 0:\n",
    "                        for curve_code in curve_code_line:\n",
    "                            loss_list_mean = df.meandam.values.tolist()\n",
    "                            loss_list_lower = df.lowerdam.values.tolist()\n",
    "                            loss_list_upper = df.upperdam.values.tolist()\n",
    "                            RPS = df.rp.values.tolist()\n",
    "                            line_risk[climate_model,curve_code] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                            }\n",
    "\n",
    "                    #assess risk for power plants and substations                \n",
    "                    elif i == 1:\n",
    "                        for curve_code in curve_code_plant:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of plants ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = df.loc[df['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                plant_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "                            \n",
    "                        for curve_code in curve_code_substation:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of substations ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = df.loc[df['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                substation_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                    }\n",
    "                            \n",
    "    return pd.DataFrame(line_risk),pd.DataFrame(plant_risk),pd.DataFrame(substation_risk)\n",
    "    #return line_risk,plant_risk,substation_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b2f45-0e6a-4452-b85a-1cf009ec3d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pg_risk_tc = country_analysis_pg('JPN','tc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c6222f6c-3c12-4b7a-84ba-544e31de677f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_CMCC-CM2-VHR4</th>\n",
       "      <th>_CNRM-CM6-1-HR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W2_7_3\n",
       "mean_risk   1.434933e...</td>\n",
       "      <td>W2_7_3\n",
       "mean_risk   1.544563e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      _CMCC-CM2-VHR4  \\\n",
       "0                    W2_7_3\n",
       "mean_risk   1.434933e...   \n",
       "\n",
       "                                      _CNRM-CM6-1-HR  \n",
       "0                    W2_7_3\n",
       "mean_risk   1.544563e...  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pg_risk_tc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5755396e-ef87-4815-ba01-1f91eb685bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for JPN fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 24.88it/s]\n",
      "point damage calculation for JPN fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 41.67it/s]\n",
      "polyline damage calculation for JPN fl (rcp8p5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 23.80it/s]\n",
      "point damage calculation for JPN fl (rcp8p5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 49.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No risk of plants ...\n",
      "No risk of plants ...\n"
     ]
    }
   ],
   "source": [
    "pg_risk_fl = country_analysis_pg('JPN','fl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f076a1e2-6f11-47d7-b19b-595720d8de7b",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dda764da-ee7e-49ea-9572-e263c80ddecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def risk_output(country_code,hazard_type,infra_type):\n",
    "    # set paths\n",
    "    data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "  \n",
    "    if infra_type == 'osm':\n",
    "        line_risk,plant_risk,substation_risk,tower_risk,pole_risk = country_analysis_osm(country_code,hazard_type)\n",
    "        \n",
    "        if hazard_type == 'tc':\n",
    "            climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "\n",
    "            for climate_model in climate_models:\n",
    "                if climate_model == '':\n",
    "                    writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}_{}_risk'.format(country_code,infra_type,hazard_type,'present')+'.xlsx'),\n",
    "                                            engine='openpyxl')\n",
    "                else:\n",
    "                    writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}{}_risk'.format(country_code,infra_type,hazard_type,climate_model)+'.xlsx'),\n",
    "                                            engine='openpyxl')\n",
    "                    \n",
    "                # write each dataframe to a different sheet\n",
    "                if len(line_risk) != 0:\n",
    "                    line_risk[climate_model].to_excel(writer, sheet_name='line_risk')\n",
    "                if len(substation_risk) != 0:\n",
    "                    substation_risk[climate_model].to_excel(writer, sheet_name='substation_risk')\n",
    "                if len(tower_risk) != 0:\n",
    "                    tower_risk[climate_model].to_excel(writer, sheet_name='tower_risk')\n",
    "                if len(pole_risk) != 0:\n",
    "                    pole_risk[climate_model].to_excel(writer, sheet_name='pole_risk')\n",
    "                \n",
    "                # save the Excel file\n",
    "                writer.save()\n",
    "\n",
    "        elif hazard_type == 'fl':\n",
    "            climate_models = ['historical','rcp8p5']\n",
    "\n",
    "            for climate_model in climate_models:\n",
    "\n",
    "                # create a Pandas Excel writer using openpyxl engine\n",
    "                writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}_{}_risk'.format(country_code,infra_type,hazard_type,climate_model)+'.xlsx'), engine='openpyxl')\n",
    "                \n",
    "                # write each dataframe to a different sheet\n",
    "                if len(line_risk) != 0:\n",
    "                    line_risk[climate_model].to_excel(writer, sheet_name='line_risk')\n",
    "                if len(plant_risk) != 0:\n",
    "                    plant_risk[climate_model].to_excel(writer, sheet_name='plant_risk')\n",
    "                if len(substation_risk) != 0:\n",
    "                    substation_risk[climate_model].to_excel(writer, sheet_name='substation_risk')\n",
    "                if len(tower_risk) != 0:\n",
    "                    tower_risk[climate_model].to_excel(writer, sheet_name='tower_risk')\n",
    "                if len(pole_risk) != 0:\n",
    "                    pole_risk[climate_model].to_excel(writer, sheet_name='pole_risk')\n",
    "                \n",
    "                # save the Excel file\n",
    "                writer.save()\n",
    "\n",
    "    elif infra_type == 'gov':\n",
    "        line_risk,plant_risk,substation_risk = country_analysis_pg(country_code,hazard_type)\n",
    "        \n",
    "        if hazard_type == 'tc':\n",
    "            climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "\n",
    "            for climate_model in climate_models:\n",
    "                if climate_model == '':\n",
    "                    writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}_{}_risk'.format(country_code,infra_type,hazard_type,'present')+'.xlsx'),\n",
    "                                            engine='openpyxl')\n",
    "                else:\n",
    "                    writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}{}_risk'.format(country_code,infra_type,hazard_type,climate_model)+'.xlsx'),\n",
    "                                            engine='openpyxl')\n",
    "                \n",
    "                # write each dataframe to a different sheet\n",
    "                if len(line_risk) != 0:\n",
    "                    line_risk[climate_model].to_excel(writer, sheet_name='line_risk')\n",
    "                if len(substation_risk) != 0:\n",
    "                    substation_risk[climate_model].to_excel(writer, sheet_name='substation_risk')\n",
    "                    \n",
    "                # save the Excel file\n",
    "                writer.save()\n",
    "\n",
    "        elif hazard_type == 'fl':\n",
    "            climate_models = ['historical','rcp8p5']\n",
    "\n",
    "            for climate_model in climate_models:\n",
    "\n",
    "                # create a Pandas Excel writer using openpyxl engine\n",
    "                writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}_risk'.format(country_code,infra_type,hazard_type)+'.xlsx'), engine='openpyxl')\n",
    "                \n",
    "                # write each dataframe to a different sheet\n",
    "                if len(line_risk) != 0:\n",
    "                    line_risk[climate_model].to_excel(writer, sheet_name='line_risk')\n",
    "                if len(plant_risk) != 0:\n",
    "                    plant_risk[climate_model].to_excel(writer, sheet_name='plant_risk')\n",
    "                if len(substation_risk) != 0:\n",
    "                    substation_risk[climate_model].to_excel(writer, sheet_name='substation_risk')\n",
    "\n",
    "                # save the Excel file\n",
    "                writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d978c878-bbbf-47c5-abc2-2df27355e8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for JPN tc (): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.50it/s]\n",
      "point damage calculation for JPN tc (): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:09<00:00,  7.22it/s]\n",
      "polyline damage calculation for JPN tc (_CMCC-CM2-VHR4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.05it/s]\n",
      "point damage calculation for JPN tc (_CMCC-CM2-VHR4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:09<00:00,  7.20it/s]\n",
      "polyline damage calculation for JPN tc (_CNRM-CM6-1-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.49it/s]\n",
      "point damage calculation for JPN tc (_CNRM-CM6-1-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:09<00:00,  7.30it/s]\n",
      "polyline damage calculation for JPN tc (_EC-Earth3P-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.07it/s]\n",
      "point damage calculation for JPN tc (_EC-Earth3P-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:09<00:00,  7.30it/s]\n",
      "polyline damage calculation for JPN tc (_HadGEM3-GC31-HM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.42it/s]\n",
      "point damage calculation for JPN tc (_HadGEM3-GC31-HM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:09<00:00,  7.09it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ''",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrisk_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJPN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[62], line 61\u001b[0m, in \u001b[0;36mrisk_output\u001b[1;34m(country_code, hazard_type, infra_type)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# write each dataframe to a different sheet\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line_risk) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 61\u001b[0m     \u001b[43mline_risk\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclimate_model\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_excel(writer, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mline_risk\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(substation_risk) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     63\u001b[0m     substation_risk[climate_model]\u001b[38;5;241m.\u001b[39mto_excel(writer, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubstation_risk\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "risk_output('JPN','tc','gov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "f25448e6-5b5d-434d-8332-8471a4a7e2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rp</th>\n",
       "      <th>curve</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>meandam</th>\n",
       "      <th>lowerdam</th>\n",
       "      <th>upperdam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>1.658665e+09</td>\n",
       "      <td>4.523633e+08</td>\n",
       "      <td>2.261816e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.500</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>1.730781e+09</td>\n",
       "      <td>4.720311e+08</td>\n",
       "      <td>2.360156e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>2.388258e+09</td>\n",
       "      <td>6.513430e+08</td>\n",
       "      <td>3.256715e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.100</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>2.552940e+09</td>\n",
       "      <td>6.962564e+08</td>\n",
       "      <td>3.481282e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.040</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>2.767454e+09</td>\n",
       "      <td>7.547601e+08</td>\n",
       "      <td>3.773801e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.020</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>2.921201e+09</td>\n",
       "      <td>7.966912e+08</td>\n",
       "      <td>3.983456e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.010</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>3.080762e+09</td>\n",
       "      <td>8.402079e+08</td>\n",
       "      <td>4.201039e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.004</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>3.356950e+09</td>\n",
       "      <td>9.155320e+08</td>\n",
       "      <td>4.577660e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.002</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>3.528684e+09</td>\n",
       "      <td>9.623684e+08</td>\n",
       "      <td>4.811842e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>3.698459e+09</td>\n",
       "      <td>1.008671e+09</td>\n",
       "      <td>5.043354e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rp  curve asset_type       meandam      lowerdam      upperdam\n",
       "0   1.000  plant      plant  1.658665e+09  4.523633e+08  2.261816e+09\n",
       "2   0.500  plant      plant  1.730781e+09  4.720311e+08  2.360156e+09\n",
       "4   0.200  plant      plant  2.388258e+09  6.513430e+08  3.256715e+09\n",
       "6   0.100  plant      plant  2.552940e+09  6.962564e+08  3.481282e+09\n",
       "8   0.040  plant      plant  2.767454e+09  7.547601e+08  3.773801e+09\n",
       "10  0.020  plant      plant  2.921201e+09  7.966912e+08  3.983456e+09\n",
       "12  0.010  plant      plant  3.080762e+09  8.402079e+08  4.201039e+09\n",
       "14  0.004  plant      plant  3.356950e+09  9.155320e+08  4.577660e+09\n",
       "16  0.002  plant      plant  3.528684e+09  9.623684e+08  4.811842e+09\n",
       "18  0.001  plant      plant  3.698459e+09  1.008671e+09  5.043354e+09"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_damage_infra[1]['historical'].loc[osm_damage_infra[1]['historical']['asset_type'] == 'plant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "b3065e13-2e48-4dd3-b027-5d7815a9f94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.000\n",
       "1     1.000\n",
       "2     1.000\n",
       "3     0.500\n",
       "4     0.500\n",
       "5     0.500\n",
       "6     0.200\n",
       "7     0.200\n",
       "8     0.200\n",
       "9     0.100\n",
       "10    0.100\n",
       "11    0.100\n",
       "12    0.040\n",
       "13    0.040\n",
       "14    0.040\n",
       "15    0.020\n",
       "16    0.020\n",
       "17    0.020\n",
       "18    0.010\n",
       "19    0.010\n",
       "20    0.010\n",
       "21    0.004\n",
       "22    0.004\n",
       "23    0.004\n",
       "24    0.002\n",
       "25    0.002\n",
       "26    0.002\n",
       "27    0.001\n",
       "28    0.001\n",
       "29    0.001\n",
       "Name: rp, dtype: float64"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_damage_infra[0]['historical'].loc[:,\"rp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769479e-4a85-4931-af47-fb1ff09b73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def clip_gridfinder(country_code):\n",
    "    base_map_path = os.path.join(data_path,'base_map')\n",
    "\n",
    "    cty_boundary_path = os.path.join(base_map_path,'gadm41_{}.gpkg'.format(country_code))\n",
    "    cty_boundary = gpd.read_file(cty_boundary_path)\n",
    "    #mask = pd.DataFrame(mask.copy())\n",
    "    #mask.geometry = pygeos.from_shapely(mask.geometry)\n",
    "    #mask['geometry'] = reproject(mask)\n",
    "\n",
    "    gridfinder_path = r'C:\\Users\\mye500\\OneDrive - Vrije Universiteit Amsterdam\\01_Research-Projects\\01_risk_assessment\\PG_data\\gridfinder\\grid.gpkg'\n",
    "    gridfinder = gpd.read_file(gridfinder_path)\n",
    "    #gridfinder = pd.DataFrame(gridfinder.copy())\n",
    "    #gridfinder.geometry = pygeos.from_shapely(gridfinder.geometry)\n",
    "    #gridfinder['geometry'] = reproject(gridfinder)\n",
    "\n",
    "    clipped = gpd.clip(gridfinder,cty_boundary)\n",
    "\n",
    "    return clipped\n",
    "\n",
    "clip_gridfinder('TWN')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
