{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5263e9-087d-4863-a4e5-839dc33017bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from osgeo import ogr,gdal\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pyproj\n",
    "from pygeos import from_wkb,from_wkt\n",
    "import pygeos\n",
    "from tqdm import tqdm\n",
    "from shapely.wkb import loads\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from shapely.geometry import mapping\n",
    "pd.options.mode.chained_assignment = None\n",
    "from rasterio.mask import mask\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate\n",
    "from collections.abc import Iterable\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy import integrate\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f11a902-f512-4fd3-b035-af38041ee083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdal.SetConfigOption(\"OSM_CONFIG_FILE\", os.path.join('..',\"osmconf.ini\"))\n",
    "\n",
    "# change paths to make it work on your own machine\n",
    "data_path = os.path.join('C:\\\\','Data','pg_risk_analysis')\n",
    "tc_path = os.path.join(data_path,'tc_netcdf')\n",
    "fl_path = os.path.join(data_path,'GLOFRIS')\n",
    "osm_data_path = os.path.join('C:\\\\','Data','country_osm')\n",
    "pg_data_path = os.path.join(data_path,'pg_data')\n",
    "vul_curve_path = os.path.join(data_path,'vulnerability_curves','input_vulnerability_data.xlsx')\n",
    "output_path = os.path.join('C:\\\\','projects','pg_risk_analysis_output','output')\n",
    "ne_path = os.path.join(data_path,'..',\"natural_earth\",\"ne_10m_admin_0_countries.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc83d355-8c98-4ae6-97b1-3203a29a4fe1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten(xs):\n",
    "    for x in xs:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            yield from flatten(x)\n",
    "        else:\n",
    "            yield x\n",
    "\n",
    "def query_b(geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    This function builds an SQL query from the values passed to the retrieve() function.\n",
    "    Arguments:\n",
    "         *geoType* : Type of geometry (osm layer) to search for.\n",
    "         *keyCol* : A list of keys/columns that should be selected from the layer.\n",
    "         ***valConstraint* : A dictionary of constraints for the values. e.g. WHERE 'value'>20 or 'value'='constraint'\n",
    "    Returns:\n",
    "        *string: : a SQL query string.\n",
    "    \"\"\"\n",
    "    query = \"SELECT \" + \"osm_id\"\n",
    "    for a in keyCol: query+= \",\"+ a  \n",
    "    query += \" FROM \" + geoType + \" WHERE \"\n",
    "    # If there are values in the dictionary, add constraint clauses\n",
    "    if valConstraint: \n",
    "        for a in [*valConstraint]:\n",
    "            # For each value of the key, add the constraint\n",
    "            for b in valConstraint[a]: query += a + b\n",
    "        query+= \" AND \"\n",
    "    # Always ensures the first key/col provided is not Null.\n",
    "    query+= \"\"+str(keyCol[0]) +\" IS NOT NULL\" \n",
    "    return query \n",
    "\n",
    "\n",
    "def retrieve(osm_path,geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    Function to extract specified geometry and keys/values from OpenStreetMap\n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "            for which we want to do the analysis.     \n",
    "        *geoType* : Type of Geometry to retrieve. e.g. lines, multipolygons, etc.\n",
    "        *keyCol* : These keys will be returned as columns in the dataframe.\n",
    "        ***valConstraint: A dictionary specifiying the value constraints.  \n",
    "        A key can have multiple values (as a list) for more than one constraint for key/value.  \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with all columns, geometries, and constraints specified.    \n",
    "    \"\"\"\n",
    "    driver=ogr.GetDriverByName('OSM')\n",
    "    data = driver.Open(osm_path)\n",
    "    query = query_b(geoType,keyCol,**valConstraint)\n",
    "    sql_lyr = data.ExecuteSQL(query)\n",
    "    features =[]\n",
    "    # cl = columns \n",
    "    cl = ['osm_id'] \n",
    "    for a in keyCol: cl.append(a)\n",
    "    if data is not None:\n",
    "        print('query is finished, lets start the loop')\n",
    "        for feature in tqdm(sql_lyr,desc='extract'):\n",
    "            #try:\n",
    "            if feature.GetField(keyCol[0]) is not None:\n",
    "                geom1 = (feature.geometry().ExportToWkt())\n",
    "                #print(geom1)\n",
    "                geom = from_wkt(feature.geometry().ExportToWkt()) \n",
    "                if geom is None:\n",
    "                    continue\n",
    "                # field will become a row in the dataframe.\n",
    "                field = []\n",
    "                for i in cl: field.append(feature.GetField(i))\n",
    "                field.append(geom)   \n",
    "                features.append(field)\n",
    "            #except:\n",
    "            #    print(\"WARNING: skipped OSM feature\")   \n",
    "    else:\n",
    "        print(\"ERROR: Nonetype error when requesting SQL. Check required.\")    \n",
    "    cl.append('geometry')                   \n",
    "    if len(features) > 0:\n",
    "        return pd.DataFrame(features,columns=cl)\n",
    "    else:\n",
    "        print(\"WARNING: No features or No Memory. returning empty GeoDataFrame\") \n",
    "        return pd.DataFrame(columns=['osm_id','geometry'])\n",
    "\n",
    "def power_polyline(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract all energy linestrings from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'lines',['power','voltage'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).rename(columns={'power': 'asset'})\n",
    "    \n",
    "    #print(df) #check infra keys\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def power_polygon(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract building polygons from OpenStreetMap    \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with all unique building polygons.    \n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['power','plant_source'])\n",
    "    df = df.reset_index(drop=True).rename(columns={'power': 'asset'})\n",
    "    \n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"substation\"', case=False)]  = 'substation' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"plant\"', case=False)] = 'plant'\n",
    "        \n",
    "    df = df.loc[(df.asset == 'substation') | (df.asset == 'plant')]\n",
    "        \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def power_point(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract energy points from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy points.\n",
    "    \"\"\"   \n",
    "    df = retrieve(osm_path,'points',['other_tags'])\n",
    "    df = df.loc[(df.other_tags.str.contains('power'))]  #keep rows containing power data       \n",
    "    df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})\n",
    "        \n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"tower\"', case=False)]  = 'power_tower' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"pole\"', case=False)] = 'power_pole' #specify row\n",
    "    \n",
    "    df = df.loc[(df.asset == 'power_tower') | (df.asset == 'power_pole')]\n",
    "            \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36fe9f2e-0be0-47fd-83ca-67f3a272279d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reproject(df_ds, current_crs=\"epsg:4326\", approximate_crs=\"epsg:3857\"):\n",
    "    \"\"\"\n",
    "    Reproject a GeoPandas DataFrame from one CRS to another.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_ds : geopandas.GeoDataFrame\n",
    "        The input GeoPandas DataFrame to reproject.\n",
    "    current_crs : str, optional\n",
    "        The current CRS of the input geometry column, by default \"epsg:4326\".\n",
    "    approximate_crs : str, optional\n",
    "        The target CRS to reproject to, by default \"epsg:3857\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    geopandas.GeoSeries\n",
    "        The reprojected geometry column as a GeoPandas GeoSeries.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the input geometries as a numpy array of coordinates\n",
    "    geometries = df_ds['geometry']\n",
    "    coords = pygeos.get_coordinates(geometries)\n",
    "\n",
    "    # Transform the coordinates using pyproj\n",
    "    transformer = pyproj.Transformer.from_crs(current_crs, approximate_crs, always_xy=True)\n",
    "    new_coords = transformer.transform(coords[:, 0], coords[:, 1])\n",
    "\n",
    "    # Create a new GeoSeries with the reprojected coordinates\n",
    "    return pygeos.set_coordinates(geometries.copy(), np.array(new_coords).T)\n",
    "\n",
    "def buffer_assets(assets, buffer_size=100):\n",
    "    \"\"\"\n",
    "    Create a buffer of a specified size around the geometries in a GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        assets (GeoDataFrame): A GeoDataFrame containing geometries to be buffered.\n",
    "        buffer_size (int, optional): The distance in the units of the GeoDataFrame's CRS to buffer the geometries.\n",
    "            Defaults to 100.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A new GeoDataFrame with an additional column named 'buffered' containing the buffered\n",
    "            geometries.\n",
    "    \"\"\"\n",
    "    # Create a buffer of the specified size around the geometries\n",
    "    assets['buffered'] = pygeos.buffer(assets.geometry.values, buffer_size)\n",
    "    \n",
    "    return assets\n",
    "\n",
    "def load_curves_maxdam(country_code,vul_curve_path,hazard_type):\n",
    "    \"\"\"Load vulnerability curves and maximum damages for a specific country and hazard type.\n",
    "\n",
    "    Args:\n",
    "        country_code (str): Country code for the desired country.\n",
    "        vul_curve_path (str): Path to the input vulnerability curves file.\n",
    "        hazard_type (str): Type of hazard ('tc' for tropical cyclone, 'fl' for coastal flooding).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two pandas DataFrames:\n",
    "               - curves: Vulnerability curves.\n",
    "               - maxdam: Maximum damages.\n",
    "    \"\"\"\n",
    "\n",
    "    # dictionary of GDP per capita ratio for each country\n",
    "    gdp_ratio = {\n",
    "        \"BRN\": {\"ratio_usa\": 0.5201},\n",
    "        \"KHM\": {\"ratio_usa\": 0.0240},\n",
    "        \"CHN\": {\"ratio_usa\": 0.1772},\n",
    "        \"IDN\": {\"ratio_usa\": 0.0647},\n",
    "        \"JPN\": {\"ratio_usa\": 0.5912},\n",
    "        \"LAO\": {\"ratio_usa\": 0.0434},\n",
    "        \"MYS\": {\"ratio_usa\": 0.1775},\n",
    "        \"MNG\": {\"ratio_usa\": 0.0703},\n",
    "        \"MMR\": {\"ratio_usa\": 0.0276},\n",
    "        \"PRK\": {\"ratio_usa\": 0.0106},\n",
    "        \"PHL\": {\"ratio_usa\": 0.0547},\n",
    "        \"SGP\": {\"ratio_usa\": 1.0091},\n",
    "        \"KOR\": {\"ratio_usa\": 0.5367},\n",
    "        \"TWN\": {\"ratio_usa\": 0.4888},\n",
    "        \"THA\": {\"ratio_usa\": 0.1034},\n",
    "        \"VNM\": {\"ratio_usa\": 0.0573},\n",
    "        \"HKG\": {\"ratio_usa\": 0.7091},\n",
    "        \"MAC\": {\"ratio_usa\": 0.5913}}\n",
    "    \n",
    "    if hazard_type == 'tc':\n",
    "        sheet_name = 'wind_curves'\n",
    "        \n",
    "        # load curves and maximum damages as separate inputs\n",
    "        curves = pd.read_excel(vul_curve_path,sheet_name=sheet_name,skiprows=11)\n",
    "        \n",
    "        # dictionary of design wind speeds for each country\n",
    "        design_wind_speed = {\n",
    "            \"BRN\": {\"dws\": 32},\n",
    "            \"KHM\": {\"dws\": 32},\n",
    "            \"CHN\": {\"dws\": 52},\n",
    "            \"IDN\": {\"dws\": 32},\n",
    "            \"JPN\": {\"dws\": 52},\n",
    "            \"LAO\": {\"dws\": 32},\n",
    "            \"MYS\": {\"dws\": 32},\n",
    "            \"MNG\": {\"dws\": 0},\n",
    "            \"MMR\": {\"dws\": 39},\n",
    "            \"PRK\": {\"dws\": 39},\n",
    "            \"PHL\": {\"dws\": 52},\n",
    "            \"SGP\": {\"dws\": 32},\n",
    "            \"KOR\": {\"dws\": 52},\n",
    "            \"TWN\": {\"dws\": 60},\n",
    "            \"THA\": {\"dws\": 39},\n",
    "            \"VNM\": {\"dws\": 44}}\n",
    "        dws = design_wind_speed.get(country_code, {}).get(\"dws\", None)\n",
    "        \n",
    "        # shift design wind speed of all curves to 60 m/s\n",
    "        scaling_factor = dws / 60\n",
    "\n",
    "        curves = curves.apply(lambda x: x * scaling_factor if pd.api.types.is_numeric_dtype(x) else x)\n",
    "        curves = curves.set_index('Wind speed (m/s)')\n",
    "        \n",
    "    elif hazard_type == 'fl':\n",
    "        sheet_name = 'flooding_curves'    \n",
    "        \n",
    "        # load curves and maximum damages as separate inputs\n",
    "        curves = pd.read_excel(vul_curve_path,sheet_name=sheet_name,skiprows=11,index_col=[0])\n",
    "\n",
    "    maxdam = pd.read_excel(vul_curve_path,sheet_name=sheet_name,index_col=[0],header=[0,1]).iloc[:8]\n",
    "    curves.columns = maxdam.columns\n",
    "    \n",
    "    #interpolate the curves to fill missing values\n",
    "    curves = curves.interpolate()\n",
    "    \n",
    "    #transpose maxdam so its easier work with the dataframe\n",
    "    maxdam = maxdam.T\n",
    "    \n",
    "    ratio_usa = gdp_ratio.get(country_code, {}).get(\"ratio_usa\", None)\n",
    "\n",
    "    if ratio_usa is not None:\n",
    "        print(f\"The ratio_usa for {country_code} is {ratio_usa}\")\n",
    "    else:\n",
    "        print(f\"No ratio_usa found for {country_code}\")\n",
    "        \n",
    "    maxdam['MaxDam'] = maxdam['MaxDam'] * ratio_usa\n",
    "    maxdam['LowerDam'] = maxdam['LowerDam'] * ratio_usa\n",
    "    maxdam['UpperDam'] = maxdam['UpperDam'] * ratio_usa\n",
    "\n",
    "    return curves,maxdam\n",
    "\n",
    "\n",
    "def overlay_hazard_assets(df_ds, assets):\n",
    "    \"\"\"\n",
    "    Overlay a set of assets with a hazard dataset and return the subset of assets that intersect with\n",
    "    one or more hazard polygons or lines.\n",
    "    \n",
    "    Args:\n",
    "        df_ds (GeoDataFrame): A GeoDataFrame containing the hazard dataset.\n",
    "        assets (GeoDataFrame): A GeoDataFrame containing the assets to be overlaid with the hazard dataset.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: A numpy array of integers representing the indices of the hazard geometries that intersect with\n",
    "            the assets. If the assets have a 'buffered' column, the buffered geometries are used for the overlay.\n",
    "    \"\"\"\n",
    "    hazard_tree = pygeos.STRtree(df_ds.geometry.values)\n",
    "    #if (pygeos.get_type_id(assets.iloc[0].geometry) == 3) | (pygeos.get_type_id(assets.iloc[0].geometry) == 6):\n",
    "    if len(assets) > 0:\n",
    "        if (pygeos.get_type_id(assets.iloc[0].geometry) == 3) or (pygeos.get_type_id(assets.iloc[0].geometry) == 6):\n",
    "            return hazard_tree.query_bulk(assets.geometry,predicate='intersects')    \n",
    "        else:\n",
    "            return hazard_tree.query_bulk(assets.buffered,predicate='intersects')\n",
    "    \n",
    "    else:\n",
    "        return hazard_tree.query_bulk(assets.buffered,predicate='intersects')\n",
    "    \n",
    "    \n",
    "def get_damage_per_asset_per_rp(asset,df_ds,assets,curves,maxdam,return_period,country):\n",
    "    \"\"\"\n",
    "    Calculates the damage per asset per return period based on asset type, hazard curves and maximum damage\n",
    "\n",
    "    Args:\n",
    "        asset (tuple): Tuple with two dictionaries, containing the asset index and the hazard point index of the asset\n",
    "        df_ds (pandas.DataFrame): A pandas DataFrame containing hazard points with a 'geometry' column\n",
    "        assets (geopandas.GeoDataFrame): A GeoDataFrame containing asset geometries and asset type information\n",
    "        curves (dict): A dictionary with the asset types as keys and their corresponding hazard curves as values\n",
    "        maxdam (pandas.DataFrame): A pandas DataFrame containing the maximum damage for each asset type\n",
    "        return_period (str): The return period for which the damage should be calculated\n",
    "        country (str): The country for which the damage should be calculated\n",
    "\n",
    "    Returns:\n",
    "        list or tuple: Depending on the input, the function either returns a list of tuples with the asset index, the curve name and the calculated damage, or a tuple with None,\n",
    "        None, None if no hazard points are found\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = df_ds.iloc[asset[1]['hazard_point'].values].reset_index()\n",
    "    get_hazard_points = get_hazard_points.loc[pygeos.intersects(get_hazard_points.geometry.values,assets.iloc[asset[0]].geometry)]\n",
    "\n",
    "    asset_type = assets.iloc[asset[0]].asset\n",
    "    asset_geom = assets.iloc[asset[0]].geometry\n",
    "\n",
    "    if asset_type in ['plant','substation','generator']:\n",
    "        # if plant,substation are points, do not calculate the area\n",
    "        if pygeos.area(asset_geom) == 0:\n",
    "            maxdam_asset = maxdam.loc[asset_type].MaxDam\n",
    "            lowerdam_asset = maxdam.loc[asset_type].LowerDam\n",
    "            upperdam_asset = maxdam.loc[asset_type].UpperDam\n",
    "        else:\n",
    "            maxdam_asset = maxdam.loc[asset_type].MaxDam/pygeos.area(asset_geom)\n",
    "            lowerdam_asset = maxdam.loc[asset_type].LowerDam/pygeos.area(asset_geom)\n",
    "            upperdam_asset = maxdam.loc[asset_type].UpperDam/pygeos.area(asset_geom)\n",
    "    else:\n",
    "        maxdam_asset = maxdam.loc[asset_type].MaxDam\n",
    "        lowerdam_asset = maxdam.loc[asset_type].LowerDam\n",
    "        upperdam_asset = maxdam.loc[asset_type].UpperDam\n",
    "\n",
    "    hazard_intensity = curves[asset_type].index.values\n",
    "    \n",
    "    if isinstance(curves[asset_type],pd.core.series.Series):\n",
    "        fragility_values = curves[asset_type].values.flatten()\n",
    "        only_one = True\n",
    "        curve_name = curves[asset_type].name\n",
    "    elif len(curves[asset_type].columns) == 1:\n",
    "        fragility_values = curves[asset_type].values.flatten()      \n",
    "        only_one = True   \n",
    "        curve_name = curves[asset_type].columns[0]\n",
    "    else:\n",
    "        fragility_values = curves[asset_type].values#.T[0]\n",
    "        maxdam_asset = maxdam_asset.values#[0]\n",
    "        only_one = False\n",
    "\n",
    "    if len(get_hazard_points) == 0:\n",
    "        if only_one:\n",
    "            return [return_period,asset[0],curve_name,0,0,0]\n",
    "        else:\n",
    "            return [return_period,asset[0],curves[asset_type].columns[0],0,0,0]\n",
    "            \n",
    "    else:\n",
    "        if only_one:    \n",
    "            # run the calculation as normal when the asset just has a single curve\n",
    "            if pygeos.get_type_id(asset_geom) == 1:            \n",
    "                get_hazard_points['overlay_meters'] = pygeos.length(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "                return [return_period,asset[0],curve_name,np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,\n",
    "                                                             fragility_values))*get_hazard_points.overlay_meters*maxdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,\n",
    "                                                             fragility_values))*get_hazard_points.overlay_meters*lowerdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,\n",
    "                                                             fragility_values))*get_hazard_points.overlay_meters*upperdam_asset)]\n",
    "\n",
    "            elif (pygeos.get_type_id(asset_geom) == 3) | (pygeos.get_type_id(asset_geom) == 6) :\n",
    "                get_hazard_points['overlay_m2'] = pygeos.area(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "                return [return_period,asset[0],curve_name,get_hazard_points.apply(lambda x: np.interp(x[return_period],hazard_intensity, \n",
    "                                                                  fragility_values)*maxdam_asset*x.overlay_m2,axis=1).sum(),\n",
    "                                                          get_hazard_points.apply(lambda x: np.interp(x[return_period],hazard_intensity, \n",
    "                                                                  fragility_values)*lowerdam_asset*x.overlay_m2,axis=1).sum(),\n",
    "                                                          get_hazard_points.apply(lambda x: np.interp(x[return_period],hazard_intensity, \n",
    "                                                                  fragility_values)*upperdam_asset*x.overlay_m2,axis=1).sum()]  \n",
    "\n",
    "            else:\n",
    "                return [return_period,asset[0],curve_name,np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                             hazard_intensity,fragility_values))*maxdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                             hazard_intensity,fragility_values))*lowerdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                             hazard_intensity,fragility_values))*upperdam_asset)]\n",
    "        else:\n",
    "            # run the calculation when the asset has multiple curves\n",
    "            if pygeos.get_type_id(asset_geom) == 1:            \n",
    "                get_hazard_points['overlay_meters'] = pygeos.length(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            elif (pygeos.get_type_id(asset_geom) == 3) | (pygeos.get_type_id(asset_geom) == 6) :\n",
    "                get_hazard_points['overlay_m2'] = pygeos.area(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            \n",
    "            collect_all = []\n",
    "            for iter_,curve_ids in enumerate(curves[asset_type].columns):\n",
    "                if pygeos.get_type_id(asset_geom) == 1:\n",
    "                    collect_all.append([return_period,asset[0],curves[asset_type].columns[iter_],\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*get_hazard_points.overlay_meters*maxdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*get_hazard_points.overlay_meters*lowerdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*get_hazard_points.overlay_meters*upperdam_asset[iter_])])\n",
    "                                   \n",
    "                elif (pygeos.get_type_id(asset_geom) == 3) | (pygeos.get_type_id(asset_geom) == 6) :\n",
    "                    collect_all.append([return_period,asset[0],curves[asset_type].columns[iter_],\n",
    "                                        get_hazard_points.apply(lambda x: np.interp(x[return_period], hazard_intensity,\n",
    "                                                                                    fragility_values.T[iter_])*maxdam_asset[iter_]*x.overlay_m2,axis=1).sum(),\n",
    "                                        get_hazard_points.apply(lambda x: np.interp(x[return_period], hazard_intensity,\n",
    "                                                                                    fragility_values.T[iter_])*lowerdam_asset[iter_]*x.overlay_m2,axis=1).sum(),\n",
    "                                        get_hazard_points.apply(lambda x: np.interp(x[return_period], hazard_intensity,\n",
    "                                                                                    fragility_values.T[iter_])*upperdam_asset[iter_]*x.overlay_m2,axis=1).sum()])\n",
    "\n",
    "                else:\n",
    "                    collect_all.append([return_period,asset[0],curves[asset_type].columns[iter_],\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*maxdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*lowerdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*upperdam_asset[iter_])])\n",
    "            return collect_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9896d9f-4592-4552-8876-c1340f3e09f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_storm_data(climate_model,basin,bbox):\n",
    "    \"\"\"\n",
    "    Load storm data from a NetCDF file and process it to return a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - climate_model (str): name of the climate model\n",
    "    - basin (str): name of the basin\n",
    "    - bbox (tuple): bounding box coordinates in the format (minx, miny, maxx, maxy)\n",
    "    - ne_crs (str): CRS string of the North-East projection\n",
    "\n",
    "    Returns:\n",
    "    - df_ds (pd.DataFrame): pandas DataFrame with interpolated wind speeds for different return periods and geometry column\n",
    "    \"\"\"\n",
    "\n",
    "    filename = os.path.join(tc_path, f'STORM_FIXED_RETURN_PERIODS{climate_model}_{basin}.nc')\n",
    "    \n",
    "    # load data from NetCDF file\n",
    "    with xr.open_dataset(filename) as ds:\n",
    "        \n",
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        ds = ds.rio.clip_box(minx=bbox[0], miny=bbox[1], maxx=bbox[2], maxy=bbox[3])\n",
    "        \n",
    "        #convert 10-min sustained wind speed to 3-s gust wind speed\n",
    "        ds['mean_3s'] = ds['mean']/0.88*1.11\n",
    "\n",
    "        # get the mean values\n",
    "        df_ds = ds['mean_3s'].to_dataframe().unstack(level=2).reset_index()\n",
    "\n",
    "        # create geometry values and drop lat lon columns\n",
    "        df_ds['geometry'] = [pygeos.points(x) for x in list(zip(df_ds['lon'], df_ds['lat']))]\n",
    "        df_ds = df_ds.drop(['lat', 'lon'], axis=1, level=0)\n",
    "        \n",
    "        # interpolate wind speeds of 1,2,5,25,and 250-yr return period\n",
    "        ## rename columns to return periods (must be integer for interpolating)\n",
    "        df_ds_geometry = pd.DataFrame()\n",
    "        df_ds_geometry['geometry'] = df_ds['geometry']\n",
    "        df_ds = df_ds.drop(['geometry'], axis=1, level=0)\n",
    "        df_ds = df_ds['mean_3s']\n",
    "        df_ds.columns = [int(x) for x in ds['mean_3s']['rp']]\n",
    "        df_ds[1] = np.nan\n",
    "        df_ds[2] = np.nan\n",
    "        df_ds[5] = np.nan\n",
    "        df_ds[25] = np.nan\n",
    "        df_ds[250] = np.nan\n",
    "        df_ds = df_ds.reindex(sorted(df_ds.columns), axis=1)\n",
    "        df_ds = df_ds.interpolate(method='pchip', axis=1, limit_direction='both')\n",
    "        df_ds['geometry'] = df_ds_geometry['geometry']\n",
    "        df_ds = df_ds[[1, 2, 5, 10, 25, 50, 100, 250, 500, 1000, 'geometry']]\n",
    "        \n",
    "        # rename columns to return periods\n",
    "        df_ds.columns = ['1_{}{}'.format(int(x), climate_model) for x in [1, 2, 5, 10, 25, 50, 100, 250, 500, 1000]] +['geometry']\n",
    "        df_ds['geometry'] = pygeos.buffer(df_ds.geometry, radius=0.1/2, cap_style='square').values\n",
    "        \n",
    "        # reproject the geometry column to the specified CRS\n",
    "        df_ds['geometry'] = reproject(df_ds)\n",
    "            \n",
    "        # drop all non values to reduce size\n",
    "        #df_ds = df_ds.loc[~df_ds['1_10000{}'.format(climate_model)].isna()].reset_index(drop=True)\n",
    "        df_ds = df_ds.fillna(0)\n",
    "\n",
    "    return df_ds\n",
    "\n",
    "def open_storm_data(country_code):\n",
    "    \"\"\"\n",
    "    This function loads STORM data for a given country code, clips it based on the country geometry,\n",
    "    and combines data from different basins and climate models.\n",
    "\n",
    "    Args:\n",
    "    - country_code (str): a 3-letter ISO code of the country of interest\n",
    "\n",
    "    Returns:\n",
    "    - df_ds (dict): a dictionary containing STORM data for different climate models, organized by basin\n",
    "    \"\"\"\n",
    "   \n",
    "    # list of available climate models\n",
    "    climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "\n",
    "    # dictionary of basins for each country\n",
    "    country_basin = {\n",
    "        \"BRN\": [\"WP\"],\n",
    "        \"KHM\": [\"WP\"],\n",
    "        \"CHN\": [\"WP\", \"NI\"],\n",
    "        \"IDN\": [\"SI\", \"SP\", \"NI\", \"WP\"],\n",
    "        \"JPN\": [\"WP\"],\n",
    "        \"LAO\": [\"WP\"],\n",
    "        \"MYS\": [\"WP\", \"NI\"],\n",
    "        \"MNG\": [\"WP\", \"NI\"],\n",
    "        \"MMR\": [\"NI\", \"WP\"],\n",
    "        \"PRK\": [\"WP\"],\n",
    "        \"PHL\": [\"WP\"],\n",
    "        \"SGP\": [\"WP\"],\n",
    "        \"KOR\": [\"WP\"],\n",
    "        \"TWN\": [\"WP\"],\n",
    "        \"THA\": [\"WP\", \"NI\"],\n",
    "        \"VNM\": [\"WP\"]\n",
    "    }\n",
    "\n",
    "    # load country geometry file and create geometry to clip\n",
    "    ne_countries = gpd.read_file(os.path.join(data_path,'..',\"natural_earth\",\"ne_10m_admin_0_countries.shp\"))\n",
    "    bbox = ne_countries.loc[ne_countries['ISO_A3']==country_code].geometry.buffer(1).values[0].bounds\n",
    "\n",
    "    df_ds = {}\n",
    "    for climate_model in climate_models:\n",
    "        concat_prep = []\n",
    "\n",
    "        #combine STORM data from different basins\n",
    "        if \"WP\" in country_basin[country_code]:\n",
    "            WP = load_storm_data(climate_model,'WP',bbox)\n",
    "            concat_prep.append(WP)\n",
    "        if \"SP\" in country_basin[country_code]:\n",
    "            SP = load_storm_data(climate_model,'SP',bbox)\n",
    "            concat_prep.append(SP)\n",
    "        if \"NI\" in country_basin[country_code]:            \n",
    "            NI = load_storm_data(climate_model,'NI',bbox)\n",
    "            concat_prep.append(NI)            \n",
    "        if \"SI\" in country_basin[country_code]:       \n",
    "            SI = load_storm_data(climate_model,'SI',bbox)\n",
    "            concat_prep.append(SI)            \n",
    "                   \n",
    "        df_ds_cl = pd.concat(concat_prep, keys=country_basin[country_code])\n",
    "        df_ds_cl = df_ds_cl.reset_index(drop=True)\n",
    "        df_ds[climate_model] = df_ds_cl\n",
    "\n",
    "    return df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db04d6f5-7835-4323-b55e-089b4d66d20a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_1</th>\n",
       "      <th>1_2</th>\n",
       "      <th>1_5</th>\n",
       "      <th>1_10</th>\n",
       "      <th>1_25</th>\n",
       "      <th>1_50</th>\n",
       "      <th>1_100</th>\n",
       "      <th>1_250</th>\n",
       "      <th>1_500</th>\n",
       "      <th>1_1000</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.059509</td>\n",
       "      <td>21.062296</td>\n",
       "      <td>21.070679</td>\n",
       "      <td>21.084724</td>\n",
       "      <td>21.127390</td>\n",
       "      <td>21.200173</td>\n",
       "      <td>21.351295</td>\n",
       "      <td>21.834707</td>\n",
       "      <td>22.644577</td>\n",
       "      <td>23.747100</td>\n",
       "      <td>POLYGON ((12913060.932 568480.588, 12913060.93...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.652051</td>\n",
       "      <td>24.643577</td>\n",
       "      <td>24.618305</td>\n",
       "      <td>24.576689</td>\n",
       "      <td>24.455587</td>\n",
       "      <td>24.266004</td>\n",
       "      <td>23.931081</td>\n",
       "      <td>23.246079</td>\n",
       "      <td>22.942404</td>\n",
       "      <td>23.698079</td>\n",
       "      <td>POLYGON ((12924192.881 568480.588, 12924192.88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.971906</td>\n",
       "      <td>28.944054</td>\n",
       "      <td>28.860980</td>\n",
       "      <td>28.724116</td>\n",
       "      <td>28.325357</td>\n",
       "      <td>27.699389</td>\n",
       "      <td>26.586265</td>\n",
       "      <td>24.236320</td>\n",
       "      <td>22.810727</td>\n",
       "      <td>23.568668</td>\n",
       "      <td>POLYGON ((12935324.83 568480.588, 12935324.83 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.745839</td>\n",
       "      <td>20.749080</td>\n",
       "      <td>20.758798</td>\n",
       "      <td>20.774989</td>\n",
       "      <td>20.823505</td>\n",
       "      <td>20.904165</td>\n",
       "      <td>21.064632</td>\n",
       "      <td>21.537098</td>\n",
       "      <td>22.280710</td>\n",
       "      <td>23.487972</td>\n",
       "      <td>POLYGON ((12946456.779 568480.588, 12946456.77...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.995612</td>\n",
       "      <td>32.956003</td>\n",
       "      <td>32.837771</td>\n",
       "      <td>32.642688</td>\n",
       "      <td>32.072085</td>\n",
       "      <td>31.169016</td>\n",
       "      <td>29.535995</td>\n",
       "      <td>25.888949</td>\n",
       "      <td>23.097848</td>\n",
       "      <td>23.450698</td>\n",
       "      <td>POLYGON ((12957588.728 568480.588, 12957588.72...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20291</th>\n",
       "      <td>45.053105</td>\n",
       "      <td>45.599223</td>\n",
       "      <td>47.249789</td>\n",
       "      <td>49.913732</td>\n",
       "      <td>55.108316</td>\n",
       "      <td>57.796014</td>\n",
       "      <td>60.283663</td>\n",
       "      <td>62.399877</td>\n",
       "      <td>63.633549</td>\n",
       "      <td>64.834125</td>\n",
       "      <td>POLYGON ((14170971.178 2535554.62, 14170971.17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20292</th>\n",
       "      <td>45.142617</td>\n",
       "      <td>45.684890</td>\n",
       "      <td>47.330674</td>\n",
       "      <td>49.999396</td>\n",
       "      <td>55.105428</td>\n",
       "      <td>58.178080</td>\n",
       "      <td>60.419198</td>\n",
       "      <td>62.713213</td>\n",
       "      <td>63.973463</td>\n",
       "      <td>64.905099</td>\n",
       "      <td>POLYGON ((14182103.127 2535554.62, 14182103.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20293</th>\n",
       "      <td>45.124814</td>\n",
       "      <td>45.671325</td>\n",
       "      <td>47.324048</td>\n",
       "      <td>49.993222</td>\n",
       "      <td>55.157299</td>\n",
       "      <td>58.405548</td>\n",
       "      <td>60.729709</td>\n",
       "      <td>62.975060</td>\n",
       "      <td>64.463318</td>\n",
       "      <td>65.869845</td>\n",
       "      <td>POLYGON ((14193235.076 2535554.62, 14193235.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20294</th>\n",
       "      <td>45.303961</td>\n",
       "      <td>45.839151</td>\n",
       "      <td>47.442258</td>\n",
       "      <td>50.004214</td>\n",
       "      <td>55.127359</td>\n",
       "      <td>58.350225</td>\n",
       "      <td>60.568494</td>\n",
       "      <td>63.581671</td>\n",
       "      <td>65.095541</td>\n",
       "      <td>66.598622</td>\n",
       "      <td>POLYGON ((14204367.025 2535554.62, 14204367.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20295</th>\n",
       "      <td>45.105078</td>\n",
       "      <td>45.651775</td>\n",
       "      <td>47.290368</td>\n",
       "      <td>49.910788</td>\n",
       "      <td>55.161009</td>\n",
       "      <td>58.198184</td>\n",
       "      <td>60.229218</td>\n",
       "      <td>63.675251</td>\n",
       "      <td>65.162814</td>\n",
       "      <td>66.966922</td>\n",
       "      <td>POLYGON ((14215498.974 2535554.62, 14215498.97...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20296 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1_1        1_2        1_5       1_10       1_25       1_50  \\\n",
       "0      21.059509  21.062296  21.070679  21.084724  21.127390  21.200173   \n",
       "1      24.652051  24.643577  24.618305  24.576689  24.455587  24.266004   \n",
       "2      28.971906  28.944054  28.860980  28.724116  28.325357  27.699389   \n",
       "3      20.745839  20.749080  20.758798  20.774989  20.823505  20.904165   \n",
       "4      32.995612  32.956003  32.837771  32.642688  32.072085  31.169016   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "20291  45.053105  45.599223  47.249789  49.913732  55.108316  57.796014   \n",
       "20292  45.142617  45.684890  47.330674  49.999396  55.105428  58.178080   \n",
       "20293  45.124814  45.671325  47.324048  49.993222  55.157299  58.405548   \n",
       "20294  45.303961  45.839151  47.442258  50.004214  55.127359  58.350225   \n",
       "20295  45.105078  45.651775  47.290368  49.910788  55.161009  58.198184   \n",
       "\n",
       "           1_100      1_250      1_500     1_1000  \\\n",
       "0      21.351295  21.834707  22.644577  23.747100   \n",
       "1      23.931081  23.246079  22.942404  23.698079   \n",
       "2      26.586265  24.236320  22.810727  23.568668   \n",
       "3      21.064632  21.537098  22.280710  23.487972   \n",
       "4      29.535995  25.888949  23.097848  23.450698   \n",
       "...          ...        ...        ...        ...   \n",
       "20291  60.283663  62.399877  63.633549  64.834125   \n",
       "20292  60.419198  62.713213  63.973463  64.905099   \n",
       "20293  60.729709  62.975060  64.463318  65.869845   \n",
       "20294  60.568494  63.581671  65.095541  66.598622   \n",
       "20295  60.229218  63.675251  65.162814  66.966922   \n",
       "\n",
       "                                                geometry  \n",
       "0      POLYGON ((12913060.932 568480.588, 12913060.93...  \n",
       "1      POLYGON ((12924192.881 568480.588, 12924192.88...  \n",
       "2      POLYGON ((12935324.83 568480.588, 12935324.83 ...  \n",
       "3      POLYGON ((12946456.779 568480.588, 12946456.77...  \n",
       "4      POLYGON ((12957588.728 568480.588, 12957588.72...  \n",
       "...                                                  ...  \n",
       "20291  POLYGON ((14170971.178 2535554.62, 14170971.17...  \n",
       "20292  POLYGON ((14182103.127 2535554.62, 14182103.12...  \n",
       "20293  POLYGON ((14193235.076 2535554.62, 14193235.07...  \n",
       "20294  POLYGON ((14204367.025 2535554.62, 14204367.02...  \n",
       "20295  POLYGON ((14215498.974 2535554.62, 14215498.97...  \n",
       "\n",
       "[20296 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tc = open_storm_data('PHL')\n",
    "df_tc['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c7ff6dd-a3f2-4e2a-becd-e5cffa38edde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clip_flood_data(country_code):\n",
    "    \"\"\"\n",
    "    Clip global flood data for a specific country.\n",
    "\n",
    "    Args:\n",
    "        country_code (str): Country code for the desired country.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # load country geometry file and create geometry to clip\n",
    "    ne_countries = gpd.read_file(ne_path)\n",
    "    geometry = ne_countries.loc[ne_countries['ISO_A3']==country_code].geometry.values[0]\n",
    "    geoms = [mapping(geometry)]\n",
    "    \n",
    "    rps = ['0001','0002','0005','0010','0025','0050','0100','0250','0500','1000']\n",
    "    climate_models = ['historical','rcp8p5']\n",
    "    \n",
    "    for rp in rps:\n",
    "        #global input_file\n",
    "        for climate_model in climate_models:\n",
    "            if climate_model=='historical':\n",
    "                input_file = os.path.join(fl_path,'global',\n",
    "                                          'inuncoast_{}_nosub_hist_rp{}_0.tif'.format(climate_model,rp)) \n",
    " \n",
    "            elif climate_model=='rcp8p5':\n",
    "                input_file = os.path.join(fl_path,'global',\n",
    "                                          'inuncoast_{}_nosub_2030_rp{}_0.tif'.format(climate_model,rp))\n",
    "\n",
    "            # load raster file and save clipped version\n",
    "            with rasterio.open(input_file) as src:\n",
    "                out_image, out_transform = mask(src, geoms, crop=True)\n",
    "                out_meta = src.meta\n",
    "\n",
    "                out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "\n",
    "                if 'scistor' in fl_path:\n",
    "                    file_path = os.path.join(fl_path,'country','_'.join([country_code]+input_file.split('_')[6:]))\n",
    "                else:\n",
    "                    file_path = os.path.join(fl_path,'country','_'.join([country_code]+input_file.split('_')[3:]))\n",
    "\n",
    "                with rasterio.open(file_path, \"w\", **out_meta) as dest:\n",
    "                    dest.write(out_image)\n",
    "\n",
    "def load_flood_data(country_code,climate_model):\n",
    "    \"\"\"\n",
    "    Load flood data for a specific country and climate model.\n",
    "\n",
    "    Args:\n",
    "        country_code (str): Country code for the desired country.\n",
    "        climate_model (str): Climate model ('historical' or 'rcp8p5').\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Flood data for the specified country and climate model.\n",
    "    \"\"\"\n",
    "     \n",
    "    rps = ['0001','0002','0005','0010','0025','0050','0100','0250','0500','1000']\n",
    "    collect_df_ds = []\n",
    "    \n",
    "    if climate_model=='historical':\n",
    "        print('Loading historical coastal flood data ...')\n",
    "        for rp in rps:\n",
    "            #for file in files:\n",
    "            file_path = os.path.join(fl_path,'country','{}_{}_nosub_hist_rp{}_0.tif'.format(country_code,climate_model,rp))\n",
    "            with xr.open_dataset(file_path) as ds: #, engine=\"rasterio\"\n",
    "                df_ds = ds.to_dataframe().reset_index()\n",
    "                df_ds['geometry'] = pygeos.points(df_ds.x,y=df_ds.y)\n",
    "                df_ds = df_ds.rename(columns={'band_data': 'rp'+rp}) #rename to return period\n",
    "                \n",
    "                # move from meters to centimeters\n",
    "                df_ds['rp'+rp] = (df_ds['rp'+rp]*100)         \n",
    "                df_ds = df_ds.drop(['band','x', 'y','spatial_ref'], axis=1)\n",
    "                df_ds = df_ds.dropna()\n",
    "                df_ds = df_ds.reset_index(drop=True)\n",
    "                df_ds.geometry= pygeos.buffer(df_ds.geometry,radius=0.0089932/2,cap_style='square').values  # the original value here is 0.00833???\n",
    "                df_ds['geometry'] = reproject(df_ds)\n",
    "                collect_df_ds.append(df_ds)\n",
    "\n",
    "        df_all = collect_df_ds[0].merge(collect_df_ds[1]).merge(collect_df_ds[2]).merge(collect_df_ds[3]).merge(collect_df_ds[4])\\\n",
    "                 .merge(collect_df_ds[5]).merge(collect_df_ds[6]).merge(collect_df_ds[7]).merge(collect_df_ds[8]).merge(collect_df_ds[9])\n",
    "        df_all = df_all.loc[df_all['rp1000']>0].reset_index(drop=True)\n",
    "\n",
    "    elif climate_model=='rcp8p5':\n",
    "        print('Loading future coastal flood data ...')\n",
    "        for rp in rps:\n",
    "            #for file in files:\n",
    "            file_path = os.path.join(fl_path,'country','{}_{}_nosub_2030_rp{}_0.tif'.format(country_code,climate_model,rp))\n",
    "            with xr.open_dataset(file_path) as ds: #, engine=\"rasterio\"\n",
    "                df_ds = ds.to_dataframe().reset_index()\n",
    "                df_ds['geometry'] = pygeos.points(df_ds.x,y=df_ds.y)\n",
    "                df_ds = df_ds.rename(columns={'band_data': 'rp'+rp}) #rename to return period\n",
    "                df_ds['rp'+rp] = (df_ds['rp'+rp]*100)\n",
    "                df_ds = df_ds.drop(['band','x', 'y','spatial_ref'], axis=1)\n",
    "                df_ds = df_ds.dropna()\n",
    "                df_ds = df_ds.reset_index(drop=True)\n",
    "                df_ds.geometry= pygeos.buffer(df_ds.geometry,radius=0.00833/2,cap_style='square').values\n",
    "                df_ds['geometry'] = reproject(df_ds)\n",
    "                collect_df_ds.append(df_ds)\n",
    "\n",
    "        df_all = collect_df_ds[0].merge(collect_df_ds[1]).merge(collect_df_ds[2]).merge(collect_df_ds[3]).merge(collect_df_ds[4])\\\n",
    "                 .merge(collect_df_ds[5]).merge(collect_df_ds[6]).merge(collect_df_ds[7]).merge(collect_df_ds[8]).merge(collect_df_ds[9])\n",
    "\n",
    "        df_all = df_all.loc[df_all['rp1000']>0].reset_index(drop=True)\n",
    "    return df_all\n",
    "\n",
    "def open_flood_data(country_code):\n",
    "    \"\"\"\n",
    "    Open and load flood data for a specific country.\n",
    "\n",
    "    Args:\n",
    "        country_code (str): Country code for the desired country.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the loaded flood data for different climate models.\n",
    "    \"\"\"\n",
    "    \n",
    "    climate_models = ['historical','rcp8p5']\n",
    "    df_ds = {}\n",
    "    for climate_model in climate_models:\n",
    "        df_ds_sc = load_flood_data(country_code,climate_model)\n",
    "        df_ds[climate_model] = df_ds_sc\n",
    "    \n",
    "    return df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecac24e1-8b8b-42a6-9471-1f3d19681549",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rp0001</th>\n",
       "      <th>geometry</th>\n",
       "      <th>rp0002</th>\n",
       "      <th>rp0005</th>\n",
       "      <th>rp0010</th>\n",
       "      <th>rp0025</th>\n",
       "      <th>rp0050</th>\n",
       "      <th>rp0100</th>\n",
       "      <th>rp0250</th>\n",
       "      <th>rp0500</th>\n",
       "      <th>rp1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>POLYGON ((13020706.501 892564.07, 13020706.501...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.009700</td>\n",
       "      <td>8.493829</td>\n",
       "      <td>12.944865</td>\n",
       "      <td>18.805336</td>\n",
       "      <td>23.230480</td>\n",
       "      <td>27.652431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>POLYGON ((13020706.501 891627.319, 13020706.50...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116062</td>\n",
       "      <td>4.538012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.241997</td>\n",
       "      <td>POLYGON ((13020706.501 890690.587, 13020706.50...</td>\n",
       "      <td>49.178028</td>\n",
       "      <td>56.403519</td>\n",
       "      <td>61.187408</td>\n",
       "      <td>67.231873</td>\n",
       "      <td>71.715996</td>\n",
       "      <td>76.167038</td>\n",
       "      <td>82.027504</td>\n",
       "      <td>86.452652</td>\n",
       "      <td>90.874603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44.351685</td>\n",
       "      <td>POLYGON ((13020706.501 889753.874, 13020706.50...</td>\n",
       "      <td>47.287716</td>\n",
       "      <td>54.513203</td>\n",
       "      <td>59.297096</td>\n",
       "      <td>65.341553</td>\n",
       "      <td>69.825684</td>\n",
       "      <td>74.276718</td>\n",
       "      <td>80.137192</td>\n",
       "      <td>84.562340</td>\n",
       "      <td>88.984283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>POLYGON ((13020706.501 887880.505, 13020706.50...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.245019</td>\n",
       "      <td>10.289478</td>\n",
       "      <td>14.773607</td>\n",
       "      <td>19.224644</td>\n",
       "      <td>25.085115</td>\n",
       "      <td>29.510260</td>\n",
       "      <td>33.932209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4447</th>\n",
       "      <td>52.176750</td>\n",
       "      <td>POLYGON ((14068965.039 926300.011, 14068965.03...</td>\n",
       "      <td>53.190361</td>\n",
       "      <td>55.684914</td>\n",
       "      <td>57.336510</td>\n",
       "      <td>59.423317</td>\n",
       "      <td>60.971439</td>\n",
       "      <td>62.508095</td>\n",
       "      <td>64.531410</td>\n",
       "      <td>66.059151</td>\n",
       "      <td>67.585770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4448</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>POLYGON ((14068965.039 925362.558, 14068965.03...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.981641</td>\n",
       "      <td>3.004956</td>\n",
       "      <td>4.532695</td>\n",
       "      <td>6.059313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4449</th>\n",
       "      <td>7.122862</td>\n",
       "      <td>POLYGON ((14089373.613 827036.51, 14089373.613...</td>\n",
       "      <td>8.115935</td>\n",
       "      <td>10.559845</td>\n",
       "      <td>12.177944</td>\n",
       "      <td>14.222383</td>\n",
       "      <td>15.739083</td>\n",
       "      <td>17.244577</td>\n",
       "      <td>19.226789</td>\n",
       "      <td>20.723534</td>\n",
       "      <td>22.219181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450</th>\n",
       "      <td>11.667931</td>\n",
       "      <td>POLYGON ((14090301.275 860725.127, 14090301.27...</td>\n",
       "      <td>12.681543</td>\n",
       "      <td>15.176094</td>\n",
       "      <td>16.827690</td>\n",
       "      <td>18.914497</td>\n",
       "      <td>20.462620</td>\n",
       "      <td>21.999275</td>\n",
       "      <td>24.022591</td>\n",
       "      <td>25.550329</td>\n",
       "      <td>27.076948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4451</th>\n",
       "      <td>82.730499</td>\n",
       "      <td>POLYGON ((14090301.275 820488.638, 14090301.27...</td>\n",
       "      <td>83.723572</td>\n",
       "      <td>86.167480</td>\n",
       "      <td>87.785576</td>\n",
       "      <td>89.830017</td>\n",
       "      <td>91.346718</td>\n",
       "      <td>92.852211</td>\n",
       "      <td>94.834427</td>\n",
       "      <td>96.331169</td>\n",
       "      <td>97.826813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4452 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         rp0001                                           geometry     rp0002  \\\n",
       "0      0.000000  POLYGON ((13020706.501 892564.07, 13020706.501...   0.000000   \n",
       "1      0.000000  POLYGON ((13020706.501 891627.319, 13020706.50...   0.000000   \n",
       "2     46.241997  POLYGON ((13020706.501 890690.587, 13020706.50...  49.178028   \n",
       "3     44.351685  POLYGON ((13020706.501 889753.874, 13020706.50...  47.287716   \n",
       "4      0.000000  POLYGON ((13020706.501 887880.505, 13020706.50...   0.000000   \n",
       "...         ...                                                ...        ...   \n",
       "4447  52.176750  POLYGON ((14068965.039 926300.011, 14068965.03...  53.190361   \n",
       "4448   0.000000  POLYGON ((14068965.039 925362.558, 14068965.03...   0.000000   \n",
       "4449   7.122862  POLYGON ((14089373.613 827036.51, 14089373.613...   8.115935   \n",
       "4450  11.667931  POLYGON ((14090301.275 860725.127, 14090301.27...  12.681543   \n",
       "4451  82.730499  POLYGON ((14090301.275 820488.638, 14090301.27...  83.723572   \n",
       "\n",
       "         rp0005     rp0010     rp0025     rp0050     rp0100     rp0250  \\\n",
       "0      0.000000   0.000000   4.009700   8.493829  12.944865  18.805336   \n",
       "1      0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2     56.403519  61.187408  67.231873  71.715996  76.167038  82.027504   \n",
       "3     54.513203  59.297096  65.341553  69.825684  74.276718  80.137192   \n",
       "4      0.000000   4.245019  10.289478  14.773607  19.224644  25.085115   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "4447  55.684914  57.336510  59.423317  60.971439  62.508095  64.531410   \n",
       "4448   0.000000   0.000000   0.000000   0.000000   0.981641   3.004956   \n",
       "4449  10.559845  12.177944  14.222383  15.739083  17.244577  19.226789   \n",
       "4450  15.176094  16.827690  18.914497  20.462620  21.999275  24.022591   \n",
       "4451  86.167480  87.785576  89.830017  91.346718  92.852211  94.834427   \n",
       "\n",
       "         rp0500     rp1000  \n",
       "0     23.230480  27.652431  \n",
       "1      0.116062   4.538012  \n",
       "2     86.452652  90.874603  \n",
       "3     84.562340  88.984283  \n",
       "4     29.510260  33.932209  \n",
       "...         ...        ...  \n",
       "4447  66.059151  67.585770  \n",
       "4448   4.532695   6.059313  \n",
       "4449  20.723534  22.219181  \n",
       "4450  25.550329  27.076948  \n",
       "4451  96.331169  97.826813  \n",
       "\n",
       "[4452 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fl = open_flood_data('PHL')\n",
    "df_fl['historical']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3de688-606c-4dd2-abe3-f37015c7442e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OSM data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "934161b6-5f01-4315-87b3-9e4a7bd70876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_osm_infrastructure(country_code,osm_data_path):\n",
    "    \"\"\"\n",
    "    Extract OSM (OpenStreetMap) infrastructure data for a specific country.\n",
    "\n",
    "    Args:\n",
    "        country_code (str): Country code for the desired country.\n",
    "        osm_data_path (str): Path to the OSM data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three pandas DataFrames:\n",
    "            - osm_lines: OSM infrastructure lines data.\n",
    "            - osm_polygons: OSM infrastructure polygons data.\n",
    "            - osm_points: OSM infrastructure points data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # lines\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    osm_lines = power_polyline(osm_path)\n",
    "    osm_lines['geometry'] = reproject(osm_lines)\n",
    "    osm_lines = buffer_assets(osm_lines.loc[osm_lines.asset.isin(\n",
    "        ['cable','minor_cable','line','minor_line'])],buffer_size=100).reset_index(drop=True)\n",
    "    \n",
    "    # polygons\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    osm_polygons = power_polygon(osm_path)\n",
    "    osm_polygons['geometry'] = reproject(osm_polygons)\n",
    "    \n",
    "    # points\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    osm_points = power_point(osm_path)\n",
    "    osm_points['geometry'] = reproject(osm_points)\n",
    "    osm_points = buffer_assets(osm_points.loc[osm_points.asset.isin(\n",
    "        ['power_tower','power_pole'])],buffer_size=100).reset_index(drop=True)\n",
    "    \n",
    "    return osm_lines,osm_polygons,osm_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de54851e-4aaa-4e38-88fc-a27efb33c918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_damage_osm(country_code,osm_power_infra,hazard_type): #NEW VERSION\n",
    "    \"\"\"\n",
    "    Assess the damage to OSM (OpenStreetMap) infrastructure for a specific country and hazard type.\n",
    "\n",
    "    Args:\n",
    "        country_code (str): Country code for the desired country.\n",
    "        osm_power_infra (tuple): A tuple containing three pandas DataFrames:\n",
    "            - osm_lines: OSM infrastructure lines data.\n",
    "            - osm_polygons: OSM infrastructure polygons data.\n",
    "            - osm_points: OSM infrastructure points data.\n",
    "        hazard_type (str): Type of hazard ('tc' for tropical cyclone or 'fl' for coastal flooding).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three pandas DataFrames:\n",
    "            - damaged_lines: Damage assessment results for OSM infrastructure lines.\n",
    "            - damaged_poly: Damage assessment results for OSM infrastructure polygons.\n",
    "            - damaged_points: Damage assessment results for OSM infrastructure points.\n",
    "    \"\"\"\n",
    "    \n",
    "    # load curves and maxdam\n",
    "    curves,maxdam = load_curves_maxdam(country_code,vul_curve_path,hazard_type)\n",
    "    \n",
    "    # read infrastructure data:\n",
    "    osm_lines,osm_poly,osm_points = osm_power_infra\n",
    "    \n",
    "    #calculate damaged lines/polygons/points in loop by climate_model\n",
    "    damaged_lines = {}\n",
    "    damaged_poly = {}\n",
    "    damaged_points = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        # read wind data\n",
    "        climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "        df_ds = open_storm_data(country_code)\n",
    "\n",
    "        # remove assets that will not have any damage\n",
    "        osm_lines = osm_lines.loc[osm_lines.asset != 'cable'].reset_index(drop=True)\n",
    "        osm_lines['asset'] = osm_lines['asset'].replace(['minor_line'], 'line')\n",
    "        osm_poly = osm_poly.loc[osm_poly.asset != 'plant'].reset_index(drop=True)            \n",
    "    \n",
    "    elif hazard_type=='fl':\n",
    "        # read flood data\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "        df_ds = open_flood_data(country_code)\n",
    "        \n",
    "    for climate_model in climate_models:\n",
    "        if hazard_type=='tc':\n",
    "            return_periods = ['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),'1_10{}'.format(climate_model),\n",
    "                              '1_25{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),\n",
    "                              '1_250{}'.format(climate_model),'1_500{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "        elif hazard_type == 'fl':\n",
    "            return_periods = ['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000']     \n",
    "    \n",
    "        # assess damage for lines\n",
    "        overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_lines).T,\n",
    "                                     columns=['asset','hazard_point'])\n",
    "\n",
    "        if len(overlay_lines) == 0:\n",
    "            damaged_lines[climate_model] = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            collect_line_damages = []\n",
    "            for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                              desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_line_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                            df_ds[climate_model],\n",
    "                                                                            osm_lines,\n",
    "                                                                            curves,\n",
    "                                                                            maxdam,\n",
    "                                                                            return_period,\n",
    "                                                                            country_code))\n",
    "\n",
    "            get_asset_type_line = dict(zip(osm_lines.index,osm_lines.asset))\n",
    "            \n",
    "            if hazard_type == 'tc':\n",
    "                results = pd.DataFrame([item for sublist in collect_line_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            elif hazard_type == 'fl':\n",
    "                #results = pd.DataFrame(collect_line_damages,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                results = pd.DataFrame(np.array(list(flatten(collect_line_damages))).reshape(\n",
    "                    int(len(list(flatten(collect_line_damages)))/6), 6),\n",
    "                                       columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                results['asset'] = results['asset'].astype(int)\n",
    "                results[['meandam','lowerdam','upperdam']] = results[['meandam','lowerdam','upperdam']].astype(float)\n",
    "                \n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                #sum damage of line, cable, and minor_line\n",
    "                results['curve'] = results['curve'].replace(['cable', 'minor_line'], 'line')\n",
    "                results['asset_type'] = results['asset_type'].replace(['cable', 'minor_line'], 'line')\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "                \n",
    "        # assess damage for polygons\n",
    "        if len(osm_poly) > 0:\n",
    "            overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_poly).T,\n",
    "                                    columns=['asset','hazard_point'])\n",
    "        else:\n",
    "            overlay_poly = pd.DataFrame()\n",
    "\n",
    "        if len(overlay_poly) == 0:\n",
    "            damaged_poly[climate_model] = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            collect_poly_damages = []\n",
    "            for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),\n",
    "                              desc='polygon damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_poly_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                            df_ds[climate_model],\n",
    "                                                                            osm_poly,\n",
    "                                                                            curves,\n",
    "                                                                            maxdam,\n",
    "                                                                            return_period,\n",
    "                                                                            country_code))\n",
    "\n",
    "            get_asset_type_poly = dict(zip(osm_poly.index,osm_poly.asset))\n",
    "            \n",
    "            results = pd.DataFrame([item for sublist in collect_poly_damages \n",
    "                                    for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "            results['asset_type'] = results.asset.apply(lambda x : get_asset_type_poly[x])    \n",
    "\n",
    "            damaged_poly[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "                \n",
    "\n",
    "        #assess damage for points\n",
    "        overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_points).T,\n",
    "                                      columns=['asset','hazard_point'])\n",
    "\n",
    "        if len(overlay_points) == 0:\n",
    "            damaged_points[climate_model] = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            collect_point_damages = []\n",
    "            for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                              desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    \n",
    "                    # check_error = get_damage_per_asset_per_rp(asset,df_ds[climate_model],osm_points,curves,maxdam,return_period,country_code)\n",
    "                    # with open(os.path.join(output_path,'get_damage_per_asset_per_rp_{}_{}.pkl'.format(country_code,climate_model)), 'wb') as f:\n",
    "                    #     pickle.dump(check_error,f)\n",
    "\n",
    "                    collect_point_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                            df_ds[climate_model],\n",
    "                                                                            osm_points,\n",
    "                                                                            curves,\n",
    "                                                                            maxdam,\n",
    "                                                                            return_period,\n",
    "                                                                            country_code))\n",
    "\n",
    "            get_asset_type_point = dict(zip(osm_points.index,osm_points.asset))\n",
    "            \n",
    "            if hazard_type == 'tc':\n",
    "                #catch and remove integers in the sublists of collect_point_damages\n",
    "                collect_point_damages = [[item for item in sublist if not isinstance(item, int)] for sublist in collect_point_damages]\n",
    "                collect_point_damages = [[item for item in sublist if len(item) == 6] for sublist in collect_point_damages]\n",
    "\n",
    "                # with open(os.path.join(output_path,'collect_point_damages_{}_{}.pkl'.format(country_code,climate_model)), 'wb') as f:\n",
    "                #     pickle.dump(collect_point_damages,f)\n",
    "\n",
    "                results = pd.DataFrame([item for sublist in collect_point_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results.drop(results.index[(results.iloc[:, 1] == '_')], inplace=True)\n",
    "                results.drop(results.index[(results.iloc[:, 1] == '3')], inplace=True)\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])\n",
    "                \n",
    "                damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "            \n",
    "            elif hazard_type == 'fl':\n",
    "                results = pd.DataFrame(np.array(list(flatten(collect_point_damages))).reshape(\n",
    "                    int(len(list(flatten(collect_point_damages)))/6), 6),\n",
    "                                       columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                results['asset'] = results['asset'].astype(int)\n",
    "                results[['meandam','lowerdam','upperdam']] = results[['meandam','lowerdam','upperdam']].astype(float)\n",
    "                \n",
    "                #return collect_point_damages,get_asset_type_point\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])    \n",
    "\n",
    "                damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "    return damaged_lines,damaged_poly,damaged_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fac009b3-9c27-402f-aa2a-bbf38fc2d819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8145/8145 [00:34<00:00, 234.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [02:49<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 440360/440360 [00:46<00:00, 9468.84it/s]\n"
     ]
    }
   ],
   "source": [
    "osm_power_infra = extract_osm_infrastructure('PHL',osm_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74247d6b-2ece-4100-8154-c6e02d93bc17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio_usa for PHL is 0.0547\n",
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for PHL fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146/146 [00:19<00:00,  7.38it/s]\n",
      "polygon damage calculation for PHL fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  7.15it/s]\n",
      "point damage calculation for PHL fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:42<00:00, 20.53it/s]\n",
      "polyline damage calculation for PHL fl (rcp8p5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:18<00:00,  7.12it/s]\n",
      "polygon damage calculation for PHL fl (rcp8p5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  7.12it/s]\n",
      "point damage calculation for PHL fl (rcp8p5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:41<00:00, 21.14it/s]\n"
     ]
    }
   ],
   "source": [
    "phl_osm_damage = assess_damage_osm('PHL',osm_power_infra,'fl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e56c1d61-40b5-4892-ac70-c20c522eb661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def country_analysis_osm(country_code,hazard_type):\n",
    "    \"\"\"\n",
    "    Perform country analysis for OSM (OpenStreetMap) infrastructure and assess damage.\n",
    "\n",
    "    Args:\n",
    "        country_code (str): Country code for the desired country.\n",
    "        hazard_type (str): Type of hazard ('tc' for tropical cyclone or 'fl' for coastal flooding).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing risk assessments for different OSM infrastructure types.\n",
    "    \"\"\"\n",
    "    \n",
    "        # extract infrastructure data from OSM\n",
    "    osm_power_infra = extract_osm_infrastructure(country_code,osm_data_path)\n",
    "    \n",
    "    # assess damage to hazard_type\n",
    "    osm_damage_infra = assess_damage_osm(country_code,osm_power_infra,hazard_type)\n",
    "    \n",
    "    line_risk = {}\n",
    "    plant_risk = {}\n",
    "    substation_risk = {}\n",
    "    tower_risk = {}\n",
    "    pole_risk = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "\n",
    "        for i in range(len(osm_damage_infra)):\n",
    "            for climate_model in climate_models:\n",
    "                df = osm_damage_infra[i][climate_model]\n",
    "\n",
    "                if len(df) == 0:\n",
    "                    print(\"No {}_{} risk of infra_type {} in {}\".format(hazard_type,climate_model,i,country_code))\n",
    "\n",
    "                else:\n",
    "                    with pd.ExcelWriter(os.path.join(output_path,'damage','{}_osm_{}{}_damage_{}'.format(country_code,hazard_type,climate_model,i)+'.xlsx')) as writer:\n",
    "                        df.to_excel(writer)\n",
    "\n",
    "                    df['rp'] = df['rp'].replace(['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),\n",
    "                                                '1_10{}'.format(climate_model),'1_25{}'.format(climate_model),'1_50{}'.format(climate_model),\n",
    "                                                '1_100{}'.format(climate_model),'1_250{}'.format(climate_model),'1_500{}'.format(climate_model),\n",
    "                                                '1_1000{}'.format(climate_model)],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "\n",
    "                    curve_code_substation = ['W2_1_1','W2_1_2','W2_1_3','W2_1_4','W2_1_5','W2_1_6','W2_2_1','W2_2_2','W2_2_3','W2_2_4','W2_2_5','W2_2_6',\n",
    "                                             'W2_3_1','W2_3_2','W2_3_3','W2_3_4','W2_3_5','W2_3_6','W2_4_1','W2_4_2','W2_4_3','W2_4_4','W2_4_5','W2_4_6',\n",
    "                                             'W2_5_1','W2_5_2','W2_5_3','W2_5_4','W2_5_5','W2_5_6','W2_6_1','W2_6_2','W2_6_3','W2_6_4','W2_6_5','W2_6_6',\n",
    "                                             'W2_7_1','W2_7_2','W2_7_3','W2_7_4','W2_7_5','W2_7_6']\n",
    "\n",
    "\n",
    "                    curve_code_tower = ['W3_1','W3_2','W3_3','W3_4','W3_5','W3_6','W3_7','W3_8','W3_9','W3_10','W3_11','W3_12','W3_13','W3_14','W3_15',\n",
    "                                        'W3_16','W3_17','W3_18','W3_19','W3_20','W3_21','W3_22','W3_23','W3_24','W3_25','W3_26','W3_27','W3_28']\n",
    "\n",
    "                    curve_code_pole = ['W4_1','W4_2','W4_3','W4_4','W4_5','W4_6','W4_7','W4_8','W4_9','W4_10','W4_11','W4_12',\n",
    "                                       'W4_13','W4_14','W4_15','W4_16','W4_17','W4_18','W4_19','W4_20','W4_21','W4_22','W4_23',\n",
    "                                       'W4_24','W4_25','W4_26','W4_27','W4_28','W4_29','W4_30','W4_31','W4_32','W4_33','W4_34',\n",
    "                                       'W4_35','W4_36','W4_37','W4_38','W4_39','W4_40','W4_41','W4_42','W4_43','W4_44','W4_45',\n",
    "                                       'W4_46','W4_47','W4_48','W4_49','W4_50','W4_51','W4_52','W4_53','W4_54','W4_55','W4_56']\n",
    "\n",
    "                    curve_code_line = ['W5_1_1','W5_1_2','W5_1_3','W5_1_4','W5_1_5','W5_1_6','W5_1_7','W5_1_8','W5_1_9','W5_1_10','W5_1_11','W5_1_12',\n",
    "                                       'W5_2_1','W5_2_2','W5_2_3','W5_2_4','W5_2_5','W5_2_6','W5_2_7','W5_2_8','W5_2_9','W5_2_10','W5_2_11','W5_2_12',\n",
    "                                       'W5_3_1','W5_3_2','W5_3_3','W5_3_4','W5_3_5','W5_3_6','W5_3_7','W5_3_8','W5_3_9','W5_3_10','W5_3_11','W5_3_12',\n",
    "                                       'W5_4_1','W5_4_2','W5_4_3','W5_4_4','W5_4_5','W5_4_6','W5_4_7','W5_4_8','W5_4_9','W5_4_10','W5_4_11','W5_4_12',\n",
    "                                       'W5_5_1','W5_5_2','W5_5_3','W5_5_4','W5_5_5','W5_5_6','W5_5_7','W5_5_8','W5_5_9','W5_5_10','W5_5_11','W5_5_12',\n",
    "                                       'W5_6_1','W5_6_2','W5_6_3','W5_6_4','W5_6_5','W5_6_6','W5_6_7','W5_6_8','W5_6_9','W5_6_10','W5_6_11','W5_6_12',\n",
    "                                       'W5_7_1','W5_7_2','W5_7_3','W5_7_4','W5_7_5','W5_7_6','W5_7_7','W5_7_8','W5_7_9','W5_7_10','W5_7_11','W5_7_12']\n",
    "\n",
    "                    #assess risk for power lines\n",
    "                    if i == 0:\n",
    "                        for curve_code in curve_code_line:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power lines ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                line_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "\n",
    "                    #assess risk for power substations                \n",
    "                    elif i == 1:                        \n",
    "                        for curve_code in curve_code_substation:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of substations ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                substation_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "\n",
    "                    #assess risk for power towers and power poles\n",
    "                    elif i == 2:\n",
    "                        for curve_code in curve_code_tower:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power towers ...\")\n",
    "\n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                tower_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "\n",
    "                        for curve_code in curve_code_pole:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power poles ...\")\n",
    "\n",
    "                            else:                    \n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                pole_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "\n",
    "    elif hazard_type=='fl':\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "    \n",
    "        for i in range(len(osm_damage_infra)):\n",
    "            for climate_model in climate_models:\n",
    "                df = osm_damage_infra[i][climate_model]\n",
    "                    \n",
    "                if len(df) == 0:\n",
    "                    print(\"No {}_{} risk of infra_type {} in {}\".format(hazard_type,climate_model,i,country_code))\n",
    "\n",
    "                else:\n",
    "                    with pd.ExcelWriter(os.path.join(output_path,'damage','{}_osm_{}_{}_damage_{}'.format(country_code,hazard_type,climate_model,i)+'.xlsx')) as writer:\n",
    "                        df.to_excel(writer)\n",
    "\n",
    "                    df['rp'] = df['rp'].replace(['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "                    \n",
    "                    curve_code_plant = ['F1_1_1','F1_1_2','F1_1_3']\n",
    "                    curve_code_substation = ['F2_1_1','F2_1_2','F2_1_3']\n",
    "                    curve_code_tower = ['F3_1_1','F3_1_2']\n",
    "                    curve_code_pole = ['F4_1_1','F4_1_2','F4_1_3','F4_1_4']\n",
    "                    curve_code_line = ['F5_1_1','F5_1_2','F5_1_3','F5_1_4','F5_1_5','F5_1_6','F5_1_7','F5_1_8',\n",
    "                                      'F5_1_9','F5_1_10','F5_1_11','F5_1_12']\n",
    "                    curve_code_minor_line = ['F5_2']\n",
    "                    curve_code_cable = ['F5_3_1','F5_3_2']\n",
    "\n",
    "                    #assess risk for power lines\n",
    "                    if i == 0:\n",
    "                        for curve_code in curve_code_line:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power lines ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                line_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "\n",
    "                    #assess risk for power plants and substations                \n",
    "                    elif i == 1:\n",
    "                        for curve_code in curve_code_plant:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of plants ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                plant_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "\n",
    "                        for curve_code in curve_code_substation:    \n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of substations ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                substation_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                    }\n",
    "\n",
    "                    #assess risk for power towers and power poles\n",
    "                    elif i == 2:\n",
    "                        for curve_code in curve_code_tower:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power towers ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                tower_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "                            \n",
    "                        for curve_code in curve_code_pole:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power poles ...\")\n",
    "                            \n",
    "                            else:                    \n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                pole_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "                                \n",
    "    return pd.DataFrame(line_risk),pd.DataFrame(plant_risk),pd.DataFrame(substation_risk),pd.DataFrame(tower_risk),pd.DataFrame(pole_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76293670-ff91-480a-a318-47c25f886a59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8145/8145 [00:33<00:00, 241.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [02:52<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 440360/440360 [00:46<00:00, 9455.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio_usa for PHL is 0.0547\n",
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for PHL fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146/146 [00:19<00:00,  7.42it/s]\n",
      "polygon damage calculation for PHL fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  6.74it/s]\n",
      "point damage calculation for PHL fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:41<00:00, 21.19it/s]\n",
      "polyline damage calculation for PHL fl (rcp8p5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:19<00:00,  6.96it/s]\n",
      "polygon damage calculation for PHL fl (rcp8p5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  7.27it/s]\n",
      "point damage calculation for PHL fl (rcp8p5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:41<00:00, 20.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(             historical                                                      \\\n",
       "                  F5_1_1       F5_1_2       F5_1_3       F5_1_4       F5_1_5   \n",
       " mean_risk   1560.590879  1716.649967  2028.768142  2543.763132  2798.142234   \n",
       " lower_risk  1170.443159  1287.487475  1521.576107  1907.822349  2098.606675   \n",
       " upper_risk  1950.738598  2145.812458  2535.960178  3179.703915  3497.677792   \n",
       " \n",
       "                                                                              \\\n",
       "                  F5_1_6       F5_1_7       F5_1_8       F5_1_9      F5_1_10   \n",
       " mean_risk   3306.891142  1045.595889  1150.152689  1359.275585  1704.325109   \n",
       " lower_risk  2480.168357   784.196917   862.614517  1019.456689  1278.243832   \n",
       " upper_risk  4133.613928  1306.994861  1437.690861  1699.094481  2130.406387   \n",
       " \n",
       "             ...       rcp8p5                                        \\\n",
       "             ...       F5_1_3      F5_1_4       F5_1_5       F5_1_6   \n",
       " mean_risk   ...  2418.692525  3032.66832  3335.938477  3942.467708   \n",
       " lower_risk  ...  1814.019394  2274.50124  2501.953857  2956.850781   \n",
       " upper_risk  ...  3023.365657  3790.83540  4169.923096  4928.084635   \n",
       " \n",
       "                                                                              \\\n",
       "                  F5_1_7       F5_1_8       F5_1_9      F5_1_10      F5_1_11   \n",
       " mean_risk   1246.556917  1371.209284  1620.525100  2031.892318  2235.077117   \n",
       " lower_risk   934.917688  1028.406963  1215.393825  1523.919238  1676.307838   \n",
       " upper_risk  1558.196146  1714.011605  2025.656375  2539.865397  2793.846396   \n",
       " \n",
       "                          \n",
       "                 F5_1_12  \n",
       " mean_risk   2641.457797  \n",
       " lower_risk  1981.093348  \n",
       " upper_risk  3301.822246  \n",
       " \n",
       " [3 rows x 24 columns],\n",
       "                historical                                     rcp8p5  \\\n",
       "                    F1_1_1        F1_1_2        F1_1_3         F1_1_1   \n",
       " mean_risk   257273.973172  1.293763e+06  1.293763e+06  305462.206095   \n",
       " lower_risk  192955.479879  9.703221e+05  9.703221e+05  229096.654571   \n",
       " upper_risk  321592.466465  1.617203e+06  1.617203e+06  381827.757618   \n",
       " \n",
       "                                         \n",
       "                   F1_1_2        F1_1_3  \n",
       " mean_risk   1.536089e+06  1.536089e+06  \n",
       " lower_risk  1.152067e+06  1.152067e+06  \n",
       " upper_risk  1.920111e+06  1.920111e+06  ,\n",
       "             historical                               rcp8p5               \\\n",
       "                 F2_1_1      F2_1_2       F2_1_3      F2_1_1       F2_1_2   \n",
       " mean_risk   372.173484  744.346968  1860.867419  465.906975   931.813949   \n",
       " lower_risk  279.130113  558.260226  1395.650564  349.430231   698.860462   \n",
       " upper_risk  465.216855  930.433710  2326.084274  582.383718  1164.767437   \n",
       " \n",
       "                          \n",
       "                  F2_1_3  \n",
       " mean_risk   2329.534873  \n",
       " lower_risk  1747.151155  \n",
       " upper_risk  2911.918592  ,\n",
       "             historical                  rcp8p5            \n",
       "                 F3_1_1      F3_1_2      F3_1_1      F3_1_2\n",
       " mean_risk   153.378860  500.302498  147.984442  482.706587\n",
       " lower_risk  115.034145  375.226873  110.988331  362.029940\n",
       " upper_risk  191.723575  625.378122  184.980552  603.383233,\n",
       "            historical                                      rcp8p5             \\\n",
       "                F4_1_1     F4_1_2     F4_1_3     F4_1_4     F4_1_1     F4_1_2   \n",
       " mean_risk   12.462007  16.217157  21.563473  13.068771  15.420635  20.067302   \n",
       " lower_risk   9.346505  12.162868  16.172605   9.801579  11.565476  15.050477   \n",
       " upper_risk  15.577509  20.271447  26.954341  16.335964  19.275793  25.084128   \n",
       " \n",
       "                                   \n",
       "                F4_1_3     F4_1_4  \n",
       " mean_risk   26.682896  16.171452  \n",
       " lower_risk  20.012172  12.128589  \n",
       " upper_risk  33.353620  20.214315  )"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phl_osm_risk = country_analysis_osm('PHL','fl')\n",
    "phl_osm_risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ba717-0015-4527-bf1a-a9556f87a541",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Government data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef53a27c-f539-43db-9870-1ee933ddf7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_pg_infrastructure(country_code):\n",
    "    \"\"\"\n",
    "    Extract the infrastructure data from the government power grid data (GOV)\n",
    "        for a specific country.\n",
    "\n",
    "    Args:\n",
    "        country_code (str): Country code for the desired country.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two pandas GeoDataFrames:\n",
    "            - pg_lines: GOV lines data.\n",
    "            - pg_points: GOV points data.\n",
    "    \"\"\"\n",
    "    \n",
    "    files = [x for x in os.listdir(pg_data_path)  if country_code in x ]\n",
    "    pg_types = ['line','point']\n",
    "    \n",
    "    for pg_type in pg_types:\n",
    "        #print(os.path.isfile(os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))))\n",
    "        if os.path.isfile(os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))):\n",
    "            if pg_type=='line':\n",
    "                for file in files: \n",
    "                    file_path = os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))\n",
    "\n",
    "                    pg_data_country = gpd.read_file(file_path)\n",
    "                    pg_data_country = pd.DataFrame(pg_data_country.copy())\n",
    "                    pg_data_country.geometry = pygeos.from_shapely(pg_data_country.geometry)\n",
    "                    pg_data_country['geometry'] = reproject(pg_data_country)\n",
    "\n",
    "                pg_lines = buffer_assets(pg_data_country.loc[pg_data_country.asset.isin(['line'])],buffer_size=100).reset_index(drop=True)\n",
    "\n",
    "            elif pg_type=='point':\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))\n",
    "\n",
    "                    pg_data_country = gpd.read_file(file_path)\n",
    "                    pg_data_country = pd.DataFrame(pg_data_country.copy())\n",
    "                    pg_data_country.geometry = pygeos.from_shapely(pg_data_country.geometry)\n",
    "                    pg_data_country['geometry'] = reproject(pg_data_country)\n",
    "\n",
    "                pg_points = buffer_assets(pg_data_country.loc[pg_data_country.asset.isin(['plant','substation','power_tower','power_pole'])],buffer_size=100).reset_index(drop=True)\n",
    "\n",
    "    return pg_lines,pg_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0797c42-fd24-45f7-a0c0-51783767aa2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_damage_pg(country_code,pg_infra,hazard_type):\n",
    "    \"\"\"\n",
    "    Assess the damage to government power grid data (GOV) based on a hazard type.\n",
    "\n",
    "    Args:\n",
    "        country_code (str): Country code for the desired country.\n",
    "        pg_infra (tuple): Tuple containing two pandas GeoDataFrames:\n",
    "            - pg_lines: GOV lines data.\n",
    "            - pg_points: GOV points data.\n",
    "        hazard_type (str): Type of hazard ('tc' for tropical cyclone, 'fl' for coastal flood).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two pandas DataFrames:\n",
    "            - damaged_lines: Damage assessment results for GOV lines.\n",
    "            - damaged_points: Damage assessment results for GOV points.\n",
    "    \"\"\"\n",
    "    \n",
    "    # load curves and maxdam\n",
    "    curves,maxdam = load_curves_maxdam(country_code,vul_curve_path,hazard_type)\n",
    "    \n",
    "    # read infrastructure data:\n",
    "    pg_lines,pg_points = pg_infra\n",
    "    \n",
    "    #calculate damaged lines/polygons/points in loop by climate_model\n",
    "    damaged_lines = {}\n",
    "    damaged_points = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        # read wind data\n",
    "        climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "        df_ds = open_storm_data(country_code)\n",
    "        \n",
    "        # remove assets that will not have any damage\n",
    "        pg_points = pg_points.loc[pg_points.asset != 'plant'].reset_index(drop=True)\n",
    "    \n",
    "    elif hazard_type == 'fl':\n",
    "        # read flood data\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "        df_ds = open_flood_data(country_code)\n",
    "        \n",
    "    for climate_model in climate_models:\n",
    "        if hazard_type=='tc':\n",
    "            return_periods = ['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),'1_10{}'.format(climate_model),\n",
    "                              '1_25{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),\n",
    "                              '1_250{}'.format(climate_model),'1_500{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "        elif hazard_type == 'fl':\n",
    "            return_periods = ['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'] \n",
    "            \n",
    "        # assess damage for lines\n",
    "        overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],pg_lines).T,\n",
    "                                     columns=['asset','hazard_point'])\n",
    "\n",
    "        if len(overlay_lines) == 0:\n",
    "            damaged_lines[climate_model] = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            collect_line_damages = []\n",
    "            for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                              desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_line_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                            df_ds[climate_model],\n",
    "                                                                            pg_lines,\n",
    "                                                                            curves,\n",
    "                                                                            maxdam,\n",
    "                                                                            return_period,\n",
    "                                                                            country_code))\n",
    "\n",
    "            get_asset_type_line = dict(zip(pg_lines.index,pg_lines.asset))\n",
    "            \n",
    "            if hazard_type=='tc':\n",
    "                results = pd.DataFrame([item for sublist in collect_line_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "                \n",
    "            elif hazard_type == 'fl':\n",
    "                results = pd.DataFrame(np.array(list(flatten(collect_line_damages))).reshape(\n",
    "                    int(len(list(flatten(collect_line_damages)))/6), 6),\n",
    "                                       columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                results['asset'] = results['asset'].astype(int)\n",
    "                results[['meandam','lowerdam','upperdam']] = results[['meandam','lowerdam','upperdam']].astype(float)\n",
    "                \n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "        # assess damage for points\n",
    "        overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],pg_points).T,\n",
    "                                      columns=['asset','hazard_point'])\n",
    "\n",
    "        if len(overlay_points) == 0:\n",
    "            damaged_points[climate_model] = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            collect_point_damages = []\n",
    "            for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                              desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_point_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                            df_ds[climate_model],\n",
    "                                                                            pg_points,\n",
    "                                                                            curves,\n",
    "                                                                            maxdam,\n",
    "                                                                            return_period,\n",
    "                                                                            country_code))\n",
    "\n",
    "            get_asset_type_point = dict(zip(pg_points.index,pg_points.asset))\n",
    "            \n",
    "            if hazard_type == 'tc':\n",
    "                results = pd.DataFrame([item for sublist in collect_point_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "            elif hazard_type == 'fl':\n",
    "                results = pd.DataFrame(np.array(list(flatten(collect_point_damages))).reshape(\n",
    "                    int(len(list(flatten(collect_point_damages)))/6), 6),\n",
    "                                       columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                results['asset'] = results['asset'].astype(int)\n",
    "                results[['meandam','lowerdam','upperdam']] = results[['meandam','lowerdam','upperdam']].astype(float)\n",
    "\n",
    "            results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])    \n",
    "\n",
    "            damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "                \n",
    "    return damaged_lines,damaged_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98d174ad-3c24-44ea-88cb-0b88ee406e40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def country_analysis_pg(country_code,hazard_type):\n",
    "    \"\"\"\n",
    "    Perform risk analysis for a specific country and hazard type.\n",
    "\n",
    "    Args:\n",
    "        country_code (str): Country code for the desired country.\n",
    "        hazard_type (str, optional): Type of hazard.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the risk assessment results for power grid lines, plants, and substations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract infrastructure data from gov data\n",
    "    pg_power_infra = extract_pg_infrastructure(country_code)\n",
    "    \n",
    "    # assess damage to hazard_type\n",
    "    pg_damage_infra = assess_damage_pg(country_code,pg_power_infra,hazard_type)\n",
    "    \n",
    "    line_risk = {}\n",
    "    plant_risk = {}\n",
    "    substation_risk = {}\n",
    "\n",
    "    for i in range(len(pg_damage_infra)):\n",
    "        if hazard_type=='tc':\n",
    "            climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "            \n",
    "        elif hazard_type=='fl':\n",
    "            climate_models = ['historical','rcp8p5']\n",
    "            \n",
    "        for climate_model in climate_models:\n",
    "            if len(pg_damage_infra[i]) == 0:\n",
    "                df = pd.DataFrame()\n",
    "                print(\"No {}_{} risk of infra_type {} in {}\".format(hazard_type,climate_model,i,country_code))\n",
    "\n",
    "            elif len(pg_damage_infra[i]) >= 0:\n",
    "                df = pg_damage_infra[i][climate_model]\n",
    "\n",
    "                with pd.ExcelWriter(os.path.join(output_path,'damage','{}_pg_{}{}_damage_{}'.format(country_code,hazard_type,climate_model,i)+'.xlsx')) as writer:\n",
    "                    df.to_excel(writer)\n",
    "                \n",
    "                if hazard_type=='tc':\n",
    "                    curve_code_substation = ['W2_1_1','W2_1_2','W2_1_3','W2_1_4','W2_1_5','W2_1_6','W2_2_1','W2_2_2','W2_2_3','W2_2_4','W2_2_5','W2_2_6',\n",
    "                                             'W2_3_1','W2_3_2','W2_3_3','W2_3_4','W2_3_5','W2_3_6','W2_4_1','W2_4_2','W2_4_3','W2_4_4','W2_4_5','W2_4_6',\n",
    "                                             'W2_5_1','W2_5_2','W2_5_3','W2_5_4','W2_5_5','W2_5_6','W2_6_1','W2_6_2','W2_6_3','W2_6_4','W2_6_5','W2_6_6',\n",
    "                                             'W2_7_1','W2_7_2','W2_7_3','W2_7_4','W2_7_5','W2_7_6']\n",
    "\n",
    "                    curve_code_line = ['W5_1_1','W5_1_2','W5_1_3','W5_1_4','W5_1_5','W5_1_6','W5_1_7','W5_1_8','W5_1_9','W5_1_10','W5_1_11','W5_1_12',\n",
    "                                       'W5_2_1','W5_2_2','W5_2_3','W5_2_4','W5_2_5','W5_2_6','W5_2_7','W5_2_8','W5_2_9','W5_2_10','W5_2_11','W5_2_12',\n",
    "                                       'W5_3_1','W5_3_2','W5_3_3','W5_3_4','W5_3_5','W5_3_6','W5_3_7','W5_3_8','W5_3_9','W5_3_10','W5_3_11','W5_3_12',\n",
    "                                       'W5_4_1','W5_4_2','W5_4_3','W5_4_4','W5_4_5','W5_4_6','W5_4_7','W5_4_8','W5_4_9','W5_4_10','W5_4_11','W5_4_12',\n",
    "                                       'W5_5_1','W5_5_2','W5_5_3','W5_5_4','W5_5_5','W5_5_6','W5_5_7','W5_5_8','W5_5_9','W5_5_10','W5_5_11','W5_5_12',\n",
    "                                       'W5_6_1','W5_6_2','W5_6_3','W5_6_4','W5_6_5','W5_6_6','W5_6_7','W5_6_8','W5_6_9','W5_6_10','W5_6_11','W5_6_12',\n",
    "                                       'W5_7_1','W5_7_2','W5_7_3','W5_7_4','W5_7_5','W5_7_6','W5_7_7','W5_7_8','W5_7_9','W5_7_10','W5_7_11','W5_7_12']\n",
    "                    \n",
    "                    \n",
    "                elif hazard_type=='fl':\n",
    "                    curve_code_plant = ['F1_1_1','F1_1_2','F1_1_3']\n",
    "                    curve_code_substation = ['F2_1_1','F2_1_2','F2_1_3']\n",
    "                    curve_code_line = ['F5_1_1','F5_1_2','F5_1_3','F5_1_4','F5_1_5','F5_1_6','F5_1_7','F5_1_8',\n",
    "                                       'F5_1_9','F5_1_10','F5_1_11','F5_1_12']\n",
    "\n",
    "                #assess risk for power lines\n",
    "                if i == 0:\n",
    "                    if len(df) > 0:\n",
    "                        if hazard_type == 'tc':\n",
    "                            df['rp'] = df['rp'].replace(['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),\n",
    "                                                '1_10{}'.format(climate_model),'1_25{}'.format(climate_model),'1_50{}'.format(climate_model),\n",
    "                                                '1_100{}'.format(climate_model),'1_250{}'.format(climate_model),'1_500{}'.format(climate_model),\n",
    "                                                '1_1000{}'.format(climate_model)],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "\n",
    "                        elif hazard_type == 'fl':\n",
    "                            df['rp'] = df['rp'].replace(['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'],\n",
    "                                                        [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "                        \n",
    "                        for curve_code in curve_code_line:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power lines ...\")\n",
    "\n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "\n",
    "                                line_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "                \n",
    "                    else:\n",
    "                        line_risk = pd.DataFrame()\n",
    "                \n",
    "                #assess risk for power plants and substations                \n",
    "                elif i == 1:\n",
    "                    if len(df) > 0:\n",
    "                        if hazard_type == 'tc':\n",
    "                            df['rp'] = df['rp'].replace(['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),\n",
    "                                                '1_10{}'.format(climate_model),'1_25{}'.format(climate_model),'1_50{}'.format(climate_model),\n",
    "                                                '1_100{}'.format(climate_model),'1_250{}'.format(climate_model),'1_500{}'.format(climate_model),\n",
    "                                                '1_1000{}'.format(climate_model)],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "\n",
    "                        elif hazard_type == 'fl':\n",
    "                            df['rp'] = df['rp'].replace(['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'],\n",
    "                                                        [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "\n",
    "                            for curve_code in curve_code_plant:\n",
    "                                loss_list = df.loc[df['curve'] == curve_code]\n",
    "                                if len(loss_list) == 0:\n",
    "                                    print(\"No risk of plants ...\")\n",
    "\n",
    "                                else:\n",
    "                                    loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                    loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                    loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                    RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                    RPS = RPS.rp.values.tolist()\n",
    "                                    plant_risk[climate_model,curve_code] = {\n",
    "                                        'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                        'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                        'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                    }\n",
    "\n",
    "                        for curve_code in curve_code_substation:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of substations ...\")\n",
    "\n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "\n",
    "                                substation_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "                    else:\n",
    "                        plant_risk = pd.DataFrame()\n",
    "                        substation_risk = pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(line_risk),pd.DataFrame(plant_risk),pd.DataFrame(substation_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "822d08b2-d36a-408b-9cb0-8133df2a15d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio_usa for PRK is 0.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for PRK tc (): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:12<00:00,  1.51s/it]\n",
      "point damage calculation for PRK tc (): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:07<00:00,  3.76it/s]\n",
      "polyline damage calculation for PRK tc (_CMCC-CM2-VHR4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:10<00:00,  1.48s/it]\n",
      "point damage calculation for PRK tc (_CMCC-CM2-VHR4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:07<00:00,  3.81it/s]\n",
      "polyline damage calculation for PRK tc (_CNRM-CM6-1-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:11<00:00,  1.48s/it]\n",
      "point damage calculation for PRK tc (_CNRM-CM6-1-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:07<00:00,  3.83it/s]\n",
      "polyline damage calculation for PRK tc (_EC-Earth3P-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:11<00:00,  1.49s/it]\n",
      "point damage calculation for PRK tc (_EC-Earth3P-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:07<00:00,  3.80it/s]\n",
      "polyline damage calculation for PRK tc (_HadGEM3-GC31-HM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:12<00:00,  1.51s/it]\n",
      "point damage calculation for PRK tc (_HadGEM3-GC31-HM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:07<00:00,  3.80it/s]\n"
     ]
    }
   ],
   "source": [
    "phl_pg_risk = country_analysis_pg('PRK','tc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f076a1e2-6f11-47d7-b19b-595720d8de7b",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dda764da-ee7e-49ea-9572-e263c80ddecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def risk_output(country_code,hazard_type,infra_type):\n",
    "    \"\"\"\n",
    "    Generate risk output based on country, hazard type, and infrastructure type.\n",
    "\n",
    "    Args:\n",
    "        country_code (str): Country code for the desired country.\n",
    "        hazard_type (str): Type of hazard ('tc' for tropical cyclone, 'fl' for coastal flood).\n",
    "        infra_type (str): Type of infrastructure ('osm' for OpenStreetMap, 'gov' for government data).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    if hazard_type == 'tc':\n",
    "        climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "        \n",
    "    elif hazard_type == 'fl':\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "  \n",
    "    if infra_type == 'osm':\n",
    "        line_risk,plant_risk,substation_risk,tower_risk,pole_risk = country_analysis_osm(country_code,hazard_type)\n",
    "    \n",
    "    elif infra_type == 'gov':\n",
    "        line_risk,plant_risk,substation_risk = country_analysis_pg(country_code,hazard_type)\n",
    "            \n",
    "    for climate_model in climate_models:\n",
    "        if hazard_type == 'tc':\n",
    "            if climate_model == '':\n",
    "                writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}_{}_risk'.format(country_code,infra_type,hazard_type,'present')+'.xlsx'),\n",
    "                                        engine='openpyxl')\n",
    "            else:\n",
    "                writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}{}_risk'.format(country_code,infra_type,hazard_type,climate_model)+'.xlsx'),\n",
    "                                        engine='openpyxl')\n",
    "        elif hazard_type == 'fl':\n",
    "            writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}_{}_risk'.format(country_code,infra_type,hazard_type,climate_model)+'.xlsx'),\n",
    "                                        engine='openpyxl')\n",
    "        \n",
    "        if infra_type == 'osm':\n",
    "            # write each dataframe to a different sheet\n",
    "            if len(line_risk) != 0:\n",
    "                line_risk[climate_model].to_excel(writer, sheet_name='line_risk')\n",
    "            if len(plant_risk) != 0:\n",
    "                plant_risk[climate_model].to_excel(writer, sheet_name='plant_risk')\n",
    "            if len(substation_risk) != 0:\n",
    "                substation_risk[climate_model].to_excel(writer, sheet_name='substation_risk')\n",
    "            if len(tower_risk) != 0:\n",
    "                tower_risk[climate_model].to_excel(writer, sheet_name='tower_risk')\n",
    "            if len(pole_risk) != 0:\n",
    "                pole_risk[climate_model].to_excel(writer, sheet_name='pole_risk')\n",
    "\n",
    "        elif infra_type == 'gov':\n",
    "            # write each dataframe to a different sheet\n",
    "            if len(line_risk) != 0:\n",
    "                line_risk[climate_model].to_excel(writer, sheet_name='line_risk')\n",
    "            if len(plant_risk) != 0:\n",
    "                plant_risk[climate_model].to_excel(writer, sheet_name='plant_risk')\n",
    "            if len(substation_risk) != 0:\n",
    "                substation_risk[climate_model].to_excel(writer, sheet_name='substation_risk')\n",
    "            \n",
    "        # save the Excel file\n",
    "        if writer.sheets:\n",
    "            writer.save()\n",
    "\n",
    "        else:\n",
    "            df = pd.DataFrame()\n",
    "            df.loc[1:3, 0] = ['mean_risk', 'lower_risk', 'upper_risk']\n",
    "\n",
    "            if hazard_type == 'tc':\n",
    "                if climate_model == '':\n",
    "                    writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}_{}_risk'.format(country_code,infra_type,hazard_type,'present')+'.xlsx'),\n",
    "                                            engine='openpyxl')\n",
    "                else:\n",
    "                    writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}{}_risk'.format(country_code,infra_type,hazard_type,climate_model)+'.xlsx'),\n",
    "                                            engine='openpyxl')\n",
    "            elif hazard_type == 'fl':\n",
    "                writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}_{}_risk'.format(country_code,infra_type,hazard_type,climate_model)+'.xlsx'),\n",
    "                                            engine='openpyxl')\n",
    "\n",
    "            df.to_excel(writer,sheet_name='line_risk', index=False)\n",
    "            writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac3700-3233-4c73-a688-3549808ff356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
