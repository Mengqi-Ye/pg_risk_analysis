{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5263e9-087d-4863-a4e5-839dc33017bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mye500\\AppData\\Local\\Temp\\ipykernel_23656\\517661299.py:1: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from osgeo import ogr,gdal\n",
    "import os\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pyproj\n",
    "from pygeos import from_wkb,from_wkt\n",
    "import pygeos\n",
    "from tqdm import tqdm\n",
    "from shapely.wkb import loads\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from shapely.geometry import mapping\n",
    "pd.options.mode.chained_assignment = None\n",
    "from rasterio.mask import mask\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f11a902-f512-4fd3-b035-af38041ee083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdal.SetConfigOption(\"OSM_CONFIG_FILE\", os.path.join('..',\"osmconf.ini\"))\n",
    "\n",
    "# change paths to make it work on your own machine\n",
    "data_path = os.path.join('C:\\\\','Data','pg_risk_analysis')\n",
    "tc_path = os.path.join(data_path,'tc_netcdf')\n",
    "fl_path = os.path.join(data_path,'GLOFRIS')\n",
    "osm_data_path = os.path.join('C:\\\\','Data','country_osm')\n",
    "pg_data_path = os.path.join(data_path,'pg_data')\n",
    "vul_curve_path = os.path.join(data_path,'vulnerability_curves','input_vulnerability_data.xlsx')\n",
    "output_path = os.path.join('C:\\\\','projects','pg_risk_analysis','output')\n",
    "ne_path = os.path.join(data_path,'..',\"natural_earth\",\"ne_10m_admin_0_countries.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc83d355-8c98-4ae6-97b1-3203a29a4fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_b(geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    This function builds an SQL query from the values passed to the retrieve() function.\n",
    "    Arguments:\n",
    "         *geoType* : Type of geometry (osm layer) to search for.\n",
    "         *keyCol* : A list of keys/columns that should be selected from the layer.\n",
    "         ***valConstraint* : A dictionary of constraints for the values. e.g. WHERE 'value'>20 or 'value'='constraint'\n",
    "    Returns:\n",
    "        *string: : a SQL query string.\n",
    "    \"\"\"\n",
    "    query = \"SELECT \" + \"osm_id\"\n",
    "    for a in keyCol: query+= \",\"+ a  \n",
    "    query += \" FROM \" + geoType + \" WHERE \"\n",
    "    # If there are values in the dictionary, add constraint clauses\n",
    "    if valConstraint: \n",
    "        for a in [*valConstraint]:\n",
    "            # For each value of the key, add the constraint\n",
    "            for b in valConstraint[a]: query += a + b\n",
    "        query+= \" AND \"\n",
    "    # Always ensures the first key/col provided is not Null.\n",
    "    query+= \"\"+str(keyCol[0]) +\" IS NOT NULL\" \n",
    "    return query \n",
    "\n",
    "\n",
    "def retrieve(osm_path,geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    Function to extract specified geometry and keys/values from OpenStreetMap\n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.     \n",
    "        *geoType* : Type of Geometry to retrieve. e.g. lines, multipolygons, etc.\n",
    "        *keyCol* : These keys will be returned as columns in the dataframe.\n",
    "        ***valConstraint: A dictionary specifiying the value constraints.  \n",
    "        A key can have multiple values (as a list) for more than one constraint for key/value.  \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with all columns, geometries, and constraints specified.    \n",
    "    \"\"\"\n",
    "    driver=ogr.GetDriverByName('OSM')\n",
    "    data = driver.Open(osm_path)\n",
    "    query = query_b(geoType,keyCol,**valConstraint)\n",
    "    sql_lyr = data.ExecuteSQL(query)\n",
    "    features =[]\n",
    "    # cl = columns \n",
    "    cl = ['osm_id'] \n",
    "    for a in keyCol: cl.append(a)\n",
    "    if data is not None:\n",
    "        print('query is finished, lets start the loop')\n",
    "        for feature in tqdm(sql_lyr,desc='extract'):\n",
    "            #try:\n",
    "            if feature.GetField(keyCol[0]) is not None:\n",
    "                geom1 = (feature.geometry().ExportToWkt())\n",
    "                #print(geom1)\n",
    "                geom = from_wkt(feature.geometry().ExportToWkt()) \n",
    "                if geom is None:\n",
    "                    continue\n",
    "                # field will become a row in the dataframe.\n",
    "                field = []\n",
    "                for i in cl: field.append(feature.GetField(i))\n",
    "                field.append(geom)   \n",
    "                features.append(field)\n",
    "            #except:\n",
    "            #    print(\"WARNING: skipped OSM feature\")   \n",
    "    else:\n",
    "        print(\"ERROR: Nonetype error when requesting SQL. Check required.\")    \n",
    "    cl.append('geometry')                   \n",
    "    if len(features) > 0:\n",
    "        return pd.DataFrame(features,columns=cl)\n",
    "    else:\n",
    "        print(\"WARNING: No features or No Memory. returning empty GeoDataFrame\") \n",
    "        return pd.DataFrame(columns=['osm_id','geometry'])\n",
    "\n",
    "def power_polyline(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract all energy linestrings from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'lines',['power','voltage'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).rename(columns={'power': 'asset'})\n",
    "    \n",
    "    #print(df) #check infra keys\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def power_polygon(osm_path): # check with joel, something was wrong here with extracting substations\n",
    "    \"\"\"\n",
    "    Function to extract energy polygons from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['other_tags']) \n",
    "    \n",
    "    df = df.loc[(df.other_tags.str.contains('power'))]   #keep rows containing power data         \n",
    "    df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})     \n",
    "    \n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"substation\"', case=False)]  = 'substation' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"plant\"', case=False)] = 'plant' #specify row\n",
    "    \n",
    "    df = df.loc[(df.asset == 'substation') | (df.asset == 'plant')]\n",
    "            \n",
    "    return df.reset_index(drop=True) \n",
    "\n",
    "def electricity(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract building polygons from OpenStreetMap    \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with all unique building polygons.    \n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['power'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).rename(columns={'power': 'asset'})\n",
    "    \n",
    "    #df = df[df.asset!='generator']\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"substation\"', case=False)]  = 'substation' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"plant\"', case=False)] = 'plant' #specify row\n",
    "    \n",
    "    #print(df)  #check infra keys\n",
    "    \n",
    "    df = df.loc[(df.asset == 'substation') | (df.asset == 'plant')]\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def retrieve_poly_subs(osm_path, w_list, b_list):\n",
    "    \"\"\"\n",
    "    Function to extract electricity substation polygons from OpenStreetMap\n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region\n",
    "        for which we want to do the analysis.\n",
    "        *w_list* :  white list of keywords to search in the other_tags columns\n",
    "        *b_list* :  black list of keywords of rows that should not be selected\n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique substation.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['other_tags'])\n",
    "    df = df[df.other_tags.str.contains('substation', case=False, na=False)]\n",
    "    #df = df.loc[(df.other_tags.str.contains('substation'))]\n",
    "    df = df[~df.other_tags.str.contains('|'.join(b_list))]\n",
    "    #df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})\n",
    "    df['asset']  = 'substation' #specify row\n",
    "    #df = df.loc[(df.asset == 'substation')] #specify row\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def power_point(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract energy points from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"   \n",
    "    df = retrieve(osm_path,'points',['other_tags']) \n",
    "    df = df.loc[(df.other_tags.str.contains('power'))]  #keep rows containing power data       \n",
    "    df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})     \n",
    "        \n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"tower\"', case=False)]  = 'power_tower' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"pole\"', case=False)] = 'power_pole' #specify row\n",
    "    #df['asset'].loc[df['asset'].str.contains('\"utility\"=>\"power\"', case=False)] = 'power_tower' #specify row\n",
    "    \n",
    "    df = df.loc[(df.asset == 'power_tower') | (df.asset == 'power_pole')]\n",
    "            \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36fe9f2e-0be0-47fd-83ca-67f3a272279d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reproject(df_ds, current_crs=\"epsg:4326\", approximate_crs=\"epsg:3857\"):\n",
    "\n",
    "    # Extract the input geometries as a numpy array of coordinates\n",
    "    geometries = df_ds['geometry']\n",
    "    coords = pygeos.get_coordinates(geometries)\n",
    "\n",
    "    # Transform the coordinates using pyproj\n",
    "    transformer = pyproj.Transformer.from_crs(current_crs, approximate_crs, always_xy=True)\n",
    "    new_coords = transformer.transform(coords[:, 0], coords[:, 1])\n",
    "\n",
    "    # Create a new GeoSeries with the reprojected coordinates\n",
    "    return pygeos.set_coordinates(geometries.copy(), np.array(new_coords).T)\n",
    "\n",
    "def buffer_assets(assets, buffer_size=100):\n",
    "    \"\"\"\n",
    "    Create a buffer of a specified size around the geometries in a GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        assets (GeoDataFrame): A GeoDataFrame containing geometries to be buffered.\n",
    "        buffer_size (int, optional): The distance in the units of the GeoDataFrame's CRS to buffer the geometries.\n",
    "            Defaults to 100.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A new GeoDataFrame with an additional column named 'buffered' containing the buffered\n",
    "            geometries.\n",
    "    \"\"\"\n",
    "    # Create a buffer of the specified size around the geometries\n",
    "    assets['buffered'] = pygeos.buffer(assets.geometry.values, buffer_size)\n",
    "    \n",
    "    return assets\n",
    "\n",
    "def load_curves_maxdam(vul_curve_path,hazard_type):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        data_path ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "\n",
    "    if hazard_type == 'tc':\n",
    "        sheet_name = 'wind_curves'\n",
    "    \n",
    "    elif hazard_type == 'fl':\n",
    "        sheet_name = 'flooding_curves'\n",
    "    \n",
    "    # load curves and maximum damages as separate inputs\n",
    "    curves = pd.read_excel(vul_curve_path,sheet_name=sheet_name,skiprows=10,index_col=[0])\n",
    "    \n",
    "    if hazard_type == 'fl':\n",
    "        maxdam = pd.read_excel(vul_curve_path,sheet_name=sheet_name,index_col=[0]).iloc[:8]\n",
    "    elif hazard_type == 'tc':\n",
    "        maxdam = pd.read_excel(vul_curve_path,sheet_name=sheet_name,index_col=[0],header=[0,1]).iloc[:7]\n",
    "        maxdam = maxdam.rename({'substation_point':'substation'},level=0,axis=1)\n",
    "            \n",
    "    curves.columns = maxdam.columns\n",
    "        \n",
    "    #transpose maxdam so its easier work with the dataframe\n",
    "    maxdam = maxdam.T\n",
    "\n",
    "    #interpolate the curves to fill missing values\n",
    "    curves = curves.interpolate()\n",
    "    \n",
    "    #print(curves)\n",
    "   \n",
    "    return curves,maxdam\n",
    "\n",
    "\n",
    "def overlay_hazard_assets(df_ds, assets):\n",
    "    \"\"\"\n",
    "    Overlay a set of assets with a hazard dataset and return the subset of assets that intersect with\n",
    "    one or more hazard polygons or lines.\n",
    "    \n",
    "    Args:\n",
    "        df_ds (GeoDataFrame): A GeoDataFrame containing the hazard dataset.\n",
    "        assets (GeoDataFrame): A GeoDataFrame containing the assets to be overlaid with the hazard dataset.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: A numpy array of integers representing the indices of the hazard geometries that intersect with\n",
    "            the assets. If the assets have a 'buffered' column, the buffered geometries are used for the overlay.\n",
    "    \"\"\"\n",
    "    hazard_tree = pygeos.STRtree(df_ds.geometry.values)\n",
    "    if (pygeos.get_type_id(assets.iloc[0].geometry) == 3) | (pygeos.get_type_id(assets.iloc[0].geometry) == 6):\n",
    "        return  hazard_tree.query_bulk(assets.geometry,predicate='intersects')    \n",
    "    else:\n",
    "        return  hazard_tree.query_bulk(assets.buffered,predicate='intersects')\n",
    "    \n",
    "def get_damage_per_asset_per_rp(asset,df_ds,assets,curves,maxdam,return_period,country):\n",
    "    \"\"\"\n",
    "    Calculates the damage per asset per return period based on asset type, hazard curves and maximum damage\n",
    "\n",
    "    Args:\n",
    "        asset (tuple): Tuple with two dictionaries, containing the asset index and the hazard point index of the asset\n",
    "        df_ds (pandas.DataFrame): A pandas DataFrame containing hazard points with a 'geometry' column\n",
    "        assets (geopandas.GeoDataFrame): A GeoDataFrame containing asset geometries and asset type information\n",
    "        curves (dict): A dictionary with the asset types as keys and their corresponding hazard curves as values\n",
    "        maxdam (pandas.DataFrame): A pandas DataFrame containing the maximum damage for each asset type\n",
    "        return_period (str): The return period for which the damage should be calculated\n",
    "        country (str): The country for which the damage should be calculated\n",
    "\n",
    "    Returns:\n",
    "        list or tuple: Depending on the input, the function either returns a list of tuples with the asset index, the curve name and the calculated damage, or a tuple with None, None, None if no hazard points are found\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = df_ds.iloc[asset[1]['hazard_point'].values].reset_index()\n",
    "    get_hazard_points = get_hazard_points.loc[pygeos.intersects(get_hazard_points.geometry.values,assets.iloc[asset[0]].geometry)]\n",
    "\n",
    "    \n",
    "    asset_type = assets.iloc[asset[0]].asset\n",
    "    asset_geom = assets.iloc[asset[0]].geometry\n",
    "\n",
    "    if asset_type in ['plant','substation','generator']:\n",
    "        #if plant,substation are points, do not calculate the area\n",
    "        if pygeos.area(asset_geom) == 0:\n",
    "            maxdam_asset = maxdam.loc[asset_type].MaxDam\n",
    "            lowerdam_asset = maxdam.loc[asset_type].LowerDam\n",
    "            upperdam_asset = maxdam.loc[asset_type].UpperDam\n",
    "        else:\n",
    "            maxdam_asset = maxdam.loc[asset_type].MaxDam/pygeos.area(asset_geom)\n",
    "            lowerdam_asset = maxdam.loc[asset_type].LowerDam/pygeos.area(asset_geom)\n",
    "            upperdam_asset = maxdam.loc[asset_type].UpperDam/pygeos.area(asset_geom)\n",
    "    else:\n",
    "        maxdam_asset = maxdam.loc[asset_type].MaxDam\n",
    "        lowerdam_asset = maxdam.loc[asset_type].LowerDam\n",
    "        upperdam_asset = maxdam.loc[asset_type].UpperDam\n",
    "\n",
    "\n",
    "    hazard_intensity = curves[asset_type].index.values\n",
    "    \n",
    "    if isinstance(curves[asset_type],pd.core.series.Series):\n",
    "        fragility_values = curves[asset_type].values.flatten()\n",
    "        only_one = True\n",
    "        curve_name = curves[asset_type].name\n",
    "    elif len(curves[asset_type].columns) == 1:\n",
    "        fragility_values = curves[asset_type].values.flatten()      \n",
    "        only_one = True   \n",
    "        curve_name = curves[asset_type].columns[0]\n",
    "    else:\n",
    "        fragility_values = curves[asset_type].values#.T[0]\n",
    "        maxdam_asset = maxdam_asset.values#[0]\n",
    "        only_one = False\n",
    "\n",
    "    if len(get_hazard_points) == 0:\n",
    "        return [return_period,asset[0],None,None]\n",
    "    else:\n",
    "        if only_one:    \n",
    "            # run the calculation as normal when the asset just has a single curve\n",
    "            if pygeos.get_type_id(asset_geom) == 1:            \n",
    "                get_hazard_points['overlay_meters'] = pygeos.length(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "                return [return_period,asset[0],curve_name,np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,\n",
    "                                                             fragility_values))*get_hazard_points.overlay_meters*maxdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,\n",
    "                                                             fragility_values))*get_hazard_points.overlay_meters*lowerdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,\n",
    "                                                             fragility_values))*get_hazard_points.overlay_meters*upperdam_asset)]\n",
    "\n",
    "            elif (pygeos.get_type_id(asset_geom) == 3) | (pygeos.get_type_id(asset_geom) == 6) :\n",
    "                get_hazard_points['overlay_m2'] = pygeos.area(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "                return [return_period,asset[0],curve_name,get_hazard_points.apply(lambda x: np.interp(x[return_period],hazard_intensity, \n",
    "                                                                  fragility_values)*maxdam_asset*x.overlay_m2,axis=1).sum(),\n",
    "                                                          get_hazard_points.apply(lambda x: np.interp(x[return_period],hazard_intensity, \n",
    "                                                                  fragility_values)*lowerdam_asset*x.overlay_m2,axis=1).sum(),\n",
    "                                                          get_hazard_points.apply(lambda x: np.interp(x[return_period],hazard_intensity, \n",
    "                                                                  fragility_values)*upperdam_asset*x.overlay_m2,axis=1).sum()]  \n",
    "\n",
    "            else:\n",
    "                return [return_period,asset[0],curve_name,np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                             hazard_intensity,fragility_values))*maxdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                             hazard_intensity,fragility_values))*lowerdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                             hazard_intensity,fragility_values))*upperdam_asset)]\n",
    "        else:\n",
    "            # run the calculation when the asset has multiple curves\n",
    "            if pygeos.get_type_id(asset_geom) == 1:            \n",
    "                get_hazard_points['overlay_meters'] = pygeos.length(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            elif (pygeos.get_type_id(asset_geom) == 3) | (pygeos.get_type_id(asset_geom) == 6) :\n",
    "                get_hazard_points['overlay_m2'] = pygeos.area(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            \n",
    "            collect_all = []\n",
    "            for iter_,curve_ids in enumerate(curves[asset_type].columns):\n",
    "                if pygeos.get_type_id(asset_geom) == 1:\n",
    "                    collect_all.append([return_period,asset[0],curves[asset_type].columns[iter_],\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*get_hazard_points.overlay_meters*maxdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*get_hazard_points.overlay_meters*lowerdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*get_hazard_points.overlay_meters*upperdam_asset[iter_])])\n",
    "                                   \n",
    "                elif (pygeos.get_type_id(asset_geom) == 3) | (pygeos.get_type_id(asset_geom) == 6) :\n",
    "                    collect_all.append([return_period,asset[0],curves[asset_type].columns[iter_],\n",
    "                                        get_hazard_points.apply(lambda x: np.interp(x[return_period], hazard_intensity,\n",
    "                                                                                    fragility_values.T[iter_])*maxdam_asset[iter_]*x.overlay_m2,axis=1).sum(),\n",
    "                                        get_hazard_points.apply(lambda x: np.interp(x[return_period], hazard_intensity,\n",
    "                                                                                    fragility_values.T[iter_])*lowerdam_asset[iter_]*x.overlay_m2,axis=1).sum(),\n",
    "                                        get_hazard_points.apply(lambda x: np.interp(x[return_period], hazard_intensity,\n",
    "                                                                                    fragility_values.T[iter_])*upperdam_asset[iter_]*x.overlay_m2,axis=1).sum()])\n",
    "\n",
    "                else:\n",
    "                    collect_all.append([return_period,asset[0],curves[asset_type].columns[iter_],\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*maxdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*lowerdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*upperdam_asset[iter_])])\n",
    "            return collect_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf0e992-b4d1-46b0-a952-488ac8c5b2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Infrastructure type</th>\n",
       "      <th colspan=\"21\" halign=\"left\">power_tower</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Code</th>\n",
       "      <th>W3_1</th>\n",
       "      <th>W3_2</th>\n",
       "      <th>W3_3</th>\n",
       "      <th>W3_4</th>\n",
       "      <th>W3_5</th>\n",
       "      <th>W3_6</th>\n",
       "      <th>W3_7</th>\n",
       "      <th>W3_8</th>\n",
       "      <th>W3_9</th>\n",
       "      <th>W3_10</th>\n",
       "      <th>...</th>\n",
       "      <th>W3_17</th>\n",
       "      <th>W3_18</th>\n",
       "      <th>W3_19</th>\n",
       "      <th>W3_20</th>\n",
       "      <th>W3_21</th>\n",
       "      <th>W3_22</th>\n",
       "      <th>W3_23</th>\n",
       "      <th>W3_24</th>\n",
       "      <th>W3_25</th>\n",
       "      <th>W3_26</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wind speed (m/s)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.388889</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.235200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280.000000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285.000000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290.000000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295.000000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300.000000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows Ã— 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Infrastructure type power_tower                                                \\\n",
       "Code                       W3_1 W3_2 W3_3 W3_4 W3_5 W3_6 W3_7 W3_8 W3_9 W3_10   \n",
       "Wind speed (m/s)                                                                \n",
       "0.000000                    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   NaN   \n",
       "1.000000                    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   NaN   \n",
       "1.388889                    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   NaN   \n",
       "2.000000                    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   NaN   \n",
       "2.235200                    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   NaN   \n",
       "...                         ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   \n",
       "280.000000                  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   1.0   \n",
       "285.000000                  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   1.0   \n",
       "290.000000                  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   1.0   \n",
       "295.000000                  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   1.0   \n",
       "300.000000                  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   1.0   \n",
       "\n",
       "Infrastructure type  ...                                                  \\\n",
       "Code                 ... W3_17 W3_18 W3_19 W3_20 W3_21 W3_22 W3_23 W3_24   \n",
       "Wind speed (m/s)     ...                                                   \n",
       "0.000000             ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "1.000000             ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "1.388889             ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2.000000             ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2.235200             ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "...                  ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "280.000000           ...   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "285.000000           ...   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "290.000000           ...   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "295.000000           ...   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "300.000000           ...   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "\n",
       "Infrastructure type              \n",
       "Code                W3_25 W3_26  \n",
       "Wind speed (m/s)                 \n",
       "0.000000              0.0   0.0  \n",
       "1.000000              0.0   0.0  \n",
       "1.388889              0.0   0.0  \n",
       "2.000000              0.0   0.0  \n",
       "2.235200              0.0   0.0  \n",
       "...                   ...   ...  \n",
       "280.000000            1.0   1.0  \n",
       "285.000000            1.0   1.0  \n",
       "290.000000            1.0   1.0  \n",
       "295.000000            1.0   1.0  \n",
       "300.000000            1.0   1.0  \n",
       "\n",
       "[248 rows x 107 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_curves_maxdam(vul_curve_path,'tc')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9896d9f-4592-4552-8876-c1340f3e09f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def open_storm_data(country_code):\n",
    "    # list of available climate models\n",
    "    climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "\n",
    "    # dictionary of basins for each country\n",
    "    country_basin = {\n",
    "        \"BRN\": [\"WP\"],\n",
    "        \"KHM\": [\"WP\"],\n",
    "        \"CHN\": [\"WP\", \"NI\"],\n",
    "        \"IDN\": [\"SI\", \"SP\", \"NI\", \"WP\"],\n",
    "        \"JPN\": [\"WP\"],\n",
    "        \"LAO\": [\"WP\"],\n",
    "        \"MYS\": [\"WP\", \"NI\"],\n",
    "        \"MNG\": [\"WP\", \"NI\"],\n",
    "        \"MMR\": [\"NI\", \"WP\"],\n",
    "        \"PRK\": [\"WP\"],\n",
    "        \"PHL\": [\"WP\"],\n",
    "        \"SGP\": [\"WP\"],\n",
    "        \"KOR\": [\"WP\"],\n",
    "        \"TWN\": [\"WP\"],\n",
    "        \"THA\": [\"WP\", \"NI\"],\n",
    "        \"VNM\": [\"WP\"]\n",
    "    }\n",
    "\n",
    "    # load country geometry file and create geometry to clip\n",
    "    ne_countries = gpd.read_file(os.path.join(data_path,'..',\"natural_earth\",\"ne_10m_admin_0_countries.shp\"))\n",
    "    bbox = ne_countries.loc[ne_countries['ISO_A3']==country_code].geometry.buffer(1).values[0].bounds\n",
    "\n",
    "    df_ds = {}\n",
    "    for climate_model in climate_models:\n",
    "        concat_prep = []\n",
    "\n",
    "        #combine STORM data from different basins\n",
    "        if \"WP\" in country_basin[country_code]:\n",
    "            WP = load_storm_data(climate_model,'WP',bbox)\n",
    "            concat_prep.append(WP)\n",
    "        if \"SP\" in country_basin[country_code]:\n",
    "            SP = load_storm_data(climate_model,'SP',bbox)\n",
    "            concat_prep.append(SP)\n",
    "        if \"NI\" in country_basin[country_code]:            \n",
    "            NI = load_storm_data(climate_model,'NI',bbox)\n",
    "            concat_prep.append(NI)            \n",
    "        if \"SI\" in country_basin[country_code]:       \n",
    "            SI = load_storm_data(climate_model,'SI',bbox)\n",
    "            concat_prep.append(SI)            \n",
    "                   \n",
    "        df_ds_cl = pd.concat(concat_prep, keys=country_basin[country_code])\n",
    "        df_ds_cl = df_ds_cl.reset_index(drop=True)\n",
    "        df_ds[climate_model] = df_ds_cl\n",
    "\n",
    "    return df_ds\n",
    "\n",
    "def load_storm_data(climate_model,basin,bbox):\n",
    "\n",
    "    filename = os.path.join(tc_path, f'STORM_FIXED_RETURN_PERIODS{climate_model}_{basin}.nc')\n",
    "    \n",
    "    # load data from NetCDF file\n",
    "    with xr.open_dataset(filename) as ds:\n",
    "        \n",
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        ds = ds.rio.clip_box(minx=bbox[0], miny=bbox[1], maxx=bbox[2], maxy=bbox[3])\n",
    "        ds['mean_3s'] = ds['mean']/0.88*1.11 #convert 10-min sustained wind speed to 3-s gust wind speed\n",
    "        #print(ds.head())\n",
    "\n",
    "        # get the mean values\n",
    "        df_ds = ds['mean_3s'].to_dataframe().unstack(level=2).reset_index()\n",
    "\n",
    "        # create geometry values and drop lat lon columns\n",
    "        df_ds['geometry'] = [pygeos.points(x) for x in list(zip(df_ds['lon'], df_ds['lat']))]\n",
    "        df_ds = df_ds.drop(['lat', 'lon'], axis=1, level=0)\n",
    "        \n",
    "        # interpolate wind speeds of 1,2,5,25,and 250-yr return period\n",
    "        ## rename columns to return periods (must be integer for interpolating)\n",
    "        df_ds_geometry = pd.DataFrame()\n",
    "        df_ds_geometry['geometry'] = df_ds['geometry']\n",
    "        df_ds = df_ds.drop(['geometry'], axis=1, level=0)\n",
    "        df_ds = df_ds['mean_3s']\n",
    "        df_ds.columns = [int(x) for x in ds['mean_3s']['rp']]\n",
    "        df_ds[1] = np.nan\n",
    "        df_ds[2] = np.nan\n",
    "        df_ds[5] = np.nan\n",
    "        df_ds[25] = np.nan\n",
    "        df_ds[250] = np.nan\n",
    "        df_ds = df_ds.reindex(sorted(df_ds.columns), axis=1)\n",
    "        df_ds = df_ds.interpolate(method='linear', axis=1, limit_direction='both')\n",
    "        df_ds['geometry'] = df_ds_geometry['geometry']\n",
    "        df_ds = df_ds[[1, 2, 5, 10, 25, 50, 100, 250, 500, 1000, 'geometry']]\n",
    "        \n",
    "        \n",
    "        # rename columns to return periods\n",
    "        df_ds.columns = ['1_{}{}'.format(int(x), climate_model) for x in [1, 2, 5, 10, 25, 50, 100, 250, 500, 1000]] +['geometry']     \n",
    "        df_ds['geometry'] = pygeos.buffer(df_ds.geometry, radius=0.1/2, cap_style='square').values\n",
    "        #print(type(df_ds['geometry'][0]))\n",
    "        # reproject the geometry column to the specified CRS\n",
    "        df_ds['geometry'] = reproject(df_ds)\n",
    "        #print(type(df_ds['geometry'][0]))\n",
    "        #print(df_ds.head())\n",
    "            \n",
    "        # drop all non values to reduce size\n",
    "        #df_ds = df_ds.loc[~df_ds['1_10000{}'.format(climate_model)].isna()].reset_index(drop=True)\n",
    "        df_ds = df_ds.fillna(0)\n",
    "        #print(type(df_ds['geometry'][0]))\n",
    "\n",
    "    return df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6b43b71-49ee-4aee-aca7-db18ecbd8b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brn_wind=open_storm_data('BRN')\n",
    "#print(type(twn_wind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7059f5f8-fff4-4827-a187-bebcf957230d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_1_HadGEM3-GC31-HM</th>\n",
       "      <th>1_2_HadGEM3-GC31-HM</th>\n",
       "      <th>1_5_HadGEM3-GC31-HM</th>\n",
       "      <th>1_10_HadGEM3-GC31-HM</th>\n",
       "      <th>1_25_HadGEM3-GC31-HM</th>\n",
       "      <th>1_50_HadGEM3-GC31-HM</th>\n",
       "      <th>1_100_HadGEM3-GC31-HM</th>\n",
       "      <th>1_250_HadGEM3-GC31-HM</th>\n",
       "      <th>1_500_HadGEM3-GC31-HM</th>\n",
       "      <th>1_1000_HadGEM3-GC31-HM</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.469543</td>\n",
       "      <td>40.469543</td>\n",
       "      <td>40.469543</td>\n",
       "      <td>40.469543</td>\n",
       "      <td>47.006968</td>\n",
       "      <td>51.136926</td>\n",
       "      <td>54.000687</td>\n",
       "      <td>57.706195</td>\n",
       "      <td>59.990146</td>\n",
       "      <td>61.671156</td>\n",
       "      <td>POLYGON ((13725693.215 4397372.744, 13725693.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.559481</td>\n",
       "      <td>40.559481</td>\n",
       "      <td>40.559481</td>\n",
       "      <td>40.559481</td>\n",
       "      <td>46.998784</td>\n",
       "      <td>51.011116</td>\n",
       "      <td>54.276254</td>\n",
       "      <td>57.470437</td>\n",
       "      <td>59.796552</td>\n",
       "      <td>61.367880</td>\n",
       "      <td>POLYGON ((13736825.164 4397372.744, 13736825.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.576730</td>\n",
       "      <td>40.576730</td>\n",
       "      <td>40.576730</td>\n",
       "      <td>40.576730</td>\n",
       "      <td>47.074321</td>\n",
       "      <td>50.719132</td>\n",
       "      <td>54.085564</td>\n",
       "      <td>57.368295</td>\n",
       "      <td>59.887502</td>\n",
       "      <td>61.455106</td>\n",
       "      <td>POLYGON ((13747957.113 4397372.744, 13747957.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.619581</td>\n",
       "      <td>40.619581</td>\n",
       "      <td>40.619581</td>\n",
       "      <td>40.619581</td>\n",
       "      <td>47.139382</td>\n",
       "      <td>50.878822</td>\n",
       "      <td>53.868798</td>\n",
       "      <td>57.394859</td>\n",
       "      <td>59.747586</td>\n",
       "      <td>61.142801</td>\n",
       "      <td>POLYGON ((13759089.062 4397372.744, 13759089.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.767273</td>\n",
       "      <td>40.767273</td>\n",
       "      <td>40.767273</td>\n",
       "      <td>40.767273</td>\n",
       "      <td>47.236443</td>\n",
       "      <td>50.914861</td>\n",
       "      <td>53.647720</td>\n",
       "      <td>57.101056</td>\n",
       "      <td>59.263981</td>\n",
       "      <td>61.107820</td>\n",
       "      <td>POLYGON ((13770221.011 4397372.744, 13770221.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370</th>\n",
       "      <td>28.165473</td>\n",
       "      <td>28.165473</td>\n",
       "      <td>28.165473</td>\n",
       "      <td>28.165473</td>\n",
       "      <td>36.016688</td>\n",
       "      <td>41.022344</td>\n",
       "      <td>44.180265</td>\n",
       "      <td>48.861651</td>\n",
       "      <td>51.562568</td>\n",
       "      <td>54.256075</td>\n",
       "      <td>POLYGON ((14616249.141 5480930.477, 14616249.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6371</th>\n",
       "      <td>28.399414</td>\n",
       "      <td>28.399414</td>\n",
       "      <td>28.399414</td>\n",
       "      <td>28.399414</td>\n",
       "      <td>36.329825</td>\n",
       "      <td>41.091320</td>\n",
       "      <td>44.535876</td>\n",
       "      <td>48.959105</td>\n",
       "      <td>51.567495</td>\n",
       "      <td>54.432705</td>\n",
       "      <td>POLYGON ((14627381.09 5480930.477, 14627381.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6372</th>\n",
       "      <td>28.670507</td>\n",
       "      <td>28.670507</td>\n",
       "      <td>28.670507</td>\n",
       "      <td>28.670507</td>\n",
       "      <td>36.401315</td>\n",
       "      <td>40.999775</td>\n",
       "      <td>45.117046</td>\n",
       "      <td>49.259744</td>\n",
       "      <td>51.713517</td>\n",
       "      <td>54.310172</td>\n",
       "      <td>POLYGON ((14638513.039 5480930.477, 14638513.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6373</th>\n",
       "      <td>28.952827</td>\n",
       "      <td>28.952827</td>\n",
       "      <td>28.952827</td>\n",
       "      <td>28.952827</td>\n",
       "      <td>36.496510</td>\n",
       "      <td>41.032414</td>\n",
       "      <td>45.355875</td>\n",
       "      <td>49.466124</td>\n",
       "      <td>52.108267</td>\n",
       "      <td>54.097208</td>\n",
       "      <td>POLYGON ((14649644.988 5480930.477, 14649644.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6374</th>\n",
       "      <td>29.290191</td>\n",
       "      <td>29.290191</td>\n",
       "      <td>29.290191</td>\n",
       "      <td>29.290191</td>\n",
       "      <td>36.657309</td>\n",
       "      <td>41.175934</td>\n",
       "      <td>45.435669</td>\n",
       "      <td>49.921387</td>\n",
       "      <td>52.398132</td>\n",
       "      <td>54.091844</td>\n",
       "      <td>POLYGON ((14660776.937 5480930.477, 14660776.9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6375 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1_1_HadGEM3-GC31-HM  1_2_HadGEM3-GC31-HM  1_5_HadGEM3-GC31-HM  \\\n",
       "0               40.469543            40.469543            40.469543   \n",
       "1               40.559481            40.559481            40.559481   \n",
       "2               40.576730            40.576730            40.576730   \n",
       "3               40.619581            40.619581            40.619581   \n",
       "4               40.767273            40.767273            40.767273   \n",
       "...                   ...                  ...                  ...   \n",
       "6370            28.165473            28.165473            28.165473   \n",
       "6371            28.399414            28.399414            28.399414   \n",
       "6372            28.670507            28.670507            28.670507   \n",
       "6373            28.952827            28.952827            28.952827   \n",
       "6374            29.290191            29.290191            29.290191   \n",
       "\n",
       "      1_10_HadGEM3-GC31-HM  1_25_HadGEM3-GC31-HM  1_50_HadGEM3-GC31-HM  \\\n",
       "0                40.469543             47.006968             51.136926   \n",
       "1                40.559481             46.998784             51.011116   \n",
       "2                40.576730             47.074321             50.719132   \n",
       "3                40.619581             47.139382             50.878822   \n",
       "4                40.767273             47.236443             50.914861   \n",
       "...                    ...                   ...                   ...   \n",
       "6370             28.165473             36.016688             41.022344   \n",
       "6371             28.399414             36.329825             41.091320   \n",
       "6372             28.670507             36.401315             40.999775   \n",
       "6373             28.952827             36.496510             41.032414   \n",
       "6374             29.290191             36.657309             41.175934   \n",
       "\n",
       "      1_100_HadGEM3-GC31-HM  1_250_HadGEM3-GC31-HM  1_500_HadGEM3-GC31-HM  \\\n",
       "0                 54.000687              57.706195              59.990146   \n",
       "1                 54.276254              57.470437              59.796552   \n",
       "2                 54.085564              57.368295              59.887502   \n",
       "3                 53.868798              57.394859              59.747586   \n",
       "4                 53.647720              57.101056              59.263981   \n",
       "...                     ...                    ...                    ...   \n",
       "6370              44.180265              48.861651              51.562568   \n",
       "6371              44.535876              48.959105              51.567495   \n",
       "6372              45.117046              49.259744              51.713517   \n",
       "6373              45.355875              49.466124              52.108267   \n",
       "6374              45.435669              49.921387              52.398132   \n",
       "\n",
       "      1_1000_HadGEM3-GC31-HM  \\\n",
       "0                  61.671156   \n",
       "1                  61.367880   \n",
       "2                  61.455106   \n",
       "3                  61.142801   \n",
       "4                  61.107820   \n",
       "...                      ...   \n",
       "6370               54.256075   \n",
       "6371               54.432705   \n",
       "6372               54.310172   \n",
       "6373               54.097208   \n",
       "6374               54.091844   \n",
       "\n",
       "                                               geometry  \n",
       "0     POLYGON ((13725693.215 4397372.744, 13725693.2...  \n",
       "1     POLYGON ((13736825.164 4397372.744, 13736825.1...  \n",
       "2     POLYGON ((13747957.113 4397372.744, 13747957.1...  \n",
       "3     POLYGON ((13759089.062 4397372.744, 13759089.0...  \n",
       "4     POLYGON ((13770221.011 4397372.744, 13770221.0...  \n",
       "...                                                 ...  \n",
       "6370  POLYGON ((14616249.141 5480930.477, 14616249.1...  \n",
       "6371  POLYGON ((14627381.09 5480930.477, 14627381.09...  \n",
       "6372  POLYGON ((14638513.039 5480930.477, 14638513.0...  \n",
       "6373  POLYGON ((14649644.988 5480930.477, 14649644.9...  \n",
       "6374  POLYGON ((14660776.937 5480930.477, 14660776.9...  \n",
       "\n",
       "[6375 rows x 11 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prk_wind['_HadGEM3-GC31-HM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c7ff6dd-a3f2-4e2a-becd-e5cffa38edde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clip_flood_data(country_code):\n",
    "    \n",
    "    # load country geometry file and create geometry to clip\n",
    "    ne_countries = gpd.read_file(ne_path)\n",
    "    geometry = ne_countries.loc[ne_countries['ISO_A3']==country_code].geometry.values[0]\n",
    "    geoms = [mapping(geometry)]\n",
    "    \n",
    "    #climate_model: historical, rcp4p5, rcp8p5; time_period: hist, 2030, 2050, 2080\n",
    "    rps = ['0001','0002','0005','0010','0025','0050','0100','0250','0500','1000']\n",
    "    climate_models = ['historical','rcp8p5']\n",
    "    \n",
    "    #\"/scistor/ivm/data_catalogue/open_street_map/pg_risk_analysis/GLOFRIS/global/inuncoast_historical_nosub_hist_rp0001_0.tif\"\n",
    "\n",
    "    for rp in rps:\n",
    "        #global input_file\n",
    "        for climate_model in climate_models:\n",
    "            if climate_model=='historical':\n",
    "                input_file = os.path.join(fl_path,'global',\n",
    "                                          'inuncoast_{}_nosub_hist_rp{}_0.tif'.format(climate_model,rp)) \n",
    " \n",
    "            elif climate_model=='rcp8p5':\n",
    "                input_file = os.path.join(fl_path,'global',\n",
    "                                          'inuncoast_{}_nosub_2030_rp{}_0.tif'.format(climate_model,rp))\n",
    "\n",
    "            # load raster file and save clipped version\n",
    "            with rasterio.open(input_file) as src:\n",
    "                out_image, out_transform = mask(src, geoms, crop=True)\n",
    "                out_meta = src.meta\n",
    "\n",
    "                out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "\n",
    "                if 'scistor' in fl_path:\n",
    "                    file_path = os.path.join(fl_path,'country','_'.join([country_code]+input_file.split('_')[6:]))\n",
    "                else:\n",
    "                    file_path = os.path.join(fl_path,'country','_'.join([country_code]+input_file.split('_')[3:]))\n",
    "\n",
    "                with rasterio.open(file_path, \"w\", **out_meta) as dest:\n",
    "                    dest.write(out_image)\n",
    "\n",
    "def load_flood_data(country_code,climate_model):\n",
    "     \n",
    "    rps = ['0001','0002','0005','0010','0025','0050','0100','0250','0500','1000']\n",
    "    collect_df_ds = []\n",
    "    \n",
    "    if climate_model=='historical':\n",
    "        print('Loading historical coastal flood data ...')\n",
    "        for rp in rps:\n",
    "            #for file in files:\n",
    "            file_path = os.path.join(fl_path,'country','{}_{}_nosub_hist_rp{}_0.tif'.format(country_code,climate_model,rp))\n",
    "            with xr.open_dataset(file_path) as ds: #, engine=\"rasterio\"\n",
    "                df_ds = ds.to_dataframe().reset_index()\n",
    "                df_ds['geometry'] = pygeos.points(df_ds.x,y=df_ds.y)\n",
    "                df_ds = df_ds.rename(columns={'band_data': 'rp'+rp}) #rename to return period\n",
    "                \n",
    "                # move from meters to centimeters\n",
    "                df_ds['rp'+rp] = (df_ds['rp'+rp]*100)         \n",
    "                df_ds = df_ds.drop(['band','x', 'y','spatial_ref'], axis=1)\n",
    "                df_ds = df_ds.dropna()\n",
    "                df_ds = df_ds.reset_index(drop=True)\n",
    "                df_ds.geometry= pygeos.buffer(df_ds.geometry,radius=0.00833/2,cap_style='square').values  #?????????????????????????\n",
    "                df_ds['geometry'] = reproject(df_ds)\n",
    "                collect_df_ds.append(df_ds)\n",
    "\n",
    "        df_all = collect_df_ds[0].merge(collect_df_ds[1]).merge(collect_df_ds[2]).merge(collect_df_ds[3]).merge(collect_df_ds[4])\\\n",
    "                 .merge(collect_df_ds[5]).merge(collect_df_ds[6]).merge(collect_df_ds[7]).merge(collect_df_ds[8]).merge(collect_df_ds[9])\n",
    "        df_all = df_all.loc[df_all['rp1000']>0].reset_index(drop=True)\n",
    "\n",
    "    elif climate_model=='rcp8p5':\n",
    "        print('Loading future coastal flood data ...')\n",
    "        for rp in rps:\n",
    "            #for file in files:\n",
    "            file_path = os.path.join(fl_path,'country','{}_{}_nosub_2030_rp{}_0.tif'.format(country_code,climate_model,rp))\n",
    "            with xr.open_dataset(file_path) as ds: #, engine=\"rasterio\"\n",
    "                df_ds = ds.to_dataframe().reset_index()\n",
    "                df_ds['geometry'] = pygeos.points(df_ds.x,y=df_ds.y)\n",
    "                df_ds = df_ds.rename(columns={'band_data': 'rp'+rp}) #rename to return period\n",
    "                df_ds['rp'+rp] = (df_ds['rp'+rp]*100)\n",
    "                df_ds = df_ds.drop(['band','x', 'y','spatial_ref'], axis=1)\n",
    "                df_ds = df_ds.dropna()\n",
    "                df_ds = df_ds.reset_index(drop=True)\n",
    "                df_ds.geometry= pygeos.buffer(df_ds.geometry,radius=0.00833/2,cap_style='square').values\n",
    "                df_ds['geometry'] = reproject(df_ds)\n",
    "                collect_df_ds.append(df_ds)\n",
    "\n",
    "        df_all = collect_df_ds[0].merge(collect_df_ds[1]).merge(collect_df_ds[2]).merge(collect_df_ds[3]).merge(collect_df_ds[4])\\\n",
    "                 .merge(collect_df_ds[5]).merge(collect_df_ds[6]).merge(collect_df_ds[7]).merge(collect_df_ds[8]).merge(collect_df_ds[9])\n",
    "\n",
    "        df_all = df_all.loc[df_all['rp1000']>0].reset_index(drop=True)\n",
    "    return df_all\n",
    "\n",
    "def open_flood_data(country_code):\n",
    "    climate_models = ['historical','rcp8p5']\n",
    "    df_ds = {}\n",
    "    for climate_model in climate_models:\n",
    "        df_ds_sc = load_flood_data(country_code,climate_model)\n",
    "        df_ds[climate_model] = df_ds_sc\n",
    "    \n",
    "    return df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "2160cb1f-75ac-48f0-898e-6fdcb8bb3b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_flood_data('JPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5ac9f48-e434-4893-8c04-07e663921d94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n",
      "CPU times: total: 1min 54s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prk_flood = open_flood_data('PRK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4cb2960-1dbe-4c3e-b9c9-4a65773bddd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(prk_flood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6e24a4d-35cc-4bdd-8d69-4d2c504fe1fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'historical':          rp0001                                           geometry  \\\n",
       " 0      0.000000  POLYGON ((13844433.819 4850211.799, 13844433.8...   \n",
       " 1    213.443848  POLYGON ((13846289.144 4881697.253, 13846289.1...   \n",
       " 2     57.775497  POLYGON ((13846289.144 4862309.77, 13846289.14...   \n",
       " 3      0.000000  POLYGON ((13846289.144 4861099.31, 13846289.14...   \n",
       " 4      9.117842  POLYGON ((13846289.144 4859888.997, 13846289.1...   \n",
       " ..          ...                                                ...   \n",
       " 372   67.400185  POLYGON ((14362069.452 4935209.743, 14362069.4...   \n",
       " 373    0.304127  POLYGON ((14364852.439 4936429.321, 14364852.4...   \n",
       " 374   59.260742  POLYGON ((14414946.21 4982886.071, 14414946.21...   \n",
       " 375   69.419838  POLYGON ((14415873.872 4984111.602, 14415873.8...   \n",
       " 376   70.671585  POLYGON ((14415873.872 4982886.071, 14415873.8...   \n",
       " \n",
       "          rp0002      rp0005      rp0010      rp0025      rp0050      rp0100  \\\n",
       " 0      0.000000    0.000000    4.555368   11.911917   17.369461   22.786665   \n",
       " 1    217.017227  225.811142  231.633469  238.990021  244.447571  249.864777   \n",
       " 2     61.348869   70.142792   75.965118   83.321671   88.779213   94.196411   \n",
       " 3      0.000000    6.405592   12.227917   19.584465   25.042009   30.459213   \n",
       " 4     12.691212   21.485138   27.307463   34.664009   40.121555   45.538757   \n",
       " ..          ...         ...         ...         ...         ...         ...   \n",
       " 372   68.849197   72.415154   74.776138   77.759239   79.972275   82.168968   \n",
       " 373    1.753134    5.319095    7.680077   10.663176   12.876213   15.072906   \n",
       " 374   60.516293   63.606159   65.651924   68.236755   70.154327   72.057739   \n",
       " 375   70.675385   73.765251   75.811020   78.395851   80.313423   82.216835   \n",
       " 376   71.927139   75.017006   77.062767   79.647598   81.565170   83.468590   \n",
       " \n",
       "          rp0250      rp0500      rp1000  \n",
       " 0     29.919291   35.305023   40.686844  \n",
       " 1    256.997406  262.383118  267.764954  \n",
       " 2    101.329041  106.714775  112.096596  \n",
       " 3     37.591839   42.977570   48.359394  \n",
       " 4     52.671387   58.057117   63.438942  \n",
       " ..          ...         ...         ...  \n",
       " 372   85.061279   87.245193   89.427536  \n",
       " 373   17.965221   20.149136   22.331476  \n",
       " 374   74.563904   76.456245   78.347221  \n",
       " 375   84.722992   86.615334   88.506317  \n",
       " 376   85.974747   87.867088   89.758072  \n",
       " \n",
       " [377 rows x 11 columns],\n",
       " 'rcp8p5':          rp0001                                           geometry  \\\n",
       " 0      3.465748  POLYGON ((13844433.819 4850211.799, 13844433.8...   \n",
       " 1    230.543854  POLYGON ((13846289.144 4881697.253, 13846289.1...   \n",
       " 2     74.875496  POLYGON ((13846289.144 4862309.77, 13846289.14...   \n",
       " 3     11.138296  POLYGON ((13846289.144 4861099.31, 13846289.14...   \n",
       " 4     26.217842  POLYGON ((13846289.144 4859888.997, 13846289.1...   \n",
       " ..          ...                                                ...   \n",
       " 428   85.350189  POLYGON ((14362069.452 4935209.743, 14362069.4...   \n",
       " 429   18.254126  POLYGON ((14364852.439 4936429.321, 14364852.4...   \n",
       " 430   80.540749  POLYGON ((14414946.21 4982886.071, 14414946.21...   \n",
       " 431   90.699837  POLYGON ((14415873.872 4984111.602, 14415873.8...   \n",
       " 432   91.951591  POLYGON ((14415873.872 4982886.071, 14415873.8...   \n",
       " \n",
       "          rp0002      rp0005      rp0010      rp0025      rp0050      rp0100  \\\n",
       " 0      7.039118   15.833044   21.655369   29.011917   34.469460   39.886665   \n",
       " 1    234.117218  242.911148  248.733475  256.090027  261.547577  266.964783   \n",
       " 2     78.448868   87.242798   93.065117  100.421669  105.879211  111.296417   \n",
       " 3     14.711666   23.505592   29.327917   36.684464   42.142010   47.559212   \n",
       " 4     29.791212   38.585136   44.407463   51.764011   57.221558   62.638760   \n",
       " ..          ...         ...         ...         ...         ...         ...   \n",
       " 428   86.799194   90.365150   92.726135   95.709236   97.922272  100.118973   \n",
       " 429   19.703133   23.269093   25.630075   28.613174   30.826210   33.022903   \n",
       " 430   81.796295   84.886162   86.931923   89.516762   91.434326   93.337746   \n",
       " 431   91.955391   95.045258   97.091019   99.675850  101.593422  103.496841   \n",
       " 432   93.207138   96.297005   98.342773  100.927605  102.845169  104.748581   \n",
       " \n",
       "          rp0250      rp0500      rp1000  \n",
       " 0     47.019291   52.405022   57.786846  \n",
       " 1    274.097382  279.483124  284.864960  \n",
       " 2    118.429039  123.814774  129.196594  \n",
       " 3     54.691841   60.077572   65.459396  \n",
       " 4     69.771385   75.157120   80.538940  \n",
       " ..          ...         ...         ...  \n",
       " 428  103.011284  105.195198  107.377541  \n",
       " 429   35.915218   38.099133   40.281475  \n",
       " 430   95.843903   97.736244   99.627228  \n",
       " 431  106.002998  107.895340  109.786316  \n",
       " 432  107.254745  109.147087  111.038063  \n",
       " \n",
       " [433 rows x 11 columns]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prk_flood#['historical']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3de688-606c-4dd2-abe3-f37015c7442e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OSM data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "934161b6-5f01-4315-87b3-9e4a7bd70876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_osm_infrastructure(country_code,osm_data_path):\n",
    "    # lines\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    osm_lines = power_polyline(osm_path)\n",
    "    osm_lines['geometry'] = reproject(osm_lines)\n",
    "    osm_lines = buffer_assets(osm_lines.loc[osm_lines.asset.isin(\n",
    "        ['cable','minor_cable','line','minor_line'])],buffer_size=100).reset_index(drop=True)\n",
    "    \n",
    "    # polygons\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    osm_polygons = electricity(osm_path)\n",
    "    osm_polygons['geometry'] = reproject(osm_polygons)\n",
    "    \n",
    "    # points\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    osm_points = power_point(osm_path)\n",
    "    osm_points['geometry'] = reproject(osm_points)\n",
    "    osm_points = buffer_assets(osm_points.loc[osm_points.asset.isin(\n",
    "        ['power_tower','power_pole'])],buffer_size=100).reset_index(drop=True)\n",
    "    \n",
    "    return osm_lines,osm_polygons,osm_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b16fbb7a-d740-4b39-bfdd-127321fb4cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87/87 [00:00<00:00, 470.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:00<00:00, 82.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8774/8774 [00:00<00:00, 9981.35it/s]\n"
     ]
    }
   ],
   "source": [
    "osm_power_infra = extract_osm_infrastructure('BRN',osm_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37ec69ba-f890-4f1b-83f0-010676b57e50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(         osm_id asset voltage  \\\n",
       " 0     105187949  line    None   \n",
       " 1     105187955  line    None   \n",
       " 2     105188022  line    None   \n",
       " 3     107025236  line    None   \n",
       " 4     123992103  line  154000   \n",
       " ..          ...   ...     ...   \n",
       " 842  1082904916  line    None   \n",
       " 843  1082904917  line    None   \n",
       " 844  1085391998  line    None   \n",
       " 845  1102076042  line  154000   \n",
       " 846  1102076043  line  154000   \n",
       " \n",
       "                                               geometry  \\\n",
       " 0    LINESTRING (14173655.447 4825037.842, 14173579...   \n",
       " 1    LINESTRING (14178276.586 4800804.001, 14178162...   \n",
       " 2    LINESTRING (14163631.45 4784350.366, 14163637....   \n",
       " 3    LINESTRING (14176146.098 4797211.802, 14175880...   \n",
       " 4    LINESTRING (14097165.409 4569256.468, 14097614...   \n",
       " ..                                                 ...   \n",
       " 842  LINESTRING (13960258.8 4704246.146, 13960328.3...   \n",
       " 843  LINESTRING (13963363.767 4700422.189, 13963348...   \n",
       " 844  LINESTRING (13998362.482 4743470.643, 13998420...   \n",
       " 845  LINESTRING (14097165.409 4569256.468, 14097131...   \n",
       " 846  LINESTRING (14097165.409 4569256.468, 14097132...   \n",
       " \n",
       "                                               buffered  \n",
       " 0    POLYGON ((14173679.16 4824237.07, 14173679.019...  \n",
       " 1    POLYGON ((14178253.894 4800513.963, 14178192.4...  \n",
       " 2    POLYGON ((14163737.512 4784237.762, 14163736.3...  \n",
       " 3    POLYGON ((14175827.277 4797293.684, 14175822.3...  \n",
       " 4    POLYGON ((14097628.971 4569288.16, 14097629.18...  \n",
       " ..                                                 ...  \n",
       " 842  POLYGON ((13960393.128 4704263.212, 13960398.3...  \n",
       " 843  POLYGON ((13963254.881 4700427.179, 13963254.4...  \n",
       " 844  POLYGON ((13998343.21 4743604.392, 13998345.81...  \n",
       " 845  POLYGON ((14097153.734 4569151.368, 14097134.2...  \n",
       " 846  POLYGON ((14097104.1 4569170.158, 14097085.905...  \n",
       " \n",
       " [847 rows x 5 columns],\n",
       "      osm_id  asset                                           geometry\n",
       " 0   6966279  plant  MULTIPOLYGON (((14064889.614 4833427.572, 1406...\n",
       " 1   8310824  plant  MULTIPOLYGON (((14277215.313 5120499.658, 1427...\n",
       " 2      None  plant  MULTIPOLYGON (((14023237.033 4997533.372, 1402...\n",
       " 3      None  plant  MULTIPOLYGON (((13911073.062 4932922.15, 13911...\n",
       " 4      None  plant  MULTIPOLYGON (((14406260.84 5045985.82, 144062...\n",
       " 5      None  plant  MULTIPOLYGON (((14060802.152 4806472.61, 14061...\n",
       " 6      None  plant  MULTIPOLYGON (((14022181.714 4779305.25, 14022...\n",
       " 7      None  plant  MULTIPOLYGON (((14513349.879 5210314.68, 14513...\n",
       " 8      None  plant  MULTIPOLYGON (((13991524.37 4716830.796, 13991...\n",
       " 9      None  plant  MULTIPOLYGON (((14201127.806 4844509.687, 1420...\n",
       " 10     None  plant  MULTIPOLYGON (((13999785.668 4835145.471, 1399...\n",
       " 11     None  plant  MULTIPOLYGON (((13999082.196 4836810.896, 1399...\n",
       " 12     None  plant  MULTIPOLYGON (((14330448.137 4955642.371, 1433...\n",
       " 13     None  plant  MULTIPOLYGON (((14198735.283 4842810.905, 1419...\n",
       " 14     None  plant  MULTIPOLYGON (((14082306.417 5067451.37, 14082...\n",
       " 15     None  plant  MULTIPOLYGON (((13994390.268 4722735.296, 1399...\n",
       " 16     None  plant  MULTIPOLYGON (((14063697.327 4714085.266, 1406...\n",
       " 17     None  plant  MULTIPOLYGON (((14405972.612 5047790.583, 1440...\n",
       " 18     None  plant  MULTIPOLYGON (((13910994.092 4932914.308, 1391...\n",
       " 19     None  plant  MULTIPOLYGON (((14191409.347 4880081.234, 1419...\n",
       " 20     None  plant  MULTIPOLYGON (((14395775.791 5066705.775, 1439...\n",
       " 21     None  plant  MULTIPOLYGON (((13884765.283 4917407.964, 1388...\n",
       " 22     None  plant  MULTIPOLYGON (((14098415.438 4871913.114, 1409...\n",
       " 23     None  plant  MULTIPOLYGON (((14033823.328 4803728.559, 1403...\n",
       " 24     None  plant  MULTIPOLYGON (((13961039.75 4682983.634, 13961...\n",
       " 25     None  plant  MULTIPOLYGON (((13961157.582 4682773.985, 1396...\n",
       " 26     None  plant  MULTIPOLYGON (((13961306.572 4682853.78, 13961...\n",
       " 27     None  plant  MULTIPOLYGON (((13960914.037 4683243.132, 1396...\n",
       " 28     None  plant  MULTIPOLYGON (((13993957.224 4719836.63, 13993...\n",
       " 29     None  plant  MULTIPOLYGON (((13994234.911 4719848.574, 1399...\n",
       " 30     None  plant  MULTIPOLYGON (((14149532.391 4803973.804, 1414...\n",
       " 31     None  plant  MULTIPOLYGON (((14041611.084 4866273.826, 1404...\n",
       " 32     None  plant  MULTIPOLYGON (((14443385.813 5125678.576, 1444...\n",
       " 33     None  plant  MULTIPOLYGON (((14427425.793 5100345.21, 14427...\n",
       " 34     None  plant  MULTIPOLYGON (((14266027.069 5059295.386, 1426...\n",
       " 35     None  plant  MULTIPOLYGON (((13994490.512 4818638.512, 1399...\n",
       " 36     None  plant  MULTIPOLYGON (((14093274.181 4626107.515, 1409...\n",
       " 37     None  plant  MULTIPOLYGON (((14198023.284 5082718.092, 1419...\n",
       " 38     None  plant  MULTIPOLYGON (((14416937.793 5071984.977, 1441...\n",
       " 39     None  plant  MULTIPOLYGON (((13960922.097 4683218.788, 1396...,\n",
       "             osm_id        asset                          geometry  \\\n",
       " 0       1212135617  power_tower  POINT (14173666.089 4787906.398)   \n",
       " 1       1212135625  power_tower  POINT (14171268.468 4787421.096)   \n",
       " 2       1212135629  power_tower   POINT (14169518.18 4780015.759)   \n",
       " 3       1212135650  power_tower  POINT (14172099.401 4798338.883)   \n",
       " 4       1212135679  power_tower  POINT (14174089.894 4790613.034)   \n",
       " ...            ...          ...                               ...   \n",
       " 22742   9964213065  power_tower  POINT (13998111.668 4733598.226)   \n",
       " 22743   9987006737  power_tower  POINT (13994461.079 4715258.749)   \n",
       " 22744  10084431453  power_tower  POINT (14108426.612 4560927.727)   \n",
       " 22745  10084431454  power_tower    POINT (14107908.942 4561591.9)   \n",
       " 22746  10084431455  power_tower  POINT (14106882.755 4562645.415)   \n",
       " \n",
       "                                                 buffered  \n",
       " 0      POLYGON ((14173766.089 4787906.398, 14173764.1...  \n",
       " 1      POLYGON ((14171368.468 4787421.096, 14171366.5...  \n",
       " 2      POLYGON ((14169618.18 4780015.759, 14169616.25...  \n",
       " 3      POLYGON ((14172199.401 4798338.883, 14172197.4...  \n",
       " 4      POLYGON ((14174189.894 4790613.034, 14174187.9...  \n",
       " ...                                                  ...  \n",
       " 22742  POLYGON ((13998211.668 4733598.226, 13998209.7...  \n",
       " 22743  POLYGON ((13994561.079 4715258.749, 13994559.1...  \n",
       " 22744  POLYGON ((14108526.612 4560927.727, 14108524.6...  \n",
       " 22745  POLYGON ((14108008.942 4561591.9, 14108007.021...  \n",
       " 22746  POLYGON ((14106982.755 4562645.415, 14106980.8...  \n",
       " \n",
       " [22747 rows x 4 columns])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_power_infra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dc748e9-7357-4296-ab28-711503d43cd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(type(osm_power_infra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "abb7a943-1950-445c-8328-f6db95b7f163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_damage_osm(country_code,osm_power_infra,hazard_type):\n",
    "\n",
    "    # load curves and maxdam\n",
    "    curves,maxdam = load_curves_maxdam(vul_curve_path,hazard_type)\n",
    "    \n",
    "    # read infrastructure data:\n",
    "    osm_lines,osm_poly,osm_points = osm_power_infra\n",
    "    #print(osm_lines['asset'].unique())\n",
    "    \n",
    "    #calculate damaged lines/polygons/points in loop by climate_model\n",
    "    damaged_lines = {}\n",
    "    damaged_poly = {}\n",
    "    damaged_points = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        # read wind data\n",
    "        climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "        df_ds = open_storm_data(country_code)\n",
    "\n",
    "        # remove assets that will not have any damage\n",
    "        osm_lines = osm_lines.loc[osm_lines.asset != 'cable'].reset_index(drop=True)\n",
    "        osm_lines['asset'] = osm_lines['asset'].replace(['minor_line'], 'line')\n",
    "        osm_poly = osm_poly.loc[osm_poly.asset != 'plant'].reset_index(drop=True)\n",
    "        \n",
    "        for climate_model in climate_models:\n",
    "            return_periods = ['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),'1_10{}'.format(climate_model),\n",
    "                              '1_25{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),\n",
    "                              '1_250{}'.format(climate_model),'1_500{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "            \n",
    "            # assess damage for lines\n",
    "            overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_lines).T,\n",
    "                                         columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_lines) == 0:\n",
    "                damaged_lines[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_line_damages = []\n",
    "                for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                                  desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_line_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_lines,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_line = dict(zip(osm_lines.index,osm_lines.asset))\n",
    "                results = pd.DataFrame([item for sublist in collect_line_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            # assess damage for polygons\n",
    "            if len(osm_poly) > 0:\n",
    "                overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_poly).T,\n",
    "                                        columns=['asset','hazard_point'])\n",
    "            else:\n",
    "                overlay_poly = pd.DataFrame()\n",
    "\n",
    "            if len(overlay_poly) == 0:\n",
    "                damaged_poly[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_poly_damages = []\n",
    "                for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),\n",
    "                                  desc='polygon damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_poly_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_poly,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_poly = dict(zip(osm_poly.index,osm_poly.asset))\n",
    "                \n",
    "                results = pd.DataFrame([item for sublist in collect_poly_damages \n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_poly[x])    \n",
    "\n",
    "                damaged_poly[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            # assess damage for points\n",
    "            overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_points).T,\n",
    "                                          columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_points) == 0:\n",
    "                damaged_points[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_point_damages = []\n",
    "                for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                                  desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_point_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_points,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_point = dict(zip(osm_points.index,osm_points.asset))\n",
    "                \n",
    "                results = pd.DataFrame([item for sublist in collect_point_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])    \n",
    "\n",
    "                damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "\n",
    "    elif hazard_type=='fl':\n",
    "        # read flood data\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "        df_ds = open_flood_data(country_code)\n",
    "    \n",
    "        for climate_model in climate_models:\n",
    "            return_periods = ['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'] \n",
    "\n",
    "            # assess damage for lines\n",
    "            overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_lines).T,\n",
    "                                         columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_lines) == 0:\n",
    "                damaged_lines[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_line_damages = []\n",
    "                for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                                  desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_line_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_lines,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_line = dict(zip(osm_lines.index,osm_lines.asset))\n",
    "                results = pd.DataFrame(collect_line_damages,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                #sum damage of line, cable, and minor_line\n",
    "                results['curve'] = results['curve'].replace(['cable', 'minor_line'], 'line')\n",
    "                results['asset_type'] = results['asset_type'].replace(['cable', 'minor_line'], 'line')\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            # assess damage for polygons\n",
    "            if len(osm_poly) > 0:\n",
    "                overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_poly).T,\n",
    "                                        columns=['asset','hazard_point'])\n",
    "            else:\n",
    "                overlay_poly = pd.DataFrame()\n",
    "\n",
    "            if len(overlay_poly) == 0:\n",
    "                damaged_poly[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_poly_damages = []\n",
    "                for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),\n",
    "                                  desc='polygon damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_poly_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_poly,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_poly = dict(zip(osm_poly.index,osm_poly.asset))\n",
    "                results = pd.DataFrame(collect_poly_damages ,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_poly[x])    \n",
    "\n",
    "                damaged_poly[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            # assess damage for points\n",
    "            overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_points).T,\n",
    "                                          columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_points) == 0:\n",
    "                damaged_points[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_point_damages = []\n",
    "                for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                                  desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_point_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_points,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_point = dict(zip(osm_points.index,osm_points.asset))\n",
    "                results = pd.DataFrame(collect_point_damages ,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])    \n",
    "\n",
    "                damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "    return damaged_lines,damaged_poly,damaged_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b22ed8b1-8d29-4bdc-b2f5-8761662044e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point damage calculation for PRK fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:00<00:00, 60.38it/s]\n",
      "point damage calculation for PRK fl (rcp8p5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:00<00:00, 59.42it/s]\n"
     ]
    }
   ],
   "source": [
    "osm_damage_infra = assess_damage_osm('PRK',osm_power_infra,'fl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5a26e90-9cc0-43f1-9fa4-64eade38f82f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polygon damage calculation for BRN tc (): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.61it/s]\n",
      "polygon damage calculation for BRN tc (_CMCC-CM2-VHR4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "osm_damage_infra = assess_damage_osm('BRN',osm_power_infra,'tc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "313d91ab-0996-4aa6-8de6-49b0cb2bf424",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rp</th>\n",
       "      <th>curve</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>meandam</th>\n",
       "      <th>lowerdam</th>\n",
       "      <th>upperdam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1000_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_1_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>0.018276</td>\n",
       "      <td>0.013707</td>\n",
       "      <td>0.022845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1000_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_1_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>0.036551</td>\n",
       "      <td>0.027413</td>\n",
       "      <td>0.045689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1000_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_1_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>0.091378</td>\n",
       "      <td>0.068534</td>\n",
       "      <td>0.114223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_1000_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_2_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1000_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_2_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1_5_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_6_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>84270.363554</td>\n",
       "      <td>63202.772666</td>\n",
       "      <td>105337.954443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>1_5_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_6_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>821.411243</td>\n",
       "      <td>616.058432</td>\n",
       "      <td>1026.764053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1_5_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_7_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>29009.357101</td>\n",
       "      <td>21757.017826</td>\n",
       "      <td>36261.696376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1_5_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_7_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>59462.163961</td>\n",
       "      <td>44596.622971</td>\n",
       "      <td>74327.704951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1_5_CMCC-CM2-VHR4</td>\n",
       "      <td>W2_7_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>169661.197440</td>\n",
       "      <td>127245.898080</td>\n",
       "      <td>212076.496801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       rp   curve  asset_type        meandam       lowerdam  \\\n",
       "0    1_1000_CMCC-CM2-VHR4  W2_1_1  substation       0.018276       0.013707   \n",
       "1    1_1000_CMCC-CM2-VHR4  W2_1_2  substation       0.036551       0.027413   \n",
       "2    1_1000_CMCC-CM2-VHR4  W2_1_3  substation       0.091378       0.068534   \n",
       "3    1_1000_CMCC-CM2-VHR4  W2_2_1  substation       0.000002       0.000002   \n",
       "4    1_1000_CMCC-CM2-VHR4  W2_2_2  substation       0.000004       0.000003   \n",
       "..                    ...     ...         ...            ...            ...   \n",
       "205     1_5_CMCC-CM2-VHR4  W2_6_2  substation   84270.363554   63202.772666   \n",
       "206     1_5_CMCC-CM2-VHR4  W2_6_3  substation     821.411243     616.058432   \n",
       "207     1_5_CMCC-CM2-VHR4  W2_7_1  substation   29009.357101   21757.017826   \n",
       "208     1_5_CMCC-CM2-VHR4  W2_7_2  substation   59462.163961   44596.622971   \n",
       "209     1_5_CMCC-CM2-VHR4  W2_7_3  substation  169661.197440  127245.898080   \n",
       "\n",
       "          upperdam  \n",
       "0         0.022845  \n",
       "1         0.045689  \n",
       "2         0.114223  \n",
       "3         0.000003  \n",
       "4         0.000005  \n",
       "..             ...  \n",
       "205  105337.954443  \n",
       "206    1026.764053  \n",
       "207   36261.696376  \n",
       "208   74327.704951  \n",
       "209  212076.496801  \n",
       "\n",
       "[210 rows x 6 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract nested dict by key\n",
    "osm_damage_infra[1]['_CMCC-CM2-VHR4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e56c1d61-40b5-4892-ac70-c20c522eb661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def country_analysis_osm(country_code,hazard_type): #\n",
    "    \n",
    "    # extract infrastructure data from OSM\n",
    "    osm_power_infra = extract_osm_infrastructure(country_code,osm_data_path)\n",
    "    \n",
    "    # assess damage to hazard_type\n",
    "    osm_damage_infra = assess_damage_osm(country_code,osm_power_infra,hazard_type)\n",
    "    \n",
    "    line_risk = {}\n",
    "    plant_risk = {}\n",
    "    substation_risk = {}\n",
    "    tower_risk = {}\n",
    "    pole_risk = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "\n",
    "        for i in range(len(osm_damage_infra)):\n",
    "            for climate_model in climate_models:\n",
    "                df = osm_damage_infra[i][climate_model]\n",
    "                    \n",
    "                if len(df) == 0:\n",
    "                    print(\"No {}_{} risk of infra_type {} in {}\".format(hazard_type,climate_model,i,country_code))\n",
    "\n",
    "                else:\n",
    "                    with pd.ExcelWriter(os.path.join(output_path,'damage','{}_osm_{}{}_damage_{}'.format(country_code,hazard_type,climate_model,i)+'.xlsx')) as writer:\n",
    "                        df.to_excel(writer)\n",
    "\n",
    "                    df['rp'] = df['rp'].replace(['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),\n",
    "                                                '1_10{}'.format(climate_model),'1_25{}'.format(climate_model),'1_50{}'.format(climate_model),\n",
    "                                                '1_100{}'.format(climate_model),'1_250{}'.format(climate_model),'1_500{}'.format(climate_model),\n",
    "                                                '1_1000{}'.format(climate_model)],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "                    \n",
    "                    curve_code_substation = ['W2_1_1','W2_1_2','W2_1_3','W2_2_1','W2_2_2','W2_2_3','W2_3_1','W2_3_2','W2_3_3',\n",
    "                                            'W2_4_1','W2_4_2','W2_4_3','W2_5_1','W2_5_2','W2_5_3','W2_6_1','W2_6_2','W2_6_3',\n",
    "                                            'W2_7_1','W2_7_2','W2_7_3']\n",
    "                    \n",
    "                    curve_code_tower = ['W3_1','W3_2','W3_3','W3_4','W3_5','W3_6','W3_7','W3_8','W3_9','W3_10','W3_11','W3_12',\n",
    "                                        'W3_13','W3_14','W3_15','W3_16','W3_17','W3_18','W3_19','W3_20','W3_21','W3_22','W3_22',\n",
    "                                        'W3_24','W3_25','W3_26','W3_27','W3_28','W3_29','W3_30']\n",
    "                    \n",
    "                    curve_code_pole = ['W4_1','W4_2','W4_3','W4_4','W4_5','W4_6','W4_7','W4_8','W4_9','W4_10','W4_11','W4_12',\n",
    "                                    'W4_13','W4_14','W4_15','W4_16','W4_17','W4_18','W4_19','W4_20','W4_21','W4_22','W4_23',\n",
    "                                    'W4_24','W4_25','W4_26','W4_27','W4_28','W4_29','W4_30','W4_31','W4_32','W4_33','W4_34',\n",
    "                                    'W4_35','W4_36','W4_37','W4_38','W4_39','W4_40','W4_41','W4_42','W4_43','W4_44','W4_45',\n",
    "                                    'W4_46','W4_47','W4_48','W4_49','W4_50','W4_51','W4_52','W4_53','W4_54','W4_55']\n",
    "                    \n",
    "                    curve_code_line = ['W5_1','W5_2','W5_3']\n",
    "\n",
    "                    #assess risk for power lines\n",
    "                    if i == 0:\n",
    "                        for curve_code in curve_code_line:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                            loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                            loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                            RPS = loss_list.rp.values.tolist()\n",
    "                            line_risk[climate_model] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                            }\n",
    "                            \n",
    "                    #assess risk for power substations                \n",
    "                    elif i == 1:                        \n",
    "                        for curve_code in curve_code_substation:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                            loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                            loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                            RPS = loss_list.rp.values.tolist()\n",
    "                            substation_risk[climate_model] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                            }\n",
    "\n",
    "                    #assess risk for power towers and power poles\n",
    "                    elif i == 2:\n",
    "                        for curve_code in curve_code_tower:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power towers ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = df.loc[df['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                tower_risk[climate_model] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "                        \n",
    "                        for curve_code in curve_code_pole:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power poles ...\")\n",
    "                            \n",
    "                            else:                    \n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = df.loc[df['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                pole_risk[climate_model] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "    \n",
    "    elif hazard_type=='fl':\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "    \n",
    "        for i in range(len(osm_damage_infra)):\n",
    "            for climate_model in climate_models:\n",
    "                df = osm_damage_infra[i][climate_model]\n",
    "                    \n",
    "                if len(df) == 0:\n",
    "                    print(\"No {}_{} risk of infra_type {} in {}\".format(hazard_type,climate_model,i,country_code))\n",
    "\n",
    "                else:\n",
    "                    with pd.ExcelWriter(os.path.join(output_path,'damage','{}_osm_{}_{}_damage_{}'.format(country_code,hazard_type,climate_model,i)+'.xlsx')) as writer:\n",
    "                        df.to_excel(writer)\n",
    "\n",
    "                    df['rp'] = df['rp'].replace(['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "\n",
    "                    #assess risk for power lines\n",
    "                    if i == 0:\n",
    "                        loss_list_mean = df.meandam.values.tolist()\n",
    "                        loss_list_lower = df.lowerdam.values.tolist()\n",
    "                        loss_list_upper = df.upperdam.values.tolist()\n",
    "                        RPS = df.rp.values.tolist()\n",
    "                        line_risk[climate_model] = {\n",
    "                            'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                            'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                            'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                        }\n",
    "\n",
    "                    #assess risk for power plants and substations                \n",
    "                    elif i == 1:\n",
    "                        loss_list = df.loc[df['asset_type'] == 'plant']\n",
    "                        if len(loss_list) == 0:\n",
    "                            print(\"No risk of plants ...\")\n",
    "                        \n",
    "                        else:\n",
    "                            loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                            loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                            loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                            RPS = df.loc[df['asset_type'] == 'plant']\n",
    "                            RPS = RPS.rp.values.tolist()\n",
    "                            plant_risk[climate_model] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                            }\n",
    "                            \n",
    "                        loss_list = df.loc[df['asset_type'] == 'substation']\n",
    "                        if len(loss_list) == 0:\n",
    "                            print(\"No risk of substations ...\")\n",
    "                        \n",
    "                        else:\n",
    "                            loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                            loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                            loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                            RPS = df.loc[df['asset_type'] == 'substation']\n",
    "                            RPS = RPS.rp.values.tolist()\n",
    "                            substation_risk[climate_model] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "\n",
    "                    #assess risk for power towers and power poles\n",
    "                    elif i == 2:\n",
    "                        loss_list = df.loc[df['asset_type'] == 'power_tower']\n",
    "                        if len(loss_list) == 0:\n",
    "                            print(\"No risk of power towers ...\")\n",
    "                        \n",
    "                        else:\n",
    "                            loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                            loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                            loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                            RPS = df.loc[df['asset_type'] == 'power_tower']\n",
    "                            RPS = RPS.rp.values.tolist()\n",
    "                            tower_risk[climate_model] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                            }\n",
    "                            \n",
    "                        loss_list = df.loc[df['asset_type'] == 'power_pole']\n",
    "                        if len(loss_list) == 0:\n",
    "                            print(\"No risk of power poles ...\")\n",
    "                        \n",
    "                        else:                    \n",
    "                            loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                            loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                            loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                            RPS = df.loc[df['asset_type'] == 'power_pole']\n",
    "                            RPS = RPS.rp.values.tolist()\n",
    "                            pole_risk[climate_model] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                            }\n",
    "                            \n",
    "    return pd.DataFrame(line_risk),pd.DataFrame(plant_risk),pd.DataFrame(substation_risk),pd.DataFrame(tower_risk),pd.DataFrame(pole_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b08ddf81-6571-4e42-bbdd-b6b6e948186a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87/87 [00:00<00:00, 450.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:00<00:00, 84.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8774/8774 [00:00<00:00, 10444.73it/s]\n",
      "polygon damage calculation for BRN tc (): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.51it/s]\n",
      "polygon damage calculation for BRN tc (_CMCC-CM2-VHR4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.57it/s]\n",
      "polygon damage calculation for BRN tc (_CNRM-CM6-1-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.58it/s]\n",
      "polygon damage calculation for BRN tc (_EC-Earth3P-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.54it/s]\n",
      "polygon damage calculation for BRN tc (_HadGEM3-GC31-HM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tc_ risk of infra_type 0 in BRN\n",
      "No tc__CMCC-CM2-VHR4 risk of infra_type 0 in BRN\n",
      "No tc__CNRM-CM6-1-HR risk of infra_type 0 in BRN\n",
      "No tc__EC-Earth3P-HR risk of infra_type 0 in BRN\n",
      "No tc__HadGEM3-GC31-HM risk of infra_type 0 in BRN\n",
      "No tc_ risk of infra_type 2 in BRN\n",
      "No tc__CMCC-CM2-VHR4 risk of infra_type 2 in BRN\n",
      "No tc__CNRM-CM6-1-HR risk of infra_type 2 in BRN\n",
      "No tc__EC-Earth3P-HR risk of infra_type 2 in BRN\n",
      "No tc__HadGEM3-GC31-HM risk of infra_type 2 in BRN\n",
      "CPU times: total: 12.1 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "osm_damage_infra = country_analysis_osm('BRN','tc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "815b5c0d-739f-4a7a-bd5e-71959ce9dd05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>_CMCC-CM2-VHR4</th>\n",
       "      <th>_CNRM-CM6-1-HR</th>\n",
       "      <th>_EC-Earth3P-HR</th>\n",
       "      <th>_HadGEM3-GC31-HM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_risk</th>\n",
       "      <td>170556.628078</td>\n",
       "      <td>169519.571782</td>\n",
       "      <td>173071.127219</td>\n",
       "      <td>168483.870815</td>\n",
       "      <td>169359.116259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower_risk</th>\n",
       "      <td>127917.471058</td>\n",
       "      <td>127139.678837</td>\n",
       "      <td>129803.345414</td>\n",
       "      <td>126362.903111</td>\n",
       "      <td>127019.337194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_risk</th>\n",
       "      <td>213195.785097</td>\n",
       "      <td>211899.464728</td>\n",
       "      <td>216338.909023</td>\n",
       "      <td>210604.838519</td>\n",
       "      <td>211698.895324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _CMCC-CM2-VHR4  _CNRM-CM6-1-HR  _EC-Earth3P-HR  \\\n",
       "mean_risk   170556.628078   169519.571782   173071.127219   168483.870815   \n",
       "lower_risk  127917.471058   127139.678837   129803.345414   126362.903111   \n",
       "upper_risk  213195.785097   211899.464728   216338.909023   210604.838519   \n",
       "\n",
       "            _HadGEM3-GC31-HM  \n",
       "mean_risk      169359.116259  \n",
       "lower_risk     127019.337194  \n",
       "upper_risk     211698.895324  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_damage_infra[2] #'','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6525b38-e84d-4dd4-94f4-1d676dc36183",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'historical': {'mean': 220072615.93097857,\n",
       "   'lower': 165054461.94823387,\n",
       "   'upper': 275090769.91372323},\n",
       "  'rcp8p5': {'mean': 289554076.10272974,\n",
       "   'lower': 217165557.07704726,\n",
       "   'upper': 361942595.1284121}},\n",
       " {},\n",
       " {},\n",
       " {'historical': 132315.8115881768, 'rcp8p5': 179535.9734914216},\n",
       " {'historical': 0.0, 'rcp8p5': 0.48358291963805916})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_damage_infra#[0]['historical']['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ba717-0015-4527-bf1a-a9556f87a541",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Government data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef53a27c-f539-43db-9870-1ee933ddf7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_pg_infrastructure(country_code):\n",
    "    files = [x for x in os.listdir(pg_data_path)  if country_code in x ]\n",
    "    pg_types = ['line','point']\n",
    "    \n",
    "    for pg_type in pg_types:\n",
    "        #print(os.path.isfile(os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))))\n",
    "        if os.path.isfile(os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))):\n",
    "            if pg_type=='line':\n",
    "                for file in files: \n",
    "                    file_path = os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))\n",
    "\n",
    "                    pg_data_country = gpd.read_file(file_path)\n",
    "                    pg_data_country = pd.DataFrame(pg_data_country.copy())\n",
    "                    pg_data_country.geometry = pygeos.from_shapely(pg_data_country.geometry)\n",
    "                    pg_data_country['geometry'] = reproject(pg_data_country)\n",
    "\n",
    "                pg_lines = buffer_assets(pg_data_country.loc[pg_data_country.asset.isin(['line'])],buffer_size=100).reset_index(drop=True)\n",
    "\n",
    "            elif pg_type=='point':\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))\n",
    "\n",
    "                    pg_data_country = gpd.read_file(file_path)\n",
    "                    pg_data_country = pd.DataFrame(pg_data_country.copy())\n",
    "                    pg_data_country.geometry = pygeos.from_shapely(pg_data_country.geometry)\n",
    "                    pg_data_country['geometry'] = reproject(pg_data_country)\n",
    "\n",
    "                pg_points = buffer_assets(pg_data_country.loc[pg_data_country.asset.isin(['plant','substation','power_tower','power_pole'])],buffer_size=100).reset_index(drop=True)\n",
    "\n",
    "    return pg_lines,pg_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6a7cdb0-f9a6-4d0c-9dd9-c9d88d7e98e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "pg_infra = extract_pg_infrastructure('JPN')\n",
    "print(type(pg_infra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c78d9c1-f773-42ea-ad91-0acb4aad54bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>asset</th>\n",
       "      <th>geometry</th>\n",
       "      <th>buffered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (15911096.018 5305017.559)</td>\n",
       "      <td>POLYGON ((15911196.018 5305017.559, 15911194.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (15830095.626 5281047.141)</td>\n",
       "      <td>POLYGON ((15830195.626 5281047.141, 15830193.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (15784471.912 5372262.28)</td>\n",
       "      <td>POLYGON ((15784571.912 5372262.28, 15784569.99...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (15725429.458 5296354.502)</td>\n",
       "      <td>POLYGON ((15725529.458 5296354.502, 15725527.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (15678341.881 5273735.047)</td>\n",
       "      <td>POLYGON ((15678441.881 5273735.047, 15678439.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (14654065.093 3892423.909)</td>\n",
       "      <td>POLYGON ((14654165.093 3892423.909, 14654163.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (14567941.182 3869470.184)</td>\n",
       "      <td>POLYGON ((14568041.182 3869470.184, 14568039.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (14554522.443 3832084.632)</td>\n",
       "      <td>POLYGON ((14554622.443 3832084.632, 14554620.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (14530856.666 3761834.532)</td>\n",
       "      <td>POLYGON ((14530956.666 3761834.532, 14530954.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (14591119.005 3740566.664)</td>\n",
       "      <td>POLYGON ((14591219.005 3740566.664, 14591217.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id       asset                          geometry  \\\n",
       "0    1  substation  POINT (15911096.018 5305017.559)   \n",
       "1    2  substation  POINT (15830095.626 5281047.141)   \n",
       "2    3  substation   POINT (15784471.912 5372262.28)   \n",
       "3    4  substation  POINT (15725429.458 5296354.502)   \n",
       "4    5  substation  POINT (15678341.881 5273735.047)   \n",
       "..  ..         ...                               ...   \n",
       "63  64  substation  POINT (14654065.093 3892423.909)   \n",
       "64  65  substation  POINT (14567941.182 3869470.184)   \n",
       "65  66  substation  POINT (14554522.443 3832084.632)   \n",
       "66  67  substation  POINT (14530856.666 3761834.532)   \n",
       "67  68  substation  POINT (14591119.005 3740566.664)   \n",
       "\n",
       "                                             buffered  \n",
       "0   POLYGON ((15911196.018 5305017.559, 15911194.0...  \n",
       "1   POLYGON ((15830195.626 5281047.141, 15830193.7...  \n",
       "2   POLYGON ((15784571.912 5372262.28, 15784569.99...  \n",
       "3   POLYGON ((15725529.458 5296354.502, 15725527.5...  \n",
       "4   POLYGON ((15678441.881 5273735.047, 15678439.9...  \n",
       "..                                                ...  \n",
       "63  POLYGON ((14654165.093 3892423.909, 14654163.1...  \n",
       "64  POLYGON ((14568041.182 3869470.184, 14568039.2...  \n",
       "65  POLYGON ((14554622.443 3832084.632, 14554620.5...  \n",
       "66  POLYGON ((14530956.666 3761834.532, 14530954.7...  \n",
       "67  POLYGON ((14591219.005 3740566.664, 14591217.0...  \n",
       "\n",
       "[68 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_infra[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c0797c42-fd24-45f7-a0c0-51783767aa2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_damage_pg(country_code,pg_infra,hazard_type):\n",
    "\n",
    "    # load curves and maxdam\n",
    "    curves,maxdam = load_curves_maxdam(vul_curve_path,hazard_type)\n",
    "    \n",
    "    # read infrastructure data:\n",
    "    pg_lines,pg_points = pg_infra\n",
    "    \n",
    "    #calculate damaged lines/polygons/points in loop by climate_model\n",
    "    damaged_lines = {}\n",
    "    damaged_points = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        # read wind data\n",
    "        climate_models = ['_CMCC-CM2-VHR4','_CNRM-CM6-1-HR'] #'','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM'\n",
    "        df_ds = open_storm_data(country_code)\n",
    "        \n",
    "        # remove assets that will not have any damage\n",
    "        pg_points = pg_points.loc[pg_points.asset != 'plant'].reset_index(drop=True)\n",
    "    \n",
    "    elif hazard_type == 'fl':\n",
    "        # read flood data\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "        df_ds = open_flood_data(country_code)\n",
    "        \n",
    "    for climate_model in climate_models:\n",
    "        if hazard_type=='tc':\n",
    "            return_periods = ['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),'1_10{}'.format(climate_model),\n",
    "                              '1_25{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),\n",
    "                              '1_250{}'.format(climate_model),'1_500{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "        elif hazard_type == 'fl':\n",
    "            return_periods = ['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'] \n",
    "            \n",
    "        # assess damage for lines\n",
    "        overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],pg_lines).T,\n",
    "                                     columns=['asset','hazard_point'])\n",
    "\n",
    "        if len(overlay_lines) == 0:\n",
    "            damaged_lines[climate_model] = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            collect_line_damages = []\n",
    "            for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                              desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_line_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                            df_ds[climate_model],\n",
    "                                                                            pg_lines,\n",
    "                                                                            curves,\n",
    "                                                                            maxdam,\n",
    "                                                                            return_period,\n",
    "                                                                            country_code))\n",
    "\n",
    "            get_asset_type_line = dict(zip(pg_lines.index,pg_lines.asset))\n",
    "            \n",
    "            if hazard_type=='tc':\n",
    "                results = pd.DataFrame([item for sublist in collect_line_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "                \n",
    "            elif hazard_type == 'fl':\n",
    "                results = pd.DataFrame(collect_line_damages,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                #sum damage of line, cable, and minor_line\n",
    "                results['curve'] = results['curve'].replace(['cable', 'minor_line'], 'line')\n",
    "                results['asset_type'] = results['asset_type'].replace(['cable', 'minor_line'], 'line')\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index() \n",
    "\n",
    "        # assess damage for points\n",
    "        overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],pg_points).T,\n",
    "                                      columns=['asset','hazard_point'])\n",
    "\n",
    "        if len(overlay_points) == 0:\n",
    "            damaged_points[climate_model] = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            collect_point_damages = []\n",
    "            for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                              desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_point_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                            df_ds[climate_model],\n",
    "                                                                            pg_points,\n",
    "                                                                            curves,\n",
    "                                                                            maxdam,\n",
    "                                                                            return_period,\n",
    "                                                                            country_code))\n",
    "\n",
    "            get_asset_type_point = dict(zip(pg_points.index,pg_points.asset))\n",
    "            \n",
    "            if hazard_type == 'tc':\n",
    "                results = pd.DataFrame([item for sublist in collect_point_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "            elif hazard_type == 'fl':\n",
    "                results = pd.DataFrame(collect_point_damages ,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "            results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])    \n",
    "\n",
    "            damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "                \n",
    "    return damaged_lines,damaged_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5680499-fc0f-454f-ab9b-4452a2c120ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for JPN tc (): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.78it/s]\n",
      "point damage calculation for JPN tc (): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:08<00:00,  7.87it/s]\n",
      "polyline damage calculation for JPN tc (_CMCC-CM2-VHR4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.75it/s]\n",
      "point damage calculation for JPN tc (_CMCC-CM2-VHR4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:08<00:00,  7.79it/s]\n",
      "polyline damage calculation for JPN tc (_CNRM-CM6-1-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.72it/s]\n",
      "point damage calculation for JPN tc (_CNRM-CM6-1-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:08<00:00,  7.76it/s]\n",
      "polyline damage calculation for JPN tc (_EC-Earth3P-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.77it/s]\n",
      "point damage calculation for JPN tc (_EC-Earth3P-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:08<00:00,  7.71it/s]\n",
      "polyline damage calculation for JPN tc (_HadGEM3-GC31-HM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.77it/s]\n",
      "point damage calculation for JPN tc (_HadGEM3-GC31-HM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:08<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 50s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg_damage_infra_tc = assess_damage_pg('JPN',pg_infra,'tc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64cfcf4a-3381-4a83-9f58-6b254ce75336",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rp</th>\n",
       "      <th>curve</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>meandam</th>\n",
       "      <th>lowerdam</th>\n",
       "      <th>upperdam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1</td>\n",
       "      <td>W2_1_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>2.638182e+05</td>\n",
       "      <td>1.978637e+05</td>\n",
       "      <td>3.297728e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1</td>\n",
       "      <td>W2_1_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>5.276365e+05</td>\n",
       "      <td>3.957274e+05</td>\n",
       "      <td>6.595456e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1</td>\n",
       "      <td>W2_1_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>1.319091e+06</td>\n",
       "      <td>9.893184e+05</td>\n",
       "      <td>1.648864e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_1</td>\n",
       "      <td>W2_2_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>1.826395e+04</td>\n",
       "      <td>1.369796e+04</td>\n",
       "      <td>2.282994e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1</td>\n",
       "      <td>W2_2_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>3.652790e+04</td>\n",
       "      <td>2.739592e+04</td>\n",
       "      <td>4.565987e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1_500</td>\n",
       "      <td>W2_6_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>2.243239e+08</td>\n",
       "      <td>1.682430e+08</td>\n",
       "      <td>2.804049e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>1_500</td>\n",
       "      <td>W2_6_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>5.608099e+08</td>\n",
       "      <td>4.206074e+08</td>\n",
       "      <td>7.010123e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1_500</td>\n",
       "      <td>W2_7_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>4.358600e+07</td>\n",
       "      <td>3.268950e+07</td>\n",
       "      <td>5.448250e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1_500</td>\n",
       "      <td>W2_7_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>8.717200e+07</td>\n",
       "      <td>6.537900e+07</td>\n",
       "      <td>1.089650e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1_500</td>\n",
       "      <td>W2_7_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>2.179300e+08</td>\n",
       "      <td>1.634475e+08</td>\n",
       "      <td>2.724125e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rp   curve  asset_type       meandam      lowerdam      upperdam\n",
       "0      1_1  W2_1_1  substation  2.638182e+05  1.978637e+05  3.297728e+05\n",
       "1      1_1  W2_1_2  substation  5.276365e+05  3.957274e+05  6.595456e+05\n",
       "2      1_1  W2_1_3  substation  1.319091e+06  9.893184e+05  1.648864e+06\n",
       "3      1_1  W2_2_1  substation  1.826395e+04  1.369796e+04  2.282994e+04\n",
       "4      1_1  W2_2_2  substation  3.652790e+04  2.739592e+04  4.565987e+04\n",
       "..     ...     ...         ...           ...           ...           ...\n",
       "205  1_500  W2_6_2  substation  2.243239e+08  1.682430e+08  2.804049e+08\n",
       "206  1_500  W2_6_3  substation  5.608099e+08  4.206074e+08  7.010123e+08\n",
       "207  1_500  W2_7_1  substation  4.358600e+07  3.268950e+07  5.448250e+07\n",
       "208  1_500  W2_7_2  substation  8.717200e+07  6.537900e+07  1.089650e+08\n",
       "209  1_500  W2_7_3  substation  2.179300e+08  1.634475e+08  2.724125e+08\n",
       "\n",
       "[210 rows x 6 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_damage_infra_tc[1]['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f7d5fb3-c1c9-4e1e-a73c-fea26042357f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for JPN fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 25.00it/s]\n",
      "point damage calculation for JPN fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 45.45it/s]\n",
      "polyline damage calculation for JPN fl (rcp8p5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 24.04it/s]\n",
      "point damage calculation for JPN fl (rcp8p5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 50.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8min 7s\n",
      "Wall time: 8min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg_damage_infra_fl = assess_damage_pg('JPN',pg_infra,'fl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc6d1154-a26e-4dfe-9ee0-86f6bb80c10d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'historical':        rp       curve  asset_type        meandam       lowerdam       upperdam\n",
       "  0  rp0001  substation  substation  490588.160384  367941.120288  613235.200480\n",
       "  1  rp0002  substation  substation  517498.328609  388123.746457  646872.910762\n",
       "  2  rp0005  substation  substation  559647.208366  419735.406274  699559.010457\n",
       "  3  rp0010  substation  substation  579966.806294  434975.104721  724958.507868\n",
       "  4  rp0025  substation  substation  608736.040167  456552.030125  760920.050208\n",
       "  5  rp0050  substation  substation  632107.357808  474080.518356  790134.197260\n",
       "  6  rp0100  substation  substation  652340.431603  489255.323702  815425.539504\n",
       "  7  rp0250  substation  substation  682990.145845  512242.609384  853737.682306\n",
       "  8  rp0500  substation  substation  699857.025260  524892.768945  874821.281575\n",
       "  9  rp1000  substation  substation  721527.021566  541145.266174  901908.776957,\n",
       "  'rcp8p5':        rp       curve  asset_type        meandam       lowerdam       upperdam\n",
       "  0  rp0001  substation  substation  563081.188997  422310.891747  703851.486246\n",
       "  1  rp0002  substation  substation  576128.595727  432096.446795  720160.744658\n",
       "  2  rp0005  substation  substation  610238.220801  457678.665601  762797.776001\n",
       "  3  rp0010  substation  substation  636848.474749  477636.356061  796060.593436\n",
       "  4  rp0025  substation  substation  663008.857279  497256.642959  828761.071599\n",
       "  5  rp0050  substation  substation  688989.026262  516741.769697  861236.282828\n",
       "  6  rp0100  substation  substation  707410.163024  530557.622268  884262.703781\n",
       "  7  rp0250  substation  substation  731011.743016  548258.807262  913764.678770\n",
       "  8  rp0500  substation  substation  756650.033516  567487.525137  945812.541895\n",
       "  9  rp1000  substation  substation  773409.866355  580057.399766  966762.332944},\n",
       " {})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_damage_infra_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "98d174ad-3c24-44ea-88cb-0b88ee406e40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def country_analysis_pg(country_code,hazard_type):\n",
    "    \n",
    "    # extract infrastructure data from gov data\n",
    "    pg_power_infra = extract_pg_infrastructure(country_code)\n",
    "    \n",
    "    # assess damage to hazard_type\n",
    "    pg_damage_infra = assess_damage_pg(country_code,pg_power_infra,hazard_type)\n",
    "    \n",
    "    line_risk = {}\n",
    "    plant_risk = {}\n",
    "    substation_risk = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        climate_models = ['_CMCC-CM2-VHR4','_CNRM-CM6-1-HR'] #'','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM'\n",
    "\n",
    "        for i in range(len(pg_damage_infra)):\n",
    "            for climate_model in climate_models:\n",
    "                df = pg_damage_infra[i][climate_model]\n",
    "                    \n",
    "                if len(df) == 0:\n",
    "                    print(\"No {}_{} risk of infra_type {} in {}\".format(hazard_type,climate_model,i,country_code))\n",
    "\n",
    "                else:\n",
    "                    with pd.ExcelWriter(os.path.join(output_path,'damage','{}_pg_{}{}_damage_{}'.format(country_code,hazard_type,climate_model,i)+'.xlsx')) as writer:\n",
    "                        df.to_excel(writer)\n",
    "\n",
    "                    df['rp'] = df['rp'].replace(['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),\n",
    "                                                '1_10{}'.format(climate_model),'1_25{}'.format(climate_model),'1_50{}'.format(climate_model),\n",
    "                                                '1_100{}'.format(climate_model),'1_250{}'.format(climate_model),'1_500{}'.format(climate_model),\n",
    "                                                '1_1000{}'.format(climate_model)],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "                    \n",
    "                    curve_code_substation = ['W2_1_1','W2_1_2','W2_1_3','W2_2_1','W2_2_2','W2_2_3','W2_3_1','W2_3_2','W2_3_3',\n",
    "                                            'W2_4_1','W2_4_2','W2_4_3','W2_5_1','W2_5_2','W2_5_3','W2_6_1','W2_6_2','W2_6_3',\n",
    "                                            'W2_7_1','W2_7_2','W2_7_3']\n",
    "                    \n",
    "                    curve_code_line = ['W5_1','W5_2','W5_3']\n",
    "\n",
    "                    #assess risk for power lines\n",
    "                    if i == 0:\n",
    "                        for curve_code in curve_code_line:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                            loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                            loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                            RPS = loss_list.rp.values.tolist()\n",
    "                            \n",
    "                            line_risk[curve_code] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                            }\n",
    "                            #print(line_risk_curve)\n",
    "                    \n",
    "                    #assess risk for power substations                \n",
    "                    elif i == 1:                        \n",
    "                        for curve_code in curve_code_substation:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                            loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                            loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                            RPS = loss_list.rp.values.tolist()\n",
    "                            \n",
    "                            substation_risk[curve_code] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                            }\n",
    "\n",
    "\n",
    "    elif hazard_type=='fl':\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "    \n",
    "        for i in range(len(pg_damage_infra)):\n",
    "            for climate_model in climate_models:\n",
    "                df = pg_damage_infra[i][climate_model]\n",
    "                    \n",
    "                if len(df) == 0:\n",
    "                    print(\"No {}_{} risk of infra_type {} in {}\".format(hazard_type,climate_model,i,country_code))\n",
    "\n",
    "                else:\n",
    "                    with pd.ExcelWriter(os.path.join(output_path,'damage','{}_pg_{}_{}_damage_{}'.format(country_code,hazard_type,climate_model,i)+'.xlsx')) as writer:\n",
    "                        df.to_excel(writer)\n",
    "\n",
    "                    df['rp'] = df['rp'].replace(['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "\n",
    "                    #assess risk for power lines\n",
    "                    if i == 0:\n",
    "                        loss_list_mean = df.meandam.values.tolist()\n",
    "                        loss_list_lower = df.lowerdam.values.tolist()\n",
    "                        loss_list_upper = df.upperdam.values.tolist()\n",
    "                        RPS = df.rp.values.tolist()\n",
    "                        line_risk[climate_model] = {\n",
    "                            'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                            'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                            'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                        }\n",
    "\n",
    "                    #assess risk for power plants and substations                \n",
    "                    elif i == 1:\n",
    "                        loss_list = df.loc[df['asset_type'] == 'plant']\n",
    "                        if len(loss_list) == 0:\n",
    "                            print(\"No risk of plants ...\")\n",
    "                        \n",
    "                        else:\n",
    "                            loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                            loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                            loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                            RPS = df.loc[df['asset_type'] == 'plant']\n",
    "                            RPS = RPS.rp.values.tolist()\n",
    "                            plant_risk[climate_model] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                            }\n",
    "                            \n",
    "                        loss_list = df.loc[df['asset_type'] == 'substation']\n",
    "                        if len(loss_list) == 0:\n",
    "                            print(\"No risk of substations ...\")\n",
    "                        \n",
    "                        else:\n",
    "                            loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                            loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                            loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                            RPS = df.loc[df['asset_type'] == 'substation']\n",
    "                            RPS = RPS.rp.values.tolist()\n",
    "                            substation_risk[climate_model] = {\n",
    "                                'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "                            \n",
    "    #return pd.DataFrame(line_risk),pd.DataFrame(plant_risk),pd.DataFrame(substation_risk)\n",
    "    return line_risk,plant_risk,substation_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cdfbb5-9d4f-4f27-b722-b0326645f60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "line_risk_dict = {}\n",
    "\n",
    "for climate_model in climate_model_list:\n",
    "    # Create a dataframe for the current climate model\n",
    "    line_risk_curve = create_line_risk_curve(climate_model)\n",
    "    \n",
    "    # Create a dictionary to store the three curves for the current line risk curve\n",
    "    curve_dict = {\n",
    "        'curve_1': line_risk_curve['curve_1'],\n",
    "        'curve_2': line_risk_curve['curve_2'],\n",
    "        'curve_3': line_risk_curve['curve_3']\n",
    "    }\n",
    "    \n",
    "    # Add the curve dictionary to the line risk dictionary using the climate model as the key\n",
    "    line_risk_dict[climate_model] = curve_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b2f45-0e6a-4452-b85a-1cf009ec3d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pg_risk_tc = country_analysis_pg('JPN','tc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c6222f6c-3c12-4b7a-84ba-544e31de677f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_CMCC-CM2-VHR4</th>\n",
       "      <th>_CNRM-CM6-1-HR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W2_7_3\n",
       "mean_risk   1.434933e...</td>\n",
       "      <td>W2_7_3\n",
       "mean_risk   1.544563e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      _CMCC-CM2-VHR4  \\\n",
       "0                    W2_7_3\n",
       "mean_risk   1.434933e...   \n",
       "\n",
       "                                      _CNRM-CM6-1-HR  \n",
       "0                    W2_7_3\n",
       "mean_risk   1.544563e...  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pg_risk_tc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5755396e-ef87-4815-ba01-1f91eb685bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for JPN fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 24.88it/s]\n",
      "point damage calculation for JPN fl (historical): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 41.67it/s]\n",
      "polyline damage calculation for JPN fl (rcp8p5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 23.80it/s]\n",
      "point damage calculation for JPN fl (rcp8p5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 49.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No risk of plants ...\n",
      "No risk of plants ...\n"
     ]
    }
   ],
   "source": [
    "pg_risk_fl = country_analysis_pg('JPN','fl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f076a1e2-6f11-47d7-b19b-595720d8de7b",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dda764da-ee7e-49ea-9572-e263c80ddecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def risk_output(country_code,hazard_type,infra_type):\n",
    "  \n",
    "    if infra_type == 'osm':\n",
    "        line_risk,plant_risk,substation_risk,tower_risk,pole_risk = country_analysis_osm(country_code,hazard_type)\n",
    "        \n",
    "        if hazard_type == 'tc':\n",
    "            climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "\n",
    "            for climate_model in climate_models:\n",
    "\n",
    "                # create a Pandas Excel writer using openpyxl engine\n",
    "                writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}{}_risk'.format(country_code,infra_type,hazard_type,climate_model)+'.xlsx'),\n",
    "                                        engine='openpyxl')\n",
    "                \n",
    "                # write each dataframe to a different sheet\n",
    "                if len(line_risk) != 0:\n",
    "                    line_risk[climate_model].to_excel(writer, sheet_name='line_risk')\n",
    "                if len(substation_risk) != 0:\n",
    "                    substation_risk[climate_model].to_excel(writer, sheet_name='substation_risk')\n",
    "                if len(tower_risk) != 0:\n",
    "                    tower_risk[climate_model].to_excel(writer, sheet_name='tower_risk')\n",
    "                if len(pole_risk) != 0:\n",
    "                    pole_risk[climate_model].to_excel(writer, sheet_name='pole_risk')\n",
    "                \n",
    "                # save the Excel file\n",
    "                writer.save()\n",
    "\n",
    "        elif hazard_type == 'fl':\n",
    "            # create a Pandas Excel writer using openpyxl engine\n",
    "            writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}_risk'.format(country_code,infra_type,hazard_type)+'.xlsx'), engine='openpyxl')\n",
    "            \n",
    "            # write each dataframe to a different sheet\n",
    "            if len(line_risk) != 0:\n",
    "                line_risk.to_excel(writer, sheet_name='line_risk')\n",
    "            if len(plant_risk) != 0:\n",
    "                plant_risk.to_excel(writer, sheet_name='plant_risk')\n",
    "            if len(substation_risk) != 0:\n",
    "                substation_risk.to_excel(writer, sheet_name='substation_risk')\n",
    "            if len(tower_risk) != 0:\n",
    "                tower_risk.to_excel(writer, sheet_name='tower_risk')\n",
    "            if len(pole_risk) != 0:\n",
    "                pole_risk.to_excel(writer, sheet_name='pole_risk')\n",
    "            \n",
    "            # save the Excel file\n",
    "            writer.save()\n",
    "\n",
    "    elif infra_type == 'gov':\n",
    "        line_risk,plant_risk,substation_risk = country_analysis_pg(country_code,hazard_type)\n",
    "        \n",
    "        if hazard_type == 'tc':\n",
    "            climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "\n",
    "            for climate_model in climate_models:\n",
    "\n",
    "                # create a Pandas Excel writer using openpyxl engine\n",
    "                writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}{}_risk'.format(country_code,infra_type,hazard_type,climate_model)+'.xlsx'),\n",
    "                                        engine='openpyxl')\n",
    "                \n",
    "                # write each dataframe to a different sheet\n",
    "                if len(line_risk) != 0:\n",
    "                    line_risk[climate_model].to_excel(writer, sheet_name='line_risk')\n",
    "                if len(substation_risk) != 0:\n",
    "                    substation_risk[climate_model].to_excel(writer, sheet_name='substation_risk')\n",
    "                    \n",
    "                # save the Excel file\n",
    "                writer.save()\n",
    "\n",
    "        elif hazard_type == 'fl':\n",
    "            # create a Pandas Excel writer using openpyxl engine\n",
    "            writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}_risk'.format(country_code,infra_type,hazard_type)+'.xlsx'), engine='openpyxl')\n",
    "            \n",
    "            # write each dataframe to a different sheet\n",
    "            if len(line_risk) != 0:\n",
    "                line_risk.to_excel(writer, sheet_name='line_risk')\n",
    "            if len(plant_risk) != 0:\n",
    "                plant_risk.to_excel(writer, sheet_name='plant_risk')\n",
    "            if len(substation_risk) != 0:\n",
    "                substation_risk.to_excel(writer, sheet_name='substation_risk')\n",
    "\n",
    "            # save the Excel file\n",
    "            writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d978c878-bbbf-47c5-abc2-2df27355e8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for JPN tc (): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.50it/s]\n",
      "point damage calculation for JPN tc (): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:09<00:00,  7.22it/s]\n",
      "polyline damage calculation for JPN tc (_CMCC-CM2-VHR4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.05it/s]\n",
      "point damage calculation for JPN tc (_CMCC-CM2-VHR4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:09<00:00,  7.20it/s]\n",
      "polyline damage calculation for JPN tc (_CNRM-CM6-1-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.49it/s]\n",
      "point damage calculation for JPN tc (_CNRM-CM6-1-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:09<00:00,  7.30it/s]\n",
      "polyline damage calculation for JPN tc (_EC-Earth3P-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.07it/s]\n",
      "point damage calculation for JPN tc (_EC-Earth3P-HR): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:09<00:00,  7.30it/s]\n",
      "polyline damage calculation for JPN tc (_HadGEM3-GC31-HM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  9.42it/s]\n",
      "point damage calculation for JPN tc (_HadGEM3-GC31-HM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:09<00:00,  7.09it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ''",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrisk_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJPN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[62], line 61\u001b[0m, in \u001b[0;36mrisk_output\u001b[1;34m(country_code, hazard_type, infra_type)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# write each dataframe to a different sheet\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line_risk) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 61\u001b[0m     \u001b[43mline_risk\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclimate_model\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_excel(writer, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mline_risk\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(substation_risk) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     63\u001b[0m     substation_risk[climate_model]\u001b[38;5;241m.\u001b[39mto_excel(writer, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubstation_risk\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "risk_output('JPN','tc','gov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "f25448e6-5b5d-434d-8332-8471a4a7e2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rp</th>\n",
       "      <th>curve</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>meandam</th>\n",
       "      <th>lowerdam</th>\n",
       "      <th>upperdam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>1.658665e+09</td>\n",
       "      <td>4.523633e+08</td>\n",
       "      <td>2.261816e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.500</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>1.730781e+09</td>\n",
       "      <td>4.720311e+08</td>\n",
       "      <td>2.360156e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>2.388258e+09</td>\n",
       "      <td>6.513430e+08</td>\n",
       "      <td>3.256715e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.100</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>2.552940e+09</td>\n",
       "      <td>6.962564e+08</td>\n",
       "      <td>3.481282e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.040</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>2.767454e+09</td>\n",
       "      <td>7.547601e+08</td>\n",
       "      <td>3.773801e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.020</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>2.921201e+09</td>\n",
       "      <td>7.966912e+08</td>\n",
       "      <td>3.983456e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.010</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>3.080762e+09</td>\n",
       "      <td>8.402079e+08</td>\n",
       "      <td>4.201039e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.004</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>3.356950e+09</td>\n",
       "      <td>9.155320e+08</td>\n",
       "      <td>4.577660e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.002</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>3.528684e+09</td>\n",
       "      <td>9.623684e+08</td>\n",
       "      <td>4.811842e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>3.698459e+09</td>\n",
       "      <td>1.008671e+09</td>\n",
       "      <td>5.043354e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rp  curve asset_type       meandam      lowerdam      upperdam\n",
       "0   1.000  plant      plant  1.658665e+09  4.523633e+08  2.261816e+09\n",
       "2   0.500  plant      plant  1.730781e+09  4.720311e+08  2.360156e+09\n",
       "4   0.200  plant      plant  2.388258e+09  6.513430e+08  3.256715e+09\n",
       "6   0.100  plant      plant  2.552940e+09  6.962564e+08  3.481282e+09\n",
       "8   0.040  plant      plant  2.767454e+09  7.547601e+08  3.773801e+09\n",
       "10  0.020  plant      plant  2.921201e+09  7.966912e+08  3.983456e+09\n",
       "12  0.010  plant      plant  3.080762e+09  8.402079e+08  4.201039e+09\n",
       "14  0.004  plant      plant  3.356950e+09  9.155320e+08  4.577660e+09\n",
       "16  0.002  plant      plant  3.528684e+09  9.623684e+08  4.811842e+09\n",
       "18  0.001  plant      plant  3.698459e+09  1.008671e+09  5.043354e+09"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_damage_infra[1]['historical'].loc[osm_damage_infra[1]['historical']['asset_type'] == 'plant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "b3065e13-2e48-4dd3-b027-5d7815a9f94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.000\n",
       "1     1.000\n",
       "2     1.000\n",
       "3     0.500\n",
       "4     0.500\n",
       "5     0.500\n",
       "6     0.200\n",
       "7     0.200\n",
       "8     0.200\n",
       "9     0.100\n",
       "10    0.100\n",
       "11    0.100\n",
       "12    0.040\n",
       "13    0.040\n",
       "14    0.040\n",
       "15    0.020\n",
       "16    0.020\n",
       "17    0.020\n",
       "18    0.010\n",
       "19    0.010\n",
       "20    0.010\n",
       "21    0.004\n",
       "22    0.004\n",
       "23    0.004\n",
       "24    0.002\n",
       "25    0.002\n",
       "26    0.002\n",
       "27    0.001\n",
       "28    0.001\n",
       "29    0.001\n",
       "Name: rp, dtype: float64"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_damage_infra[0]['historical'].loc[:,\"rp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769479e-4a85-4931-af47-fb1ff09b73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def clip_gridfinder(country_code):\n",
    "    base_map_path = os.path.join(data_path,'base_map')\n",
    "\n",
    "    cty_boundary_path = os.path.join(base_map_path,'gadm41_{}.gpkg'.format(country_code))\n",
    "    cty_boundary = gpd.read_file(cty_boundary_path)\n",
    "    #mask = pd.DataFrame(mask.copy())\n",
    "    #mask.geometry = pygeos.from_shapely(mask.geometry)\n",
    "    #mask['geometry'] = reproject(mask)\n",
    "\n",
    "    gridfinder_path = r'C:\\Users\\mye500\\OneDrive - Vrije Universiteit Amsterdam\\01_Research-Projects\\01_risk_assessment\\PG_data\\gridfinder\\grid.gpkg'\n",
    "    gridfinder = gpd.read_file(gridfinder_path)\n",
    "    #gridfinder = pd.DataFrame(gridfinder.copy())\n",
    "    #gridfinder.geometry = pygeos.from_shapely(gridfinder.geometry)\n",
    "    #gridfinder['geometry'] = reproject(gridfinder)\n",
    "\n",
    "    clipped = gpd.clip(gridfinder,cty_boundary)\n",
    "\n",
    "    return clipped\n",
    "\n",
    "clip_gridfinder('TWN')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
