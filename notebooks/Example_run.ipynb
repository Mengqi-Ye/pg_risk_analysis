{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5263e9-087d-4863-a4e5-839dc33017bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from osgeo import ogr,gdal\n",
    "import os\n",
    "import xarray\n",
    "import numpy as np\n",
    "import pyproj\n",
    "from pygeos import from_wkb,from_wkt\n",
    "import pygeos\n",
    "from tqdm import tqdm\n",
    "from shapely.wkb import loads\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11a902-f512-4fd3-b035-af38041ee083",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdal.SetConfigOption(\"OSM_CONFIG_FILE\", os.path.join('..',\"osmconf.ini\"))\n",
    "\n",
    "# change paths to make it work on your own machine\n",
    "data_path = os.path.join('C:\\\\','data','pg_risk_analysis')\n",
    "netcdf_path = os.path.join(data_path,'tc_netcdf')\n",
    "osm_data_path = os.path.join('C:\\\\','Data','country_osm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc83d355-8c98-4ae6-97b1-3203a29a4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_b(geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    This function builds an SQL query from the values passed to the retrieve() function.\n",
    "    Arguments:\n",
    "         *geoType* : Type of geometry (osm layer) to search for.\n",
    "         *keyCol* : A list of keys/columns that should be selected from the layer.\n",
    "         ***valConstraint* : A dictionary of constraints for the values. e.g. WHERE 'value'>20 or 'value'='constraint'\n",
    "    Returns:\n",
    "        *string: : a SQL query string.\n",
    "    \"\"\"\n",
    "    query = \"SELECT \" + \"osm_id\"\n",
    "    for a in keyCol: query+= \",\"+ a  \n",
    "    query += \" FROM \" + geoType + \" WHERE \"\n",
    "    # If there are values in the dictionary, add constraint clauses\n",
    "    if valConstraint: \n",
    "        for a in [*valConstraint]:\n",
    "            # For each value of the key, add the constraint\n",
    "            for b in valConstraint[a]: query += a + b\n",
    "        query+= \" AND \"\n",
    "    # Always ensures the first key/col provided is not Null.\n",
    "    query+= \"\"+str(keyCol[0]) +\" IS NOT NULL\" \n",
    "    return query \n",
    "\n",
    "\n",
    "def retrieve(osm_path,geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    Function to extract specified geometry and keys/values from OpenStreetMap\n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.     \n",
    "        *geoType* : Type of Geometry to retrieve. e.g. lines, multipolygons, etc.\n",
    "        *keyCol* : These keys will be returned as columns in the dataframe.\n",
    "        ***valConstraint: A dictionary specifiying the value constraints.  \n",
    "        A key can have multiple values (as a list) for more than one constraint for key/value.  \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with all columns, geometries, and constraints specified.    \n",
    "    \"\"\"\n",
    "    driver=ogr.GetDriverByName('OSM')\n",
    "    data = driver.Open(osm_path)\n",
    "    query = query_b(geoType,keyCol,**valConstraint)\n",
    "    sql_lyr = data.ExecuteSQL(query)\n",
    "    features =[]\n",
    "    # cl = columns \n",
    "    cl = ['osm_id'] \n",
    "    for a in keyCol: cl.append(a)\n",
    "    if data is not None:\n",
    "        print('query is finished, lets start the loop')\n",
    "        for feature in tqdm(sql_lyr,desc='extract'):\n",
    "            #try:\n",
    "            if feature.GetField(keyCol[0]) is not None:\n",
    "                geom1 = (feature.geometry().ExportToWkt())\n",
    "                #print(geom1)\n",
    "                geom = from_wkt(feature.geometry().ExportToWkt()) \n",
    "                if geom is None:\n",
    "                    continue\n",
    "                # field will become a row in the dataframe.\n",
    "                field = []\n",
    "                for i in cl: field.append(feature.GetField(i))\n",
    "                field.append(geom)   \n",
    "                features.append(field)\n",
    "            #except:\n",
    "            #    print(\"WARNING: skipped OSM feature\")   \n",
    "    else:\n",
    "        print(\"ERROR: Nonetype error when requesting SQL. Check required.\")    \n",
    "    cl.append('geometry')                   \n",
    "    if len(features) > 0:\n",
    "        return pd.DataFrame(features,columns=cl)\n",
    "    else:\n",
    "        print(\"WARNING: No features or No Memory. returning empty GeoDataFrame\") \n",
    "        return pd.DataFrame(columns=['osm_id','geometry'])\n",
    "\n",
    "def power_polyline(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract all energy linestrings from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"   \n",
    "    return retrieve(osm_path,'lines',['power', 'voltage']) \n",
    "\n",
    "def power_polygon(osm_path): # check with joel, something was wrong here with extracting substations\n",
    "    \"\"\"\n",
    "    Function to extract energy polygons from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"   \n",
    "    df = retrieve(osm_path,'multipolygons',['other_tags']) \n",
    "    \n",
    "    df = df.loc[(df.other_tags.str.contains('power'))]   #keep rows containing power data         \n",
    "    df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})     \n",
    "    \n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"substation\"', case=False)]  = 'substation' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"plant\"', case=False)] = 'plant' #specify row\n",
    "    \n",
    "    df = df.loc[(df.asset == 'substation') | (df.asset == 'plant')]\n",
    "            \n",
    "    return df.reset_index(drop=True) \n",
    "\n",
    "def power_point(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract energy points from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"   \n",
    "    df = retrieve(osm_path,'points',['other_tags']) \n",
    "    \n",
    "    df = df.loc[(df.other_tags.str.contains('power'))]  #keep rows containing power data       \n",
    "    df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})     \n",
    "    \n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"tower\"', case=False)]  = 'power_tower' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"pole\"', case=False)] = 'power_pole' #specify row\n",
    "    \n",
    "    df = df.loc[(df.asset == 'power_tower') | (df.asset == 'power_pole')]\n",
    "            \n",
    "    return df.reset_index(drop=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe9f2e-0be0-47fd-83ca-67f3a272279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject(df_ds,current_crs=\"epsg:4326\",approximate_crs = \"epsg:3857\"):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        df_ds ([type]): [description]\n",
    "        current_crs (str, optional): [description]. Defaults to \"epsg:3857\".\n",
    "        approximate_crs (str, optional): [description]. Defaults to \"epsg:4326\".\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"    \n",
    "\n",
    "    geometries = df_ds['geometry']\n",
    "    coords = pygeos.get_coordinates(geometries)\n",
    "    transformer=pyproj.Transformer.from_crs(current_crs, approximate_crs,always_xy=True)\n",
    "    new_coords = transformer.transform(coords[:, 0], coords[:, 1])\n",
    "    \n",
    "    return pygeos.set_coordinates(geometries.copy(), np.array(new_coords).T) \n",
    "\n",
    "def load_curves_maxdam(data_path,hazard='wind'): \n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        data_path ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    \n",
    "    if hazard == 'wind':\n",
    "        sheet_name = 'flooding_curves'\n",
    "    elif hazard == 'flood':\n",
    "        sheet_name = 'flooding_curves'\n",
    "    \n",
    "    # load curves and maximum damages as separate inputs\n",
    "    curves = pd.read_excel(data_path,sheet_name=sheet_name,skiprows=8,index_col=[0])\n",
    "    maxdam=pd.read_excel(data_path,sheet_name=sheet_name,index_col=[0]).iloc[:5]\n",
    "    \n",
    "    curves.columns = maxdam.columns\n",
    "\n",
    "    #transpose maxdam so its easier work with the dataframe\n",
    "    maxdam = maxdam.T\n",
    "\n",
    "    #interpolate the curves to fill missing values\n",
    "    curves = curves.interpolate()\n",
    "   \n",
    "    return curves,maxdam\n",
    "\n",
    "def buffer_assets(assets,buffer_size=100):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        assets ([type]): [description]\n",
    "        buffer_size (int, optional): [description]. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"    \n",
    "    assets['buffered'] = pygeos.buffer(assets.geometry.values,buffer_size)\n",
    "    return assets\n",
    "\n",
    "def overlay_hazard_assets(df_ds,assets):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        df_ds ([type]): [description]\n",
    "        assets ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    #overlay \n",
    "    hazard_tree = pygeos.STRtree(df_ds.geometry.values)\n",
    "    if (pygeos.get_type_id(assets.iloc[0].geometry) == 3) | (pygeos.get_type_id(assets.iloc[0].geometry) == 6):\n",
    "        return  hazard_tree.query_bulk(assets.geometry,predicate='intersects')    \n",
    "    else:\n",
    "        return  hazard_tree.query_bulk(assets.buffered,predicate='intersects')\n",
    "    \n",
    "def get_damage_per_asset_per_rp(asset,df_ds,assets,curves,maxdam,return_period,country):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        asset ([type]): [description]\n",
    "        df_ds ([type]): [description]\n",
    "        assets ([type]): [description]\n",
    "        grid_size (int, optional): [description]. Defaults to 90.\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"    \n",
    "\n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = df_ds.iloc[asset[1]['hazard_point'].values].reset_index()\n",
    "    get_hazard_points = get_hazard_points.loc[pygeos.intersects(get_hazard_points.geometry.values,assets.iloc[asset[0]].geometry)]\n",
    "\n",
    "    asset_type = assets.iloc[asset[0]].asset\n",
    "    asset_geom = assets.iloc[asset[0]].geometry\n",
    "\n",
    "    if asset_type in ['plant','substation']:\n",
    "        maxdam_asset = maxdam.loc[asset_type].MaxDam/pygeos.area(asset_geom)\n",
    "    else:\n",
    "        maxdam_asset = maxdam.loc[asset_type].MaxDam\n",
    "\n",
    "\n",
    "    hazard_intensity = curves[asset_type].index.values\n",
    "    fragility_values = curves[asset_type].values\n",
    "    \n",
    "    if len(get_hazard_points) == 0:\n",
    "        return asset[0],0\n",
    "    else:\n",
    "        \n",
    "        if pygeos.get_type_id(asset_geom) == 1:\n",
    "            get_hazard_points['overlay_meters'] = pygeos.length(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            return asset[0],np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,fragility_values))*get_hazard_points.overlay_meters*maxdam_asset)\n",
    "        \n",
    "        elif  pygeos.get_type_id(asset_geom) == 3:\n",
    "            get_hazard_points['overlay_m2'] = pygeos.area(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            return asset[0],get_hazard_points.apply(lambda x: np.interp(x[return_period], hazard_intensity, fragility_values)*maxdam_asset*x.overlay_m2,axis=1).sum()     \n",
    "        \n",
    "        else:\n",
    "            return asset[0],np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,fragility_values))*maxdam_asset)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ff6dd-a3f2-4e2a-becd-e5cffa38edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_hazard_data(climate_model,hazard_type='storm'):\n",
    "    \n",
    "    if hazard_type == 'storm':\n",
    "        with xarray.open_dataset(os.path.join(netcdf_path,'STORM_FIXED_RETURN_PERIODS_WP{}.nc'.format(climate_model))) as ds:\n",
    "\n",
    "            # get the mean values\n",
    "            df_ds = ds['mean'].to_dataframe().unstack(level=2).reset_index()\n",
    "\n",
    "            # create geometry values and drop lat lon columns\n",
    "            df_ds['geometry'] = [pygeos.points(x) for x in list(zip(df_ds['lon'],df_ds['lat']))]\n",
    "            df_ds = df_ds.drop(['lat','lon'],axis=1,level=0)\n",
    "\n",
    "            #rename columns to return periods\n",
    "            return_periods = ['1_{}{}'.format(int(x),climate_model) for x in ds['rp']]\n",
    "            df_ds.columns = ['1_{}{}'.format(int(x),climate_model) for x in list(df_ds.columns.get_level_values(1))[:-1]]+['geometry']     \n",
    "            df_ds['geometry'] = pygeos.buffer(df_ds.geometry,radius=0.1/2,cap_style='square').values\n",
    "            df_ds['geometry'] = reproject(df_ds)\n",
    "            \n",
    "            # drop all nan values to reduce size\n",
    "            df_ds = df_ds.loc[~df_ds['1_10000'].isna()].reset_index(drop=True)\n",
    "            df_ds = df_ds.fillna(0)\n",
    "                        \n",
    "    elif hazard_type == 'flood':\n",
    "        \n",
    "        # THIS STILL NEEDS TO BE TESTED WITH GLOFRIS DATA\n",
    "        with xr.open_dataset(os.path.join(flood_path,'SFINCS',hazard_file), engine=\"rasterio\") as ds:\n",
    "            df_ds = ds.to_dataframe().reset_index()\n",
    "            df_ds['geometry'] = pygeos.points(df_ds.x,y=df_ds.y)\n",
    "            df_ds = df_ds.rename(columns={'band_data': 'hazard_intensity'})\n",
    "            df_ds = df_ds.drop(['band','x', 'y','spatial_ref'], axis=1)\n",
    "            df_ds = df_ds.dropna()\n",
    "            df_ds = df_ds.reset_index(drop=True)\n",
    "            df_ds.geometry= pygeos.buffer(df_ds.geometry,radius=20/2,cap_style='square').values\n",
    "            df_ds['geometry'] = reproject(df_ds)\n",
    "\n",
    "    return df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c984c03-a4fd-43d8-b638-6165609fbc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_data_path = os.path.join('C:\\\\','Data','country_osm')\n",
    "country_code = 'LAO'\n",
    "osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62829800-cdd6-47e6-9a0a-2da02fc17a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all data from OSM for power infrastructure\n",
    "#reproject to epsg:3857 to have coordinate system in meters\n",
    "\n",
    "#lines \n",
    "power_lines = power_polyline(osm_path)\n",
    "power_lines = power_lines.rename(columns={'power':'asset'})\n",
    "power_lines['geometry'] = reproject(power_lines) \n",
    "power_lines = buffer_assets(power_lines.loc[power_lines.asset.isin(\n",
    "    ['cable','minor_cable','line','minor_line'])],buffer_size=100).reset_index(drop=True)\n",
    "\n",
    "#polygons\n",
    "power_poly = power_polygon(osm_path)\n",
    "power_poly = power_poly.rename(columns={'power':'asset'})\n",
    "power_poly['geometry'] = reproject(power_poly)\n",
    "#power_poly = power_poly.loc[power_poly.asset.isin(['plant','substation'])].reset_index(drop=True)\n",
    "\n",
    "#points\n",
    "power_points = power_point(osm_path)\n",
    "power_points = power_points.rename(columns={'power':'asset'})\n",
    "power_points['geometry'] = reproject(power_points)\n",
    "power_points = buffer_assets(power_points.loc[power_points.asset.isin(\n",
    "    ['power_tower','power_pole'])],buffer_size=100).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d37345b-a1b0-486a-9d2c-86400fe78aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# load hazard data\n",
    "climate_model = ''\n",
    "df_ds = open_hazard_data(climate_model,hazard_type='storm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb709e5-71af-4380-b6f5-51784e50f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # load curves and maxdam\n",
    "curves,maxdam = load_curves_maxdam(data_path=os.path.join('..','data','infra_vulnerability_data.xlsx'))\n",
    "curves['line']=1 # remove this when things work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2e26c-3ad8-4e9a-b227-14189ec4383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines\n",
    "overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds,power_lines).T,columns=['asset','hazard_point'])\n",
    "\n",
    "collect_line_damages = []\n",
    "return_periods = ['1_100','1_1000']\n",
    "for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),desc='polyline damage calculation for {}'.format(country_code)):\n",
    "    for return_period in return_periods:\n",
    "        collect_line_damages.append([return_period,get_damage_per_asset_per_rp(asset,df_ds,power_lines,curves,maxdam,return_period,country_code)])\n",
    "\n",
    "collect_line_damages = [(line[0],line[1][0],line[1][1]) for line in collect_line_damages]\n",
    "damaged_lines = power_lines.merge(pd.DataFrame(collect_line_damages,columns=['return_period','index','damage']),left_index=True,right_on='index')\n",
    "damaged_lines = damaged_lines.drop(['buffered'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e41b37-e250-43fb-96f5-7ef7675c3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polygons\n",
    "overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds,power_poly).T,columns=['asset','hazard_point'])\n",
    "\n",
    "collect_poly_damages = []\n",
    "for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),desc='polygon damage calculation for {}'.format(country_code)):\n",
    "    for return_period in return_periods:\n",
    "        collect_poly_damages.append(get_damage_per_asset_per_rp(asset,df_ds,power_poly,curves,maxdam,return_period,country_code))\n",
    "\n",
    "damaged_poly = power_poly.merge(pd.DataFrame(collect_poly_damages,columns=['index','damage']),left_index=True,right_on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802acca9-4447-4bc2-910b-7a396dc45b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# points\n",
    "overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds,power_points).T,columns=['asset','hazard_point'])\n",
    "\n",
    "collect_point_damages = []\n",
    "for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),desc='point damage calculation for {}'.format(country_code)):\n",
    "    for return_period in return_periods:\n",
    "        collect_point_damages.append(get_damage_per_asset_per_rp(asset,df_ds,power_points,curves,maxdam,return_period,country_code))\n",
    "\n",
    "damaged_points = power_points.merge(pd.DataFrame(collect_point_damages,columns=['index','damage']),left_index=True,right_on='index')\n",
    "damaged_points = damaged_points.drop(['buffered'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a05e3c-608f-4d4a-91a4-6cbff54cd2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
