{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5263e9-087d-4863-a4e5-839dc33017bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from osgeo import ogr,gdal\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pyproj\n",
    "from pygeos import from_wkb,from_wkt\n",
    "import pygeos\n",
    "from tqdm import tqdm\n",
    "from shapely.wkb import loads\n",
    "from pathlib import Path\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f11a902-f512-4fd3-b035-af38041ee083",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdal.SetConfigOption(\"OSM_CONFIG_FILE\", os.path.join('..',\"osmconf.ini\"))\n",
    "\n",
    "# change paths to make it work on your own machine\n",
    "data_path = os.path.join('C:\\\\','data','pg_risk_analysis')\n",
    "tc_path = os.path.join(data_path,'tc_netcdf')\n",
    "osm_data_path = os.path.join('C:\\\\','data','country_osm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc83d355-8c98-4ae6-97b1-3203a29a4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_b(geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    This function builds an SQL query from the values passed to the retrieve() function.\n",
    "    Arguments:\n",
    "         *geoType* : Type of geometry (osm layer) to search for.\n",
    "         *keyCol* : A list of keys/columns that should be selected from the layer.\n",
    "         ***valConstraint* : A dictionary of constraints for the values. e.g. WHERE 'value'>20 or 'value'='constraint'\n",
    "    Returns:\n",
    "        *string: : a SQL query string.\n",
    "    \"\"\"\n",
    "    query = \"SELECT \" + \"osm_id\"\n",
    "    for a in keyCol: query+= \",\"+ a  \n",
    "    query += \" FROM \" + geoType + \" WHERE \"\n",
    "    # If there are values in the dictionary, add constraint clauses\n",
    "    if valConstraint: \n",
    "        for a in [*valConstraint]:\n",
    "            # For each value of the key, add the constraint\n",
    "            for b in valConstraint[a]: query += a + b\n",
    "        query+= \" AND \"\n",
    "    # Always ensures the first key/col provided is not Null.\n",
    "    query+= \"\"+str(keyCol[0]) +\" IS NOT NULL\" \n",
    "    return query \n",
    "\n",
    "\n",
    "def retrieve(osm_path,geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    Function to extract specified geometry and keys/values from OpenStreetMap\n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.     \n",
    "        *geoType* : Type of Geometry to retrieve. e.g. lines, multipolygons, etc.\n",
    "        *keyCol* : These keys will be returned as columns in the dataframe.\n",
    "        ***valConstraint: A dictionary specifiying the value constraints.  \n",
    "        A key can have multiple values (as a list) for more than one constraint for key/value.  \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with all columns, geometries, and constraints specified.    \n",
    "    \"\"\"\n",
    "    driver=ogr.GetDriverByName('OSM')\n",
    "    data = driver.Open(osm_path)\n",
    "    query = query_b(geoType,keyCol,**valConstraint)\n",
    "    sql_lyr = data.ExecuteSQL(query)\n",
    "    features =[]\n",
    "    # cl = columns \n",
    "    cl = ['osm_id'] \n",
    "    for a in keyCol: cl.append(a)\n",
    "    if data is not None:\n",
    "        print('query is finished, lets start the loop')\n",
    "        for feature in tqdm(sql_lyr,desc='extract'):\n",
    "            #try:\n",
    "            if feature.GetField(keyCol[0]) is not None:\n",
    "                geom1 = (feature.geometry().ExportToWkt())\n",
    "                #print(geom1)\n",
    "                geom = from_wkt(feature.geometry().ExportToWkt()) \n",
    "                if geom is None:\n",
    "                    continue\n",
    "                # field will become a row in the dataframe.\n",
    "                field = []\n",
    "                for i in cl: field.append(feature.GetField(i))\n",
    "                field.append(geom)   \n",
    "                features.append(field)\n",
    "            #except:\n",
    "            #    print(\"WARNING: skipped OSM feature\")   \n",
    "    else:\n",
    "        print(\"ERROR: Nonetype error when requesting SQL. Check required.\")    \n",
    "    cl.append('geometry')                   \n",
    "    if len(features) > 0:\n",
    "        return pd.DataFrame(features,columns=cl)\n",
    "    else:\n",
    "        print(\"WARNING: No features or No Memory. returning empty GeoDataFrame\") \n",
    "        return pd.DataFrame(columns=['osm_id','geometry'])\n",
    "\n",
    "def power_polyline(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract all energy linestrings from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'lines',['power','voltage'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).rename(columns={'power': 'asset'})\n",
    "    \n",
    "    #print(df) #check infra keys\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def power_polygon(osm_path): # check with joel, something was wrong here with extracting substations\n",
    "    \"\"\"\n",
    "    Function to extract energy polygons from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['other_tags']) \n",
    "    \n",
    "    df = df.loc[(df.other_tags.str.contains('power'))]   #keep rows containing power data         \n",
    "    df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})     \n",
    "    \n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"substation\"', case=False)]  = 'substation' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"plant\"', case=False)] = 'plant' #specify row\n",
    "    \n",
    "    df = df.loc[(df.asset == 'substation') | (df.asset == 'plant')]\n",
    "            \n",
    "    return df.reset_index(drop=True) \n",
    "\n",
    "def electricity(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract building polygons from OpenStreetMap    \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with all unique building polygons.    \n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['power'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).rename(columns={'power': 'asset'})\n",
    "    \n",
    "    #df = df[df.asset!='generator']\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"substation\"', case=False)]  = 'substation' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"plant\"', case=False)] = 'plant' #specify row\n",
    "    \n",
    "    #print(df)  #check infra keys\n",
    "    \n",
    "    df = df.loc[(df.asset == 'substation') | (df.asset == 'plant')]\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def retrieve_poly_subs(osm_path, w_list, b_list):\n",
    "    \"\"\"\n",
    "    Function to extract electricity substation polygons from OpenStreetMap\n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region\n",
    "        for which we want to do the analysis.\n",
    "        *w_list* :  white list of keywords to search in the other_tags columns\n",
    "        *b_list* :  black list of keywords of rows that should not be selected\n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique substation.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['other_tags'])\n",
    "    df = df[df.other_tags.str.contains('substation', case=False, na=False)]\n",
    "    #df = df.loc[(df.other_tags.str.contains('substation'))]\n",
    "    df = df[~df.other_tags.str.contains('|'.join(b_list))]\n",
    "    #df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})\n",
    "    df['asset']  = 'substation' #specify row\n",
    "    #df = df.loc[(df.asset == 'substation')] #specify row\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def power_point(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract energy points from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"   \n",
    "    df = retrieve(osm_path,'points',['other_tags']) \n",
    "    df = df.loc[(df.other_tags.str.contains('power'))]  #keep rows containing power data       \n",
    "    df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})     \n",
    "    \n",
    "    #print(df)\n",
    "    \n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"tower\"', case=False)]  = 'power_tower' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"pole\"', case=False)] = 'power_pole' #specify row\n",
    "    #df['asset'].loc[df['asset'].str.contains('\"utility\"=>\"power\"', case=False)] = 'power_tower' #specify row\n",
    "    \n",
    "    df = df.loc[(df.asset == 'power_tower') | (df.asset == 'power_pole')]\n",
    "            \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c41e72d-719f-4592-86b7-4d99884f3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#electricity(osm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36fe9f2e-0be0-47fd-83ca-67f3a272279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject(df_ds,current_crs=\"epsg:4326\",approximate_crs = \"epsg:3857\"):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        df_ds ([type]): [description]\n",
    "        current_crs (str, optional): [description]. Defaults to \"epsg:3857\".\n",
    "        approximate_crs (str, optional): [description]. Defaults to \"epsg:4326\".\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"    \n",
    "\n",
    "    geometries = df_ds['geometry']\n",
    "    coords = pygeos.get_coordinates(geometries)\n",
    "    transformer=pyproj.Transformer.from_crs(current_crs, approximate_crs,always_xy=True)\n",
    "    new_coords = transformer.transform(coords[:, 0], coords[:, 1])\n",
    "    \n",
    "    return pygeos.set_coordinates(geometries.copy(), np.array(new_coords).T) \n",
    "\n",
    "def load_curves_maxdam(data_path,hazard='wind'): \n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        data_path ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    \n",
    "    if hazard == 'wind':\n",
    "        sheet_name = 'flooding_curves'\n",
    "    elif hazard == 'flood':\n",
    "        sheet_name = 'flooding_curves'\n",
    "    \n",
    "    # load curves and maximum damages as separate inputs\n",
    "    curves = pd.read_excel(data_path,sheet_name=sheet_name,skiprows=8,index_col=[0])\n",
    "    maxdam=pd.read_excel(data_path,sheet_name=sheet_name,index_col=[0]).iloc[:5]\n",
    "    \n",
    "    curves.columns = maxdam.columns\n",
    "\n",
    "    #transpose maxdam so its easier work with the dataframe\n",
    "    maxdam = maxdam.T\n",
    "\n",
    "    #interpolate the curves to fill missing values\n",
    "    curves = curves.interpolate()\n",
    "   \n",
    "    return curves,maxdam\n",
    "\n",
    "def buffer_assets(assets,buffer_size=100):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        assets ([type]): [description]\n",
    "        buffer_size (int, optional): [description]. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"    \n",
    "    assets['buffered'] = pygeos.buffer(assets.geometry.values,buffer_size)\n",
    "    return assets\n",
    "\n",
    "def overlay_hazard_assets(df_ds,assets):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        df_ds ([type]): [description]\n",
    "        assets ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    #overlay \n",
    "    hazard_tree = pygeos.STRtree(df_ds.geometry.values)\n",
    "    if (pygeos.get_type_id(assets.iloc[0].geometry) == 3) | (pygeos.get_type_id(assets.iloc[0].geometry) == 6):\n",
    "        return  hazard_tree.query_bulk(assets.geometry,predicate='intersects')    \n",
    "    else:\n",
    "        return  hazard_tree.query_bulk(assets.buffered,predicate='intersects')\n",
    "    \n",
    "def get_damage_per_asset_per_rp(asset,df_ds,assets,curves,maxdam,return_period,country):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        asset ([type]): [description]\n",
    "        df_ds ([type]): [description]\n",
    "        assets ([type]): [description]\n",
    "        grid_size (int, optional): [description]. Defaults to 90.\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"    \n",
    "\n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = df_ds.iloc[asset[1]['hazard_point'].values].reset_index()\n",
    "    get_hazard_points = get_hazard_points.loc[pygeos.intersects(get_hazard_points.geometry.values,assets.iloc[asset[0]].geometry)]\n",
    "\n",
    "    asset_type = assets.iloc[asset[0]].asset\n",
    "    asset_geom = assets.iloc[asset[0]].geometry\n",
    "\n",
    "    if asset_type in ['plant','substation','generator']:\n",
    "        maxdam_asset = maxdam.loc[asset_type].MaxDam/pygeos.area(asset_geom)\n",
    "    else:\n",
    "        maxdam_asset = maxdam.loc[asset_type].MaxDam\n",
    "\n",
    "\n",
    "    hazard_intensity = curves[asset_type].index.values\n",
    "    fragility_values = curves[asset_type].values\n",
    "    \n",
    "    if len(get_hazard_points) == 0:\n",
    "        return asset[0],0\n",
    "    else:\n",
    "        \n",
    "        if pygeos.get_type_id(asset_geom) == 1:\n",
    "            get_hazard_points['overlay_meters'] = pygeos.length(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            return asset[0],np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,fragility_values))*get_hazard_points.overlay_meters*maxdam_asset)\n",
    "        \n",
    "        elif  pygeos.get_type_id(asset_geom) == 3:\n",
    "            get_hazard_points['overlay_m2'] = pygeos.area(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            return asset[0],get_hazard_points.apply(lambda x: np.interp(x[return_period], hazard_intensity, fragility_values)*maxdam_asset*x.overlay_m2,axis=1).sum()     \n",
    "        \n",
    "        else:\n",
    "            return asset[0],np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,fragility_values))*maxdam_asset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c7ff6dd-a3f2-4e2a-becd-e5cffa38edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_hazard_data(climate_model,hazard_type='storm'):\n",
    "    \n",
    "    if hazard_type == 'storm':\n",
    "        with xr.open_dataset(os.path.join(tc_path,'STORM_FIXED_RETURN_PERIODS{}_WP.nc'.format(climate_model))) as ds:\n",
    "            \"\"\"\n",
    "            TC climate model:\n",
    "                CMCC-CM2-VHR4\n",
    "                CNRM-CM6-1-HR\n",
    "                EC-Earth3P-HR\n",
    "                HadGEM3-GC31-HM\n",
    "            \"\"\"\n",
    "            print(ds.keys())\n",
    "            \n",
    "            # get the mean values\n",
    "            df_ds = ds['mean'].to_dataframe().unstack(level=2).reset_index()\n",
    "\n",
    "            # create geometry values and drop lat lon columns\n",
    "            df_ds['geometry'] = [pygeos.points(x) for x in list(zip(df_ds['lon'],df_ds['lat']))]\n",
    "            df_ds = df_ds.drop(['lat','lon'],axis=1,level=0)\n",
    "\n",
    "            #rename columns to return periods\n",
    "            return_periods = ['1_{}{}'.format(int(x),climate_model) for x in ds['rp']]\n",
    "            df_ds.columns = ['1_{}{}'.format(int(x),climate_model) for x in list(df_ds.columns.get_level_values(1))[:-1]]+['geometry']     \n",
    "            df_ds['geometry'] = pygeos.buffer(df_ds.geometry,radius=0.1/2,cap_style='square').values\n",
    "            df_ds['geometry'] = reproject(df_ds)\n",
    "            \n",
    "            # drop all non values to reduce size\n",
    "            #if climate_model == '':\n",
    "            #    df_ds = df_ds.loc[~df_ds['1_10000'].isna()].reset_index(drop=True)\n",
    "            \n",
    "            df_ds = df_ds.loc[~df_ds['1_10000{}'.format(climate_model)].isna()].reset_index(drop=True)\n",
    "            df_ds = df_ds.fillna(0)\n",
    "                        \n",
    "    elif hazard_type == 'flood':\n",
    "        \n",
    "        # THIS STILL NEEDS TO BE TESTED WITH GLOFRIS DATA\n",
    "        with xr.open_dataset(os.path.join(fl_path,'HIST/inuncoast_historical_nosub_hist_rp0500_0.nc')) as ds: #, engine=\"rasterio\"\n",
    "            df_ds = ds.to_dataframe().reset_index()\n",
    "            df_ds['geometry'] = pygeos.points(df_ds.x,y=df_ds.y)\n",
    "            df_ds = df_ds.rename(columns={'band_data': 'hazard_intensity'})\n",
    "            df_ds = df_ds.drop(['band','x', 'y','spatial_ref'], axis=1)\n",
    "            df_ds = df_ds.dropna()\n",
    "            df_ds = df_ds.reset_index(drop=True)\n",
    "            df_ds.geometry= pygeos.buffer(df_ds.geometry,radius=20/2,cap_style='square').values\n",
    "            df_ds['geometry'] = reproject(df_ds)\n",
    "        \n",
    "    return df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d67187-d8e2-402e-85fa-206ae50b0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob(os.path.join(tc_path,'*.nc')):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d37345b-a1b0-486a-9d2c-86400fe78aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CMCC-CM2-VHR4\n",
      "KeysView(<xarray.Dataset>\n",
      "Dimensions:  (rp: 28, lon: 801, lat: 550)\n",
      "Coordinates:\n",
      "  * rp       (rp) float64 10.0 20.0 30.0 40.0 50.0 ... 7e+03 8e+03 9e+03 1e+04\n",
      "  * lon      (lon) float64 100.0 100.1 100.2 100.3 ... 179.7 179.8 179.9 180.0\n",
      "  * lat      (lat) float64 5.05 5.15 5.25 5.35 5.45 ... 59.65 59.75 59.85 59.95\n",
      "Data variables:\n",
      "    mean     (lat, lon, rp) float64 ...\n",
      "    stdev    (lat, lon, rp) float64 ...\n",
      "    75_perc  (lat, lon, rp) float64 ...\n",
      "    25_perc  (lat, lon, rp) float64 ...\n",
      "Attributes:\n",
      "    description:  Empirically derived 10-min 10-meter wind speeds for fixed r...)\n",
      "_CNRM-CM6-1-HR\n",
      "KeysView(<xarray.Dataset>\n",
      "Dimensions:  (rp: 28, lon: 801, lat: 550)\n",
      "Coordinates:\n",
      "  * rp       (rp) float64 10.0 20.0 30.0 40.0 50.0 ... 7e+03 8e+03 9e+03 1e+04\n",
      "  * lon      (lon) float64 100.0 100.1 100.2 100.3 ... 179.7 179.8 179.9 180.0\n",
      "  * lat      (lat) float64 5.05 5.15 5.25 5.35 5.45 ... 59.65 59.75 59.85 59.95\n",
      "Data variables:\n",
      "    mean     (lat, lon, rp) float64 ...\n",
      "    stdev    (lat, lon, rp) float64 ...\n",
      "    75_perc  (lat, lon, rp) float64 ...\n",
      "    25_perc  (lat, lon, rp) float64 ...\n",
      "Attributes:\n",
      "    description:  Empirically derived 10-min 10-meter wind speeds for fixed r...)\n",
      "CPU times: total: 41.6 s\n",
      "Wall time: 41.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load hazard data \n",
    "#climate_model = [] #_CMCC-CM2-VHR4\n",
    "climate_model = ''\n",
    "climate_models = ['_CMCC-CM2-VHR4','_CNRM-CM6-1-HR'] #,'_EC-Earth3P-HR','_HadGEM3-GC31-HM'\n",
    "df_ds = {climate_model:[]}\n",
    "for climate_model in climate_models:\n",
    "    print(climate_model)\n",
    "    df_ds_cl = open_hazard_data(climate_model,hazard_type='storm')\n",
    "    df_ds_cl.head()\n",
    "    df_ds[climate_model] = df_ds_cl\n",
    "\n",
    "#df_ds.to_csv(os.path.join(data_path,'retrieve_data/df_ds.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97fc7c1d-20fd-4fc6-9cc1-ea8d54084a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LAO']\n",
      "C:\\data\\country_osm\\LAO.osm.pbf\n",
      "['LAO', 'VNM']\n",
      "C:\\data\\country_osm\\VNM.osm.pbf\n"
     ]
    }
   ],
   "source": [
    "osm_data_path = os.path.join('C:\\\\','data','country_osm')\n",
    "filelist = []\n",
    "country_codes = []\n",
    "for i in os.listdir(osm_data_path):\n",
    "    osm_path = os.path.join(osm_data_path,i)\n",
    "    if os.path.isfile(osm_path):\n",
    "        filelist.append(i)\n",
    "        country_codes.append(os.path.splitext(os.path.splitext(i)[0])[0])\n",
    "        print(country_codes)\n",
    "        osm_path = os.path.join(osm_data_path,i)\n",
    "        print(osm_path)\n",
    "country_codes = tuple(country_codes)\n",
    "#country_codes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46532d04-d3a9-46f0-87e6-1553b0c76c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\data\\country_osm\\LAO.osm.pbf\n",
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|██████████████████████████████████████████████████████████████████████| 412/412 [00:03<00:00, 111.78it/s]\n",
      "C:\\Users\\mye500\\AppData\\Local\\Temp\\ipykernel_12600\\1355221966.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  assets['buffered'] = pygeos.buffer(assets.geometry.values,buffer_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\data\\country_osm\\VNM.osm.pbf\n",
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|█████████████████████████████████████████████████████████████████████| 2252/2252 [00:27<00:00, 80.87it/s]\n",
      "C:\\Users\\mye500\\AppData\\Local\\Temp\\ipykernel_12600\\1355221966.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  assets['buffered'] = pygeos.buffer(assets.geometry.values,buffer_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\data\\country_osm\\LAO.osm.pbf\n",
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|█████████████████████████████████████████████████████████████████████████| 14/14 [00:10<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\data\\country_osm\\VNM.osm.pbf\n",
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|█████████████████████████████████████████████████████████████████████████| 89/89 [00:56<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\data\\country_osm\\LAO.osm.pbf\n",
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|█████████████████████████████████████████████████████████████████| 42729/42729 [00:05<00:00, 8196.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           osm_id             asset          geometry\n",
      "0       792722233  \"power\"=>\"tower\"  POINT (103 17.9)\n",
      "1       792722236  \"power\"=>\"tower\"  POINT (103 17.9)\n",
      "2      1361087849  \"power\"=>\"tower\"  POINT (103 18.6)\n",
      "3      1361087850  \"power\"=>\"tower\"    POINT (103 18)\n",
      "4      1361087851  \"power\"=>\"tower\"  POINT (103 18.5)\n",
      "...           ...               ...               ...\n",
      "12609  8833069835  \"power\"=>\"tower\"  POINT (102 20.9)\n",
      "12610  8833069836  \"power\"=>\"tower\"  POINT (102 20.9)\n",
      "12611  8833069837  \"power\"=>\"tower\"  POINT (102 20.9)\n",
      "12612  8833069838  \"power\"=>\"tower\"  POINT (102 20.9)\n",
      "12613  8833069839  \"power\"=>\"tower\"  POINT (102 20.9)\n",
      "\n",
      "[12614 rows x 3 columns]\n",
      "C:\\data\\country_osm\\VNM.osm.pbf\n",
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|███████████████████████████████████████████████████████████████| 183864/183864 [00:24<00:00, 7472.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           osm_id                                              asset  \\\n",
      "0       444483746                                   \"power\"=>\"tower\"   \n",
      "1       778791125  \"generator:method\"=>\"combustion\",\"generator:ou...   \n",
      "2       993196742                                 \"power\"=>\"station\"   \n",
      "3       997997708                                   \"power\"=>\"tower\"   \n",
      "4      1629558985                             \"power\"=>\"transformer\"   \n",
      "...           ...                                                ...   \n",
      "30187  8702733808                                   \"power\"=>\"tower\"   \n",
      "30188  8702733809                                   \"power\"=>\"tower\"   \n",
      "30189  8702733810                                   \"power\"=>\"tower\"   \n",
      "30190  8702733811                                   \"power\"=>\"tower\"   \n",
      "30191  8702733812                                   \"power\"=>\"tower\"   \n",
      "\n",
      "               geometry  \n",
      "0        POINT (106 21)  \n",
      "1      POINT (106 21.1)  \n",
      "2      POINT (106 21.1)  \n",
      "3        POINT (106 21)  \n",
      "4      POINT (109 12.2)  \n",
      "...                 ...  \n",
      "30187  POINT (104 21.2)  \n",
      "30188  POINT (104 21.2)  \n",
      "30189  POINT (104 21.2)  \n",
      "30190  POINT (104 21.2)  \n",
      "30191  POINT (104 21.2)  \n",
      "\n",
      "[30192 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# extract all data from OSM for power infrastructure\n",
    "# reproject to epsg:3857 to have coordinate system in meters\n",
    "#osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "\n",
    "country_code = ''\n",
    "\n",
    "#lines\n",
    "power_lines = {country_code:[]}\n",
    "for country_code in country_codes:\n",
    "    #for climate_model in climate_models:\n",
    "        osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "        print(osm_path)\n",
    "        power_lines_country = power_polyline(osm_path)\n",
    "        power_lines_country['geometry'] = reproject(power_lines_country)\n",
    "        power_lines_country = buffer_assets(power_lines_country.loc[power_lines_country.asset.isin(\n",
    "            ['cable','minor_cable','line','minor_line'])],buffer_size=100).reset_index(drop=True)\n",
    "        power_lines[country_code] = power_lines_country\n",
    "\n",
    "#polygons\n",
    "power_poly = {country_code:[]}\n",
    "for country_code in country_codes:\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    print(osm_path)\n",
    "    power_poly_country = electricity(osm_path)\n",
    "    power_poly_country['geometry'] = reproject(power_poly_country)\n",
    "    power_poly[country_code] = power_poly_country\n",
    "\n",
    "#points\n",
    "power_points = {country_code:[]}\n",
    "for country_code in country_codes:\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    print(osm_path)\n",
    "    power_points_country = power_point(osm_path)\n",
    "    power_points_country['geometry'] = reproject(power_points_country)\n",
    "    power_points_country = buffer_assets(power_points_country.loc[power_points_country.asset.isin(\n",
    "        ['power_tower','power_pole'])],buffer_size=100).reset_index(drop=True)\n",
    "    power_points[country_code] = power_points_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be6e7d6d-cf7f-4905-92eb-c4af4b1acc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VNM':       osm_id       asset                                           geometry\n",
       " 0   11380488       plant  MULTIPOLYGON (((1.19e+07 1.18e+06, 1.19e+07 1....\n",
       " 1       None       plant  MULTIPOLYGON (((1.18e+07 1.13e+06, 1.18e+07 1....\n",
       " 2       None  substation  MULTIPOLYGON (((1.19e+07 2.37e+06, 1.19e+07 2....\n",
       " 3       None  substation  MULTIPOLYGON (((1.19e+07 2.38e+06, 1.19e+07 2....\n",
       " 4       None       plant  MULTIPOLYGON (((1.2e+07 1.31e+06, 1.2e+07 1.31...\n",
       " ..       ...         ...                                                ...\n",
       " 78      None       plant  MULTIPOLYGON (((1.18e+07 2.04e+06, 1.18e+07 2....\n",
       " 79      None       plant  MULTIPOLYGON (((1.18e+07 2.04e+06, 1.18e+07 2....\n",
       " 80      None       plant  MULTIPOLYGON (((1.17e+07 2.35e+06, 1.17e+07 2....\n",
       " 81      None       plant  MULTIPOLYGON (((1.18e+07 2.46e+06, 1.18e+07 2....\n",
       " 82      None       plant  MULTIPOLYGON (((1.18e+07 1.13e+06, 1.18e+07 1....\n",
       " \n",
       " [83 rows x 3 columns],\n",
       " 'LAO':   osm_id       asset                                           geometry\n",
       " 0   None       plant  MULTIPOLYGON (((1.14e+07 2.13e+06, 1.14e+07 2....\n",
       " 1   None       plant  MULTIPOLYGON (((1.13e+07 2.24e+06, 1.13e+07 2....\n",
       " 2   None  substation  MULTIPOLYGON (((1.18e+07 1.92e+06, 1.18e+07 1....\n",
       " 3   None       plant  MULTIPOLYGON (((1.16e+07 2.06e+06, 1.16e+07 2....\n",
       " 4   None       plant  MULTIPOLYGON (((1.13e+07 2.4e+06, 1.13e+07 2.4...\n",
       " 5   None       plant  MULTIPOLYGON (((1.13e+07 2.42e+06, 1.13e+07 2....\n",
       " 6   None       plant  MULTIPOLYGON (((1.19e+07 1.67e+06, 1.19e+07 1....\n",
       " 7   None       plant  MULTIPOLYGON (((1.17e+07 2e+06, 1.17e+07 2e+06...\n",
       " 8   None       plant  MULTIPOLYGON (((1.14e+07 2.1e+06, 1.14e+07 2.1...\n",
       " 9   None       plant  MULTIPOLYGON (((1.18e+07 1.57e+06, 1.18e+07 1....}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cb709e5-71af-4380-b6f5-51784e50f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load curves and maxdam\n",
    "curves,maxdam = load_curves_maxdam(data_path=os.path.join('..','data','infra_vulnerability_data.xlsx'))\n",
    "curves['line'] = 1 # remove this when things work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de7639-9846-4a03-bd07-c53e9353c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return periods\n",
    "#return_periods = ['1_10{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),'1_1000{}'.format(climate_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8de6ab-88b0-479d-b04e-01845fc6c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate damaged lines in loop by country_code and climate_model\n",
    "damaged_lines = {country_code:[],climate_model:[]}\n",
    "for country_code in country_codes:\n",
    "    for climate_model in climate_models:\n",
    "        return_periods = ['1_10{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "        \n",
    "        overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],power_lines.get(country_code)).T,columns=['asset','hazard_point'])\n",
    "        collect_line_damages = []\n",
    "        for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),desc='polyline damage calculation for {}'.format(country_code)):\n",
    "            for return_period in return_periods:\n",
    "                collect_line_damages.append([return_period,get_damage_per_asset_per_rp(asset,df_ds[climate_model],power_lines.get(country_code),curves,maxdam,return_period,country_code)])\n",
    "        \n",
    "        collect_line_damages = [(line[0],line[1][0],line[1][1]) for line in collect_line_damages]\n",
    "        damaged_lines_country = power_lines.get(country_code).merge(pd.DataFrame(collect_line_damages,columns=['return_period','index','damage']),left_index=True,right_on='index')\n",
    "        damaged_lines_country = damaged_lines_country.drop(['buffered'],axis=1)\n",
    "        damaged_lines[country_code,climate_model] = damaged_lines_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b530c2-8141-46af-bc88-9c56cd83362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "damaged_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b232c05-9697-47da-a243-e2435a429594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polygon damage calculation for LAO: 100%|█████████████████████████████████████████████| 10/10 [00:00<00:00, 129.40it/s]\n",
      "polygon damage calculation for LAO: 100%|█████████████████████████████████████████████| 10/10 [00:00<00:00, 116.63it/s]\n",
      "polygon damage calculation for VNM: 100%|█████████████████████████████████████████████| 83/83 [00:00<00:00, 127.43it/s]\n",
      "polygon damage calculation for VNM: 100%|█████████████████████████████████████████████| 83/83 [00:00<00:00, 128.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# calculate damaged polygons in loop by country_code and climate_model\n",
    "damaged_poly = {country_code:[],climate_model:[]}\n",
    "for country_code in country_codes:\n",
    "    for climate_model in climate_models:\n",
    "        return_periods = ['1_10{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "        \n",
    "        overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],power_poly.get(country_code)).T,columns=['asset','hazard_point'])\n",
    "        collect_poly_damages = []\n",
    "        for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),desc='polygon damage calculation for {}'.format(country_code)):\n",
    "            for return_period in return_periods:\n",
    "                collect_poly_damages.append([return_period,get_damage_per_asset_per_rp(asset,df_ds[climate_model],power_poly.get(country_code),curves,maxdam,return_period,country_code)])\n",
    "\n",
    "        collect_poly_damages = [(line[0],line[1][0],line[1][1]) for line in collect_poly_damages]\n",
    "        damaged_poly_country = power_poly.get(country_code).merge(pd.DataFrame(collect_poly_damages,columns=['return_period','index','damage']),left_index=True,right_on='index')\n",
    "        damaged_poly[country_code,climate_model] = damaged_poly_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b5a9dce-bd12-4412-87a9-64b61110b690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point damage calculation for LAO: 100%|█████████████████████████████████████████| 12451/12451 [01:33<00:00, 133.47it/s]\n",
      "point damage calculation for LAO: 100%|█████████████████████████████████████████| 12451/12451 [01:36<00:00, 128.94it/s]\n",
      "point damage calculation for VNM: 100%|█████████████████████████████████████████| 29574/29574 [03:47<00:00, 130.24it/s]\n",
      "point damage calculation for VNM: 100%|█████████████████████████████████████████| 29574/29574 [03:49<00:00, 128.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# calculate damaged points in loop by country_code and climate_model\n",
    "damaged_points = {country_code:[]}\n",
    "for country_code in country_codes:\n",
    "    for climate_model in climate_models:\n",
    "        return_periods = ['1_10{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "        \n",
    "        overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],power_points.get(country_code)).T,columns=['asset','hazard_point'])\n",
    "        collect_point_damages = []\n",
    "        for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),desc='point damage calculation for {}'.format(country_code)):\n",
    "            for return_period in return_periods:\n",
    "                collect_point_damages.append([return_period,get_damage_per_asset_per_rp(asset,df_ds[climate_model],power_points.get(country_code),curves,maxdam,return_period,country_code)])\n",
    "\n",
    "        collect_point_damages = [(line[0],line[1][0],line[1][1]) for line in collect_point_damages]\n",
    "        damaged_points_country = power_points.get(country_code).merge(pd.DataFrame(collect_point_damages,columns=['return_period','index','damage']),left_index=True,right_on='index')\n",
    "        damaged_points_country = damaged_points_country.drop(['buffered'],axis=1)\n",
    "        damaged_points[country_code,climate_model] = damaged_points_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e9c855c-f67c-40cb-9045-9c51f2d5c02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>osm_id</th>\n",
       "      <th>asset</th>\n",
       "      <th>geometry</th>\n",
       "      <th>return_period</th>\n",
       "      <th>index</th>\n",
       "      <th>damage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>792722233</td>\n",
       "      <td>power_tower</td>\n",
       "      <td>POINT (1.14e+07 2.02e+06)</td>\n",
       "      <td>1_10_CNRM-CM6-1-HR</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>792722233</td>\n",
       "      <td>power_tower</td>\n",
       "      <td>POINT (1.14e+07 2.02e+06)</td>\n",
       "      <td>1_50_CNRM-CM6-1-HR</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>792722233</td>\n",
       "      <td>power_tower</td>\n",
       "      <td>POINT (1.14e+07 2.02e+06)</td>\n",
       "      <td>1_100_CNRM-CM6-1-HR</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>792722233</td>\n",
       "      <td>power_tower</td>\n",
       "      <td>POINT (1.14e+07 2.02e+06)</td>\n",
       "      <td>1_1000_CNRM-CM6-1-HR</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>792722236</td>\n",
       "      <td>power_tower</td>\n",
       "      <td>POINT (1.14e+07 2.02e+06)</td>\n",
       "      <td>1_10_CNRM-CM6-1-HR</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49799</th>\n",
       "      <td>8833069838</td>\n",
       "      <td>power_tower</td>\n",
       "      <td>POINT (1.14e+07 2.38e+06)</td>\n",
       "      <td>1_1000_CNRM-CM6-1-HR</td>\n",
       "      <td>12449</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49800</th>\n",
       "      <td>8833069839</td>\n",
       "      <td>power_tower</td>\n",
       "      <td>POINT (1.14e+07 2.38e+06)</td>\n",
       "      <td>1_10_CNRM-CM6-1-HR</td>\n",
       "      <td>12450</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49801</th>\n",
       "      <td>8833069839</td>\n",
       "      <td>power_tower</td>\n",
       "      <td>POINT (1.14e+07 2.38e+06)</td>\n",
       "      <td>1_50_CNRM-CM6-1-HR</td>\n",
       "      <td>12450</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49802</th>\n",
       "      <td>8833069839</td>\n",
       "      <td>power_tower</td>\n",
       "      <td>POINT (1.14e+07 2.38e+06)</td>\n",
       "      <td>1_100_CNRM-CM6-1-HR</td>\n",
       "      <td>12450</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49803</th>\n",
       "      <td>8833069839</td>\n",
       "      <td>power_tower</td>\n",
       "      <td>POINT (1.14e+07 2.38e+06)</td>\n",
       "      <td>1_1000_CNRM-CM6-1-HR</td>\n",
       "      <td>12450</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49804 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           osm_id        asset                   geometry  \\\n",
       "0       792722233  power_tower  POINT (1.14e+07 2.02e+06)   \n",
       "1       792722233  power_tower  POINT (1.14e+07 2.02e+06)   \n",
       "2       792722233  power_tower  POINT (1.14e+07 2.02e+06)   \n",
       "3       792722233  power_tower  POINT (1.14e+07 2.02e+06)   \n",
       "4       792722236  power_tower  POINT (1.14e+07 2.02e+06)   \n",
       "...           ...          ...                        ...   \n",
       "49799  8833069838  power_tower  POINT (1.14e+07 2.38e+06)   \n",
       "49800  8833069839  power_tower  POINT (1.14e+07 2.38e+06)   \n",
       "49801  8833069839  power_tower  POINT (1.14e+07 2.38e+06)   \n",
       "49802  8833069839  power_tower  POINT (1.14e+07 2.38e+06)   \n",
       "49803  8833069839  power_tower  POINT (1.14e+07 2.38e+06)   \n",
       "\n",
       "              return_period  index  damage  \n",
       "0        1_10_CNRM-CM6-1-HR      0     0.0  \n",
       "1        1_50_CNRM-CM6-1-HR      0     0.0  \n",
       "2       1_100_CNRM-CM6-1-HR      0     0.0  \n",
       "3      1_1000_CNRM-CM6-1-HR      0     0.0  \n",
       "4        1_10_CNRM-CM6-1-HR      1     0.0  \n",
       "...                     ...    ...     ...  \n",
       "49799  1_1000_CNRM-CM6-1-HR  12449     0.0  \n",
       "49800    1_10_CNRM-CM6-1-HR  12450     0.0  \n",
       "49801    1_50_CNRM-CM6-1-HR  12450     0.0  \n",
       "49802   1_100_CNRM-CM6-1-HR  12450     0.0  \n",
       "49803  1_1000_CNRM-CM6-1-HR  12450     0.0  \n",
       "\n",
       "[49804 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damaged_points['LAO','_CNRM-CM6-1-HR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ae72c-6b39-4445-9015-3f5c9b8050a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_data_path = os.path.join('C:\\\\','data','country_osm')\n",
    "country_code = 'LAO'\n",
    "osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a261d6-f964-41cf-a165-9865200f16e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all data from OSM for power infrastructurepower_polyline\n",
    "#reproject to epsg:3857 to have coordinate system in meters\n",
    "#lines\n",
    "power_lines = power_polyline(osm_path)\n",
    "power_lines = power_lines.rename(columns={'power':'asset'})\n",
    "power_lines['geometry'] = reproject(power_lines) \n",
    "power_lines = buffer_assets(power_lines.loc[power_lines.asset.isin(\n",
    "    ['cable','minor_cable','line','minor_line'])],buffer_size=100).reset_index(drop=True)\n",
    "    \n",
    "#polygons\n",
    "power_poly = electricity(osm_path)\n",
    "power_poly['geometry'] = reproject(power_poly)\n",
    "\"\"\"\n",
    "#polygons\n",
    "power_poly = power_polygon(osm_path)\n",
    "power_poly = power_poly.rename(columns={'power':'asset'})\n",
    "power_poly['geometry'] = reproject(power_poly)\n",
    "#power_poly = power_poly.loc[power_poly.asset.isin(['plant','substation'])].reset_index(drop=True)\n",
    "\n",
    "#substations polygons\n",
    "### KEY:VAL IN OTHER_TAGS ###\n",
    "w_list = ['substation'] # add in funtion '|'.join(w_list) if more than one key word\n",
    "b_list = ['minor_distribution', 'converter', 'indoor', 'pipeline', 'gas']\n",
    "power_sub = retrieve_poly_subs(osm_path,w_list,b_list)\n",
    "#power_sub = retrieve_poly_subs.rename(columns={'power':'asset'})\n",
    "power_sub['geometry'] = reproject(power_sub)\n",
    "\"\"\"\n",
    "    \n",
    "#points\n",
    "power_points = power_point(osm_path)\n",
    "#power_points = power_points.rename(columns={'power':'asset'})\n",
    "power_points['geometry'] = reproject(power_points)\n",
    "power_points = buffer_assets(power_points.loc[power_points.asset.isin(\n",
    "    ['power_tower','power_pole'])],buffer_size=100).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a16126-948e-445e-9696-4f1711d69342",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4c666-1e3a-4c52-b77b-9cea8a8475fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#power_points.to_csv(os.path.join(data_path,'retrieve_data/power_points.csv'))\n",
    "#power_lines.to_csv(os.path.join(data_path,'retrieve_data/power_lines.csv'))\n",
    "#power_poly.to_csv(os.path.join(data_path,'retrieve_data/power_poly.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187cda7-970d-4824-9e6a-d70a5071cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines\n",
    "overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds,power_lines).T,columns=['asset','hazard_point'])\n",
    "\n",
    "collect_line_damages = []\n",
    "for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),desc='polyline damage calculation for {}'.format(country_code)):\n",
    "    for return_period in return_periods:\n",
    "        collect_line_damages.append([return_period,get_damage_per_asset_per_rp(asset,df_ds,power_lines,curves,maxdam,return_period,country_code)])\n",
    "\n",
    "collect_line_damages = [(line[0],line[1][0],line[1][1]) for line in collect_line_damages]\n",
    "damaged_lines = power_lines.merge(pd.DataFrame(collect_line_damages,columns=['return_period','index','damage']),left_index=True,right_on='index')\n",
    "damaged_lines = damaged_lines.drop(['buffered'],axis=1) \n",
    "damaged_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ca7b7-bd0b-4249-8edf-90102cad5b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polygons\n",
    "overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds,power_poly).T,columns=['asset','hazard_point'])\n",
    "\n",
    "collect_poly_damages = []\n",
    "for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),desc='polygon damage calculation for {}'.format(country_code)):\n",
    "    for return_period in return_periods:\n",
    "        collect_poly_damages.append([return_period,get_damage_per_asset_per_rp(asset,df_ds,power_poly,curves,maxdam,return_period,country_code)])\n",
    "\n",
    "collect_poly_damages = [(line[0],line[1][0],line[1][1]) for line in collect_poly_damages]\n",
    "damaged_poly = power_poly.merge(pd.DataFrame(collect_poly_damages,columns=['return_period','index','damage']),left_index=True,right_on='index')\n",
    "damaged_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802acca9-4447-4bc2-910b-7a396dc45b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# points\n",
    "overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds,power_points).T,columns=['asset','hazard_point'])\n",
    "\n",
    "collect_point_damages = []\n",
    "#return_periods = ['1_10','1_50']\n",
    "for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),desc='point damage calculation for {}'.format(country_code)):\n",
    "    for return_period in return_periods:\n",
    "        collect_point_damages.append([return_period,get_damage_per_asset_per_rp(asset,df_ds,power_points,curves,maxdam,return_period,country_code)])\n",
    "\n",
    "collect_point_damages = [(line[0],line[1][0],line[1][1]) for line in collect_point_damages]\n",
    "damaged_points = power_points.merge(pd.DataFrame(collect_point_damages,columns=['return_period','index','damage']),left_index=True,right_on='index')\n",
    "damaged_points = damaged_points.drop(['buffered'],axis=1)\n",
    "\n",
    "#damaged_points.to_csv(os.path.join(data_path,'retrieve_data/damaged_points.csv'))\n",
    "damaged_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7765246-dc29-43e2-82fe-907c018e2432",
   "metadata": {},
   "outputs": [],
   "source": [
    "damaged_points\n",
    "#damaged_points['LAO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f54e2-624d-4113-863b-f633bf096f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load collected power grid data\n",
    "pg_data_path = os.path.join(data_path, 'pg_data')\n",
    "pg_data = gpd.read_file(os.path.join(pg_data_path,\"lao_line.gpkg\"))\n",
    "pg_data = pd.DataFrame(pg_data.copy())\n",
    "print(pg_data.head())\n",
    "pg_data.geometry = pygeos.from_shapely(pg_data.geometry)\n",
    "\n",
    "pg_data['geometry'] = reproject(pg_data)\n",
    "pg_data = buffer_assets(pg_data.loc[pg_data.asset.isin(['line'])],buffer_size=100).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0baa84d-4bab-4ffb-b139-0f3bafd7d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines\n",
    "overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds,pg_data).T,columns=['asset','hazard_point'])\n",
    "#print(overlay_lines.asset.unique())\n",
    "\n",
    "collect_line_damages = []\n",
    "for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),desc='polyline damage calculation for {}'.format(country_code)):\n",
    "    for return_period in return_periods:\n",
    "        collect_line_damages.append([return_period,get_damage_per_asset_per_rp(asset,df_ds,pg_data,curves,maxdam,return_period,country_code)])\n",
    "\n",
    "collect_line_damages = [(line[0],line[1][0],line[1][1]) for line in collect_line_damages]\n",
    "damaged_lines = pg_data.merge(pd.DataFrame(collect_line_damages,columns=['return_period','index','damage']),left_index=True,right_on='index')\n",
    "damaged_lines = damaged_lines.drop(['buffered'],axis=1)\n",
    "damaged_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3279037c-3607-443a-aad9-e9bfb6a9684c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
