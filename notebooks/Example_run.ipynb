{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a5263e9-087d-4863-a4e5-839dc33017bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from osgeo import ogr,gdal\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pyproj\n",
    "from pygeos import from_wkb,from_wkt\n",
    "import pygeos\n",
    "from tqdm import tqdm\n",
    "from shapely.wkb import loads\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from shapely.geometry import mapping\n",
    "pd.options.mode.chained_assignment = None\n",
    "from rasterio.mask import mask\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f11a902-f512-4fd3-b035-af38041ee083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdal.SetConfigOption(\"OSM_CONFIG_FILE\", os.path.join('..',\"osmconf.ini\"))\n",
    "\n",
    "# change paths to make it work on your own machine\n",
    "data_path = os.path.join('C:\\\\','Data','pg_risk_analysis')\n",
    "tc_path = os.path.join(data_path,'tc_netcdf')\n",
    "fl_path = os.path.join(data_path,'GLOFRIS')\n",
    "osm_data_path = os.path.join('C:\\\\','Data','country_osm')\n",
    "pg_data_path = os.path.join(data_path,'pg_data')\n",
    "vul_curve_path = os.path.join(data_path,'vulnerability_curves','input_vulnerability_data.xlsx')\n",
    "output_path = os.path.join('C:\\\\','projects','pg_risk_analysis','output')\n",
    "ne_path = os.path.join(data_path,'..',\"natural_earth\",\"ne_10m_admin_0_countries.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2bb37aa-ca3b-430d-8c74-8ee2fd8df315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wind speed (m/s)</th>\n",
       "      <th>Failure probability</th>\n",
       "      <th>Failure probability.1</th>\n",
       "      <th>Failure probability.2</th>\n",
       "      <th>Failure probability.3</th>\n",
       "      <th>Failure probability.4</th>\n",
       "      <th>Failure probability.5</th>\n",
       "      <th>Failure probability.6</th>\n",
       "      <th>Failure probability.7</th>\n",
       "      <th>Failure probability.8</th>\n",
       "      <th>...</th>\n",
       "      <th>Failure probability.99</th>\n",
       "      <th>Failure probability.100</th>\n",
       "      <th>Failure probability.101</th>\n",
       "      <th>Failure probability.102</th>\n",
       "      <th>Failure probability.103</th>\n",
       "      <th>Failure probability.104</th>\n",
       "      <th>Failure probability.105</th>\n",
       "      <th>Failure probability.106</th>\n",
       "      <th>Failure probability.107</th>\n",
       "      <th>Failure probability.108</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.388889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.235200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>290.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>290.113636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>295.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>296.420455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>295 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Wind speed (m/s)  Failure probability  Failure probability.1  \\\n",
       "0            0.000000                  NaN                    NaN   \n",
       "1            1.000000                  NaN                    NaN   \n",
       "2            1.388889                  NaN                    NaN   \n",
       "3            2.000000                  NaN                    NaN   \n",
       "4            2.235200                  NaN                    NaN   \n",
       "..                ...                  ...                    ...   \n",
       "290        290.000000                  NaN                    NaN   \n",
       "291        290.113636                  1.0                    1.0   \n",
       "292        295.000000                  NaN                    NaN   \n",
       "293        296.420455                  1.0                    1.0   \n",
       "294        300.000000                  NaN                    NaN   \n",
       "\n",
       "     Failure probability.2  Failure probability.3  Failure probability.4  \\\n",
       "0                      NaN                    NaN                    NaN   \n",
       "1                      NaN                    NaN                    NaN   \n",
       "2                      NaN                    NaN                    NaN   \n",
       "3                      NaN                    NaN                    NaN   \n",
       "4                      NaN                    NaN                    NaN   \n",
       "..                     ...                    ...                    ...   \n",
       "290                    NaN                    NaN                    NaN   \n",
       "291                    1.0                    1.0                    1.0   \n",
       "292                    NaN                    NaN                    NaN   \n",
       "293                    1.0                    1.0                    1.0   \n",
       "294                    NaN                    NaN                    NaN   \n",
       "\n",
       "     Failure probability.5  Failure probability.6  Failure probability.7  \\\n",
       "0                      NaN                    NaN                    NaN   \n",
       "1                      NaN                    NaN                    NaN   \n",
       "2                      NaN                    NaN                    NaN   \n",
       "3                      NaN                    NaN                    NaN   \n",
       "4                      NaN                    NaN                    NaN   \n",
       "..                     ...                    ...                    ...   \n",
       "290                    NaN                    NaN                    NaN   \n",
       "291                    NaN                    NaN                    NaN   \n",
       "292                    NaN                    NaN                    NaN   \n",
       "293                    NaN                    NaN                    NaN   \n",
       "294                    NaN                    NaN                    NaN   \n",
       "\n",
       "     Failure probability.8  ...  Failure probability.99  \\\n",
       "0                      NaN  ...                     0.0   \n",
       "1                      NaN  ...                     NaN   \n",
       "2                      NaN  ...                     0.0   \n",
       "3                      NaN  ...                     NaN   \n",
       "4                      NaN  ...                     NaN   \n",
       "..                     ...  ...                     ...   \n",
       "290                    NaN  ...                     NaN   \n",
       "291                    NaN  ...                     NaN   \n",
       "292                    NaN  ...                     NaN   \n",
       "293                    NaN  ...                     NaN   \n",
       "294                    NaN  ...                     NaN   \n",
       "\n",
       "     Failure probability.100  Failure probability.101  \\\n",
       "0                        0.0                      0.0   \n",
       "1                        NaN                      NaN   \n",
       "2                        0.0                      0.0   \n",
       "3                        NaN                      NaN   \n",
       "4                        NaN                      NaN   \n",
       "..                       ...                      ...   \n",
       "290                      NaN                      NaN   \n",
       "291                      NaN                      NaN   \n",
       "292                      NaN                      NaN   \n",
       "293                      NaN                      NaN   \n",
       "294                      NaN                      NaN   \n",
       "\n",
       "     Failure probability.102  Failure probability.103  \\\n",
       "0                        0.0                      0.0   \n",
       "1                        NaN                      NaN   \n",
       "2                        0.0                      0.0   \n",
       "3                        NaN                      NaN   \n",
       "4                        NaN                      NaN   \n",
       "..                       ...                      ...   \n",
       "290                      NaN                      NaN   \n",
       "291                      NaN                      NaN   \n",
       "292                      NaN                      NaN   \n",
       "293                      NaN                      NaN   \n",
       "294                      NaN                      NaN   \n",
       "\n",
       "     Failure probability.104  Failure probability.105  \\\n",
       "0                        0.0                      0.0   \n",
       "1                        NaN                      NaN   \n",
       "2                        0.0                      0.0   \n",
       "3                        NaN                      NaN   \n",
       "4                        NaN                      NaN   \n",
       "..                       ...                      ...   \n",
       "290                      NaN                      NaN   \n",
       "291                      NaN                      NaN   \n",
       "292                      NaN                      NaN   \n",
       "293                      NaN                      NaN   \n",
       "294                      NaN                      NaN   \n",
       "\n",
       "     Failure probability.106  Failure probability.107  Failure probability.108  \n",
       "0                        0.0                      0.0                      0.0  \n",
       "1                        NaN                      NaN                      NaN  \n",
       "2                        0.0                      0.0                      0.0  \n",
       "3                        NaN                      NaN                      NaN  \n",
       "4                        NaN                      NaN                      NaN  \n",
       "..                       ...                      ...                      ...  \n",
       "290                      NaN                      NaN                      NaN  \n",
       "291                      NaN                      NaN                      NaN  \n",
       "292                      NaN                      NaN                      NaN  \n",
       "293                      NaN                      NaN                      NaN  \n",
       "294                      NaN                      NaN                      NaN  \n",
       "\n",
       "[295 rows x 110 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(vul_curve_path,sheet_name='wind_curves',skiprows=10)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64444f38-204a-4b48-87ca-4b69b072258e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABxRUlEQVR4nO3dd3gU5d7G8e+m9wRISAKEEHpvQWnSFBFRATkeEJUiWLADiorliB4V9D0H1KOiWMAK2BBUVFB6k470DgmQEEMgCenZnfePJauRlpANs9ncn+vaK7OzszP37gD58cwzz2MxDMNARERExE14mB1ARERExJlU3IiIiIhbUXEjIiIibkXFjYiIiLgVFTciIiLiVlTciIiIiFtRcSMiIiJuRcWNiIiIuBUVNyIiIuJWVNyImOyrr77CYrEwe/bss15r1aoVFouFn3/++azX6tWrR9u2bQFYsmQJFouFJUuWOC3XoUOHsFgszJgx44LbFR37XI9bbrnFaXlKasaMGVgsFg4dOuRYN3z4cOrUqVNsuzp16jB8+HDH82PHjjFhwgQ2b95cLrl+//137rzzTuLi4vDz8yMoKIi2bdvy6quvkpaWVi7HFKmsvMwOIFLZde/eHYvFwuLFixk0aJBjfVpaGlu3biUwMJDFixdz3XXXOV47cuQIBw4cYOzYsQC0bduW1atX07Rp08uev8jLL79Mjx49iq2rVq3aZc9xww03sHr1aqKjoy+43Zw5cwgJCXE8P3bsGM8//zx16tShdevWTs303nvvcf/999OoUSPGjRtH06ZNKSgoYP369bzzzjusXr2aOXPmOPWYIpWZihsRk4WHh9O8efOzWl2WLl2Kl5cXI0eOZPHixcVeK3peVEyEhITQoUOHy5L3fBo0aFDiDFarlcLCQnx9fZ2eIyIigoiIiItu16ZNG6cf+1xWr17Nfffdx7XXXsu3335b7DNfe+21PProo/z0009OOVZOTg5+fn5YLBan7E+kotJlKREX0KNHD3bv3k1SUpJj3ZIlS7jiiivo06cPGzZsIDMzs9hrnp6edOnSxfH875elhg8fTlBQEPv27aNPnz4EBQURExPDo48+Sl5eXrHjHzt2jIEDBxIcHExoaCiDBg0iOTnZKZ+t6PLWq6++yosvvkhcXBy+vr4sXryY3NxcHn30UVq3bk1oaChVq1alY8eOzJ0796z9nDp1ipEjR1K1alWCgoK44YYbOHDgABaLhQkTJji2O9dlqXP562Wpou8a4M4773RcVpswYQKffPIJFouF1atXn7WPF154AW9vb44dO3be47z88stYLBamTZt2zmLOx8eHvn37Op7//fOcK+9fP+eCBQsYMWIEERERBAQEMHv2bCwWC7/++utZ+5g6dSoWi4Xff//dsW79+vX07duXqlWr4ufnR5s2bfjiiy/O+3lEKgIVNyIuoKgF5q/FyeLFi+nWrRudO3fGYrGwfPnyYq+1bduW0NDQC+63oKCAvn37cs011zB37lxGjBjBlClTeOWVVxzb5OTk0LNnTxYsWMDEiRP58ssviYqKKnaJrCRsNhuFhYXFHn/1xhtvsGjRIv7zn//w448/0rhxY/Ly8khLS+Oxxx7j22+/ZebMmVx11VUMGDCAjz/+uNi+b7rpJj7//HOeeOIJ5syZQ/v27endu3epMp5P27ZtmT59OgDPPPMMq1evZvXq1dx1110MGjSIqKgo3nrrrWLvKSws5N133+Xmm2+mRo0a59yv1Wpl0aJFxMfHExMT45SsfzdixAi8vb355JNP+Oqrr7j55pupXr264/P81YwZM2jbti0tW7YE7H+OOnfuzKlTp3jnnXeYO3curVu3ZtCgQRftayXi0gwRMV1aWprh4eFh3HPPPYZhGEZqaqphsViMn376yTAMw7jyyiuNxx57zDAMw0hISDAA4/HHH3e8f/HixQZgLF682LFu2LBhBmB88cUXxY7Vp08fo1GjRo7nU6dONQBj7ty5xba7++67DcCYPn36BbMXHftcj7179xoHDx40AKNevXpGfn7+BfdVWFhoFBQUGCNHjjTatGnjWP/DDz8YgDF16tRi20+cONEAjOeee86xbvr06QZgHDx4sNh3ERsbW+y9sbGxxrBhwxzP161bd97P+9xzzxk+Pj7G8ePHHetmz55tAMbSpUvP+3mSk5MNwLj11lsv+Ln/6u+f53x5iz7n0KFDz9p27Nixhr+/v3Hq1CnHuh07dhiA8b///c+xrnHjxkabNm2MgoKCYu+/8cYbjejoaMNqtZY4t4grUcuNiAuoUqUKrVq1crTcLF26FE9PTzp37gxAt27dHP1s/t7f5kIsFgs33XRTsXUtW7bk8OHDjueLFy8mODi42KURgNtuu61Un+GVV15h3bp1xR5/ba3o27cv3t7eZ73vyy+/pHPnzgQFBeHl5YW3tzcffPABO3fudGyzdOlSAAYOHFjsvYMHDy5Vxkt13333AfaOwUXefPNNWrRoQdeuXS9LhvP5xz/+cda6ESNGkJOTU+wOvOnTp+Pr6+s4r/v27WPXrl3cfvvtAMVa3Pr06UNSUhK7d+++PB9CxMlU3Ii4iB49erBnzx6OHTvG4sWLiY+PJygoCLAXN5s2bSI9PZ3Fixfj5eXFVVddddF9BgQE4OfnV2ydr68vubm5jucnTpwgMjLyrPdGRUWVKn/dunVp165dscdf+5ic6+6lb775hoEDB1KzZk0+/fRTVq9ezbp16xgxYsRZGb28vKhatWqx958rd3mIjIxk0KBBvPvuu1itVn7//XeWL1/Ogw8+eMH3hYeHExAQwMGDB8st27m+12bNmnHFFVc4Lk1ZrVY+/fRT+vXr5/gOjx8/DsBjjz2Gt7d3scf9998PQGpqarnlFilPultKxEX06NGDyZMns2TJEpYsWUKfPn0crxUVMsuWLXN0fi0qfMqqWrVqrF279qz1zupQXORcd/B8+umnxMXFOTrBFvl7h+dq1apRWFhIWlpasQLH2Rkv5JFHHuGTTz5h7ty5/PTTT4SFhTlaPc7H09OTa665hh9//JEjR45Qq1atix7H19f3rM8P9gLvXM53Z9Sdd97J/fffz86dOzlw4ABJSUnceeedjtfDw8MBGD9+PAMGDDjnPho1anTRvCKuSC03Ii6ia9eueHp68tVXX7F9+3a6d+/ueC00NJTWrVvz0UcfcejQoRJdkiqpHj16kJmZybx584qt//zzz512jPOxWCz4+PgU+wWdnJx81t1S3bp1AzhroMNZs2Y5LUtRK1NOTs45X4+Pj6dTp0688sorfPbZZwwfPpzAwMCL7nf8+PEYhsHdd99Nfn7+Wa8XFBTw3XffOZ7XqVOn2N1MAIsWLeL06dOl+TgMHjwYPz8/ZsyYwYwZM6hZsya9evVyvN6oUSMaNGjAli1bzmpxK3oEBweX6pgirkItNyIuIiQkhLZt2/Ltt9/i4eHh6G9TpFu3brz22mtAyfrblNTQoUOZMmUKQ4cO5aWXXqJBgwbMnz//nKMiO9uNN97IN998w/33388tt9xCYmIi//73v4mOjmbv3r2O7Xr37k3nzp159NFHycjIID4+ntWrVzvuqPLwKPv/0+rVq4e/vz+fffYZTZo0ISgoiBo1ahS7E+qRRx5h0KBBWCwWx6Wbi+nYsSNTp07l/vvvJz4+nvvuu49mzZpRUFDApk2bmDZtGs2bN3f0jRoyZAjPPvss//rXv+jWrRs7duzgzTffvOidcX8XFhbGzTffzIwZMzh16hSPPfbYWd/Tu+++y/XXX891113H8OHDqVmzJmlpaezcuZONGzfy5ZdfluqYIq5CLTciLqRHjx4YhkGbNm2KjZ4L9uLGMAx8fHzo1KmT044ZEBDAokWL6NmzJ08++SS33HILR44ccWqryPnceeedTJo0iR9//JE+ffrwyiuv8OSTT57VmdnDw4PvvvuOW2+9lUmTJtGvXz+WL1/Op59+Cth/kZdVQEAAH374ISdOnKBXr15cccUVTJs2rdg2/fv3x9fXl+uuu44GDRqUeN93330369evJz4+nldeeYVevXrRv39/Zs6cyW233VbsOOPGjWPcuHHMmDGDm266ia+//povvvjikj7jnXfeSUpKCvn5+cXGyCnSo0cP1q5dS1hYGKNHj6Znz57cd999/PLLL/Ts2bPUxxNxFRbDMAyzQ4iIXIrPP/+c22+/nZUrVzq14Duf7777jr59+/LDDz8U6xMlIq5FxY2IVAgzZ87k6NGjtGjRAg8PD9asWcP//d//0aZNG8et4uVlx44dHD58mEceeYTAwEA2btyoKQ5EXJj63IhIhRAcHMysWbN48cUXycrKIjo6muHDh/Piiy+W+7Hvv/9+Vq5cSdu2bfnoo49U2Ii4OLXciIiIiFtRh2IRERFxKypuRERExK2ouBERERG3Uuk6FNtsNo4dO0ZwcLA6BYqIiFQQhmGQmZlJjRo1LjpwZ6Urbo4dO1ZspmIRERGpOBITEy86T1ulK26K5kpJTEw8awRYERERcU0ZGRnExMSUaM6zSlfcFF2KCgkJUXEjIiJSwZSkS4k6FIuIiIhbUXEjIiIibkXFjYiIiLiVStfnpqSsVisFBQVmx6h0vL298fT0NDuGiIhUYCpu/sYwDJKTkzl16pTZUSqtsLAwoqKiNA6RiIhcEhU3f1NU2FSvXp2AgAD9gr2MDMMgOzublJQUAKKjo01OJCIiFZGKm7+wWq2OwqZatWpmx6mU/P39AUhJSaF69eq6RCUiIqWmDsV/UdTHJiAgwOQklVvR968+TyIicilU3JyDLkWZS9+/iIiUhYobERERcSsqbkRERMStmF7cvP3228TFxeHn50d8fDzLly+/4PZ5eXk8/fTTxMbG4uvrS7169fjwww8vU1rXNWHCBCwWS7FHVFRUsW0Mw2DChAnUqFEDf39/unfvzvbt28stU05ODgEBAezatavcjiEiIvJ3phY3s2fPZvTo0Tz99NNs2rSJLl26cP3115OQkHDe9wwcOJBff/2VDz74gN27dzNz5kwaN258GVO7rmbNmpGUlOR4bN26tdjrr776KpMnT+bNN99k3bp1REVFce2115KZmVkueRYuXEhMTIzOj4hIJbJqXyrZ+YWmZjC1uJk8eTIjR47krrvuokmTJrz22mvExMQwderUc27/008/sXTpUubPn0/Pnj2pU6cOV155JZ06dbrMyV2Tl5cXUVFRjkdERITjNcMweO2113j66acZMGAAzZs356OPPiI7O5vPP//8vPscPnw4/fv35+WXXyYyMpKwsDCef/55CgsLGTduHFWrVqVWrVrnbD2bO3cuffv2BWDLli306NGD4OBgQkJCiI+PZ/369c7/EkRExDT7Uk5z54x19H5tOSkZuablMK24yc/PZ8OGDfTq1avY+l69erFq1apzvmfevHm0a9eOV199lZo1a9KwYUMee+wxcnJyyi2nYRhk5xea8jAMo1RZ9+7dS40aNYiLi+PWW2/lwIEDjtcOHjxIcnJyse/b19eXbt26nff7LrJo0SKOHTvGsmXLmDx5MhMmTODGG2+kSpUq/Pbbb4waNYpRo0aRmJjoeI/NZuP777+nX79+ANx+++3UqlWLdevWsWHDBp588km8vb1L9flERMR1WW0Gj325hbxCG7HVAogI9jUti2mD+KWmpmK1WomMjCy2PjIykuTk5HO+58CBA6xYsQI/Pz/mzJlDamoq999/P2lpaeftd5OXl0deXp7jeUZGRqly5hRYafqvn0v1HmfZ8cJ1BPiU7BS1b9+ejz/+mIYNG3L8+HFefPFFOnXqxPbt26lWrZrjOz3X93348OEL7rtq1aq88cYbeHh40KhRI1599VWys7N56qmnABg/fjyTJk1i5cqV3HrrrQCsWbMGm83maFVLSEhg3LhxjktUDRo0KPkXISIiLu+95QfYnHiKYF8vXvlHS1OH9TC9Q/HfP7xhGOf9Qmw2GxaLhc8++4wrr7ySPn36MHnyZGbMmHHe1puJEycSGhrqeMTExDj9M7iC66+/nn/84x+0aNGCnj178sMPPwDw0UcfFduuNN93kWbNmuHh8ecflcjISFq0aOF47unpSbVq1RzTJoD9ktSNN97oeN/YsWO566676NmzJ5MmTWL//v2X9kFFRMTl7D2eyeQFewB49qam1AjzNzWPaS034eHheHp6ntVKk5KSclbrQpHo6Ghq1qxJaGioY12TJk0wDIMjR46cszVg/PjxjB071vE8IyOjVAWOv7cnO164rsTbO5O/96VPPRAYGEiLFi3Yu3cvgOPOqeTk5GJzNl3o+y7y98tHFovlnOtsNpvj+bx585g4caLj+YQJE7jtttv44Ycf+PHHH3nuueeYNWsWN99886V9QBERcQmFVhuPfrmFfKuNHo0i+Gd8LbMjmddy4+PjQ3x8PAsXLiy2fuHCheftINy5c2eOHTvG6dOnHev27NmDh4cHtWqd+8v09fUlJCSk2KM0LBYLAT5epjzK0qSXl5fHzp07HYVMXFwcUVFRxb7v/Px8li5d6vQO2Xv37uXQoUNn9adq2LAhY8aMYcGCBQwYMIDp06c79bgiInL5vbN0P78fSSfEz4tJJl+OKmLqZamxY8fy/vvv8+GHH7Jz507GjBlDQkICo0aNAuytLkOHDnVsf9ttt1GtWjXuvPNOduzYwbJlyxg3bhwjRoxwTLhYWT322GMsXbqUgwcP8ttvv3HLLbeQkZHBsGHDAHuRNnr0aF5++WXmzJnDtm3bGD58OAEBAdx2221OzTJ37lx69uzpmCMqJyeHBx98kCVLlnD48GFWrlzJunXraNKkiVOPKyIil9fOpAxe/9V+heD5fs2IDPEzOZGdqbOCDxo0iBMnTvDCCy+QlJRE8+bNmT9/PrGxsQAkJSUVG/MmKCiIhQsX8tBDD9GuXTuqVavGwIEDefHFF836CC7jyJEjDB48mNTUVCIiIujQoQNr1qxxfJcAjz/+ODk5Odx///2cPHmS9u3bs2DBAoKDg52aZe7cuY6iCux9ck6cOMHQoUM5fvw44eHhDBgwgOeff96pxxURkcunwGrj0S+2UGA1uLZpJP1b1zQ7koPFKO39xhVcRkYGoaGhpKenn3WJKjc3l4MHDzpGTJbSS01NJTo6msTExLNGSC4pnQcREdf32i97eO2XvYQFeLNgTFeqB5fvv9cX+v39d6bfLSXuJS0tjcmTJ19yYSMiIq5v29F03ly0D4AX+jUv98KmtEy9LCXup2HDhjRs2NDsGCIiUk7yCq089uUWCm0G1zeP4qaW0Rd/02WmlhsREREpsf/9uo9dyZlUC/Thxf7NXeLuqL9TcSMiIiIlsiXxFFOX2gdhfbF/c6oFmTfFwoWouBEREZGLyi2wX46y2gxualWD61u43uWoIipuRERE5KKm/LKHvSmnCQ/y5YW+zcyOc0EqbkREROSCNhw+yXvLDgDw8s3NqRLoY3KiC1NxIyIiIueVW2Bl3JdbsBkwoE1NejVz/aE+VNyIiIjIef3n590cSM2ierAvz93k2pejiqi4ERERkXNadyiND1YeBOCVf7QkNMDb5EQlo+LGTUydOpWWLVs6Zj7v2LEjP/74Y7FtDMNgwoQJ1KhRA39/f7p378727dvLLVNOTg4BAQHs2rWr3I4hIiLlIzu/kMe+3IJhwMB2tejRuLrZkUpMxY2bqFWrFpMmTWL9+vWsX7+eq6++mn79+hUrXl599VUmT57Mm2++ybp164iKiuLaa68lMzOzXDItXLiQmJgYGjduXC77FxGR8vPqT7s5fCKb6FA/nrmxqdlxSkXFjZu46aab6NOnj2P6g5deeomgoCDWrFkD2FttXnvtNZ5++mkGDBhA8+bN+eijj8jOzubzzz8/736HDx9O//79efnll4mMjCQsLIznn3+ewsJCxo0bR9WqValVqxYffvjhWe+dO3cuffv2BWDLli306NGD4OBgQkJCiI+PZ/369eXzZYiISJms3n+CGasOAfbLUSF+FeNyVBHNLXUxhgEF2eYc2zsALmFYa6vVypdffklWVhYdO3YE4ODBgyQnJ9OrVy/Hdr6+vnTr1o1Vq1Zx7733nnd/ixYtolatWixbtoyVK1cycuRIVq9eTdeuXfntt9+YPXs2o0aN4tprryUmJgYAm83G999/z9dffw3A7bffTps2bZg6dSqenp5s3rwZb++K9ZdFRKQyyMorZNxXWwAYfGVtujaMMDlR6am4uZiCbHi5hjnHfuoY+ASWePOtW7fSsWNHcnNzCQoKYs6cOTRtam9KTE5OBiAyMrLYeyIjIzl8+PAF91u1alXeeOMNPDw8aNSoEa+++irZ2dk89dRTAIwfP55JkyaxcuVKbr31VgDWrFmDzWajU6dOACQkJDBu3DjHJaoGDRqU+HOJiMjlM/HHnRw5mUPNMH+evqGJ2XEuiS5LuZFGjRqxefNm1qxZw3333cewYcPYsWNHsW3+PsGZYRgXnfSsWbNmeHj8+UclMjKSFi1aOJ57enpSrVo1UlJSHOvmzp3LjTfe6Hjf2LFjueuuu+jZsyeTJk1i//79l/w5RUSkfKzYm8qnaxIA+L9bWhLkWzHbQCpm6svJO8DegmLWsUvBx8eH+vXrA9CuXTvWrVvH66+/zrvvvktUlH3QpeTkZKKj/5wPJCUl5azWnLNi/O3ykcViOec6m83meD5v3jwmTpzoeD5hwgRuu+02fvjhB3788Ueee+45Zs2axc0331yqzygiIuUjM7eAx89cjhraMZZO9cNNTnTp1HJzMRaL/dKQGY8yTiNvGAZ5eXkAxMXFERUVxcKFCx2v5+fns3TpUselI2fZu3cvhw4dKta/B6Bhw4aMGTOGBQsWMGDAAKZPn+7U44qIyKV76YedHEvPpXbVAJ7oXbHvclXLjZt46qmnuP7664mJiSEzM5NZs2axZMkSfvrpJ8DesjJ69GhefvllGjRoQIMGDXj55ZcJCAjgtttuc2qWuXPn0rNnTwIC7C1POTk5jBs3jltuuYW4uDiOHDnCunXr+Mc//uHU44qIyKVZsjuFWesSsVjsl6MCK+jlqCIVO704HD9+nCFDhpCUlERoaCgtW7bkp59+4tprr3Vs8/jjj5OTk8P999/PyZMnad++PQsWLCA4ONipWebOncuwYcMczz09PTlx4gRDhw7l+PHjhIeHM2DAAJ5//nmnHldEREovPaeAJ7/eCsCdneJoX7eayYnKzmIYhmF2iMspIyOD0NBQ0tPTCQkJKfZabm4uBw8eJC4uDj8/P5MSVmypqalER0eTmJjo6OdTWjoPIiKXz6NfbOHrjUeICw9k/sNd8PfxNDvSOV3o9/ffqc+NOFVaWhqTJ0++5MJGREQun192HOfrjUewWOA//2zpsoVNaemylDhV0QjJIiLi2k5l5zN+jv1y1N1d6hIfW9XkRM6jlhsREZFKaMK87fyRmUe9iEDGXute/ylVcSMiIlLJ/LQtmW83H8PDAv8d2Bo/b/e4HFVExc05VLI+1i5H37+ISPlJy8rnmW/tl6NGdatH65gwcwOVAxU3f1E06m52tkkTZQrw5/eviTVFRJzv2bnbSD2dT6PIYB7p6Z7z/KlD8V94enoSFhbmmCMpICDgovMuifMYhkF2djYpKSmEhYXh6elezaQiImb7/vdj/PB7Ep4eFv47sBW+Xu7576yKm78puoX5r5NAyuUVFhamW8lFRJzsj8w8nv12GwAP9KhP85qhJicqPypu/sZisRAdHU316tUpKCgwO06l4+3trRYbEREnMwyDZ77dysnsAppEh/Bgj/pmRypXKm7Ow9PTU79kRUTELczbcoyftx/H29PCf//ZCh8v9+5y696fTkREpJKz2QymLNwDwIM9GtC0xoWnLnAHKm5ERETc2Ip9qRw6kU2wrxd3d40zO85loeJGRETEjX2y5jAA/4ivRYBP5eiNouJGRETETR09lcOvO48DcEeHWJPTXD4qbkRERNzU578dxmZAp3rVqF89yOw4l42KGxERETeUV2hl9rpEAIZUolYbUHEjIiLiln7alkzq6XwiQ3y5tmmk2XEuKxU3IiIibujTMx2Jb7syFi/PyvXrvnJ9WhERkUpgZ1IG6w6dxMvDwq1Xxpgd57JTcSMiIuJmim7/vq5ZFJEhfianufxU3IiIiLiRjNwCvt10FKhct3//lYobERERNzJn41Gy8600qB5Eh7pVzY5jChU3IiIibsIwDMclqSEdY7FYLCYnMoeKGxERETex5kAa+1JOE+Djyc1tapodxzSmFzdvv/02cXFx+Pn5ER8fz/Lly8+77ZIlS7BYLGc9du3adRkTi4iIuKZP1hwC4OY2NQn28zY3jIlMLW5mz57N6NGjefrpp9m0aRNdunTh+uuvJyEh4YLv2717N0lJSY5HgwYNLlNiERER13Q8I5eft9vnkRrSsXJ2JC5ianEzefJkRo4cyV133UWTJk147bXXiImJYerUqRd8X/Xq1YmKinI8PD09L1NiERER1zRzbQJWm8GVdarSOCrE7DimMq24yc/PZ8OGDfTq1avY+l69erFq1aoLvrdNmzZER0dzzTXXsHjx4gtum5eXR0ZGRrGHiIiIOymw2pi51n7V445K3moDJhY3qampWK1WIiOLz3cRGRlJcnLyOd8THR3NtGnT+Prrr/nmm29o1KgR11xzDcuWLTvvcSZOnEhoaKjjERNT+UZqFBER9/bLjuMcz8gjPMiH3s2izI5jOi+zA/z9NjXDMM5761qjRo1o1KiR43nHjh1JTEzkP//5D127dj3ne8aPH8/YsWMdzzMyMlTgiIiIW/l4tf3271uvqI2Pl+n3CpnOtG8gPDwcT0/Ps1ppUlJSzmrNuZAOHTqwd+/e877u6+tLSEhIsYeIiIi72JeSyeoDJ/CwwOD2tc2O4xJMK258fHyIj49n4cKFxdYvXLiQTp06lXg/mzZtIjo62tnxREREKoRP19j72vRsEknNMH+T07gGUy9LjR07liFDhtCuXTs6duzItGnTSEhIYNSoUYD9ktLRo0f5+OOPAXjttdeoU6cOzZo1Iz8/n08//ZSvv/6ar7/+2syPISIiYoqsvEK+3nAE0O3ff2VqcTNo0CBOnDjBCy+8QFJSEs2bN2f+/PnExtpPUFJSUrExb/Lz83nsscc4evQo/v7+NGvWjB9++IE+ffqY9RFERKSiSlwLuenQ4Fqzk1yyuZuPkZlXSFx4IJ3rhZsdx2VYDMMwzA5xOWVkZBAaGkp6err634iIVFb5WfDOVZB2APpPhda3mZ2o1AzD4PrXl7MrOZNnbmjCXV3qmh2pXJXm97e6VIuISOXzy/P2wiakJjS+wew0l2TD4ZPsSs7Ez9uDf8brLuC/UnEjIiKVy8HlsPZd+3Lf/4FfqLl5LlHR7N/9WtUkNKDyziN1LipuRESk8sg7DXPvty/HD4f615ga51Klns5j/tYkQB2Jz0XFjYiIVB4L/wWnEiC0NvR60ew0l2z2ukQKrAatY8JoXrNitjyVJxU3IiJSOexfDOs/sC/3exN8g83Nc4msNoPPf7PfSTykg1ptzkXFjYiIuL/cDJj3kH35iruhbjdz85TBol0pHD2VQ5UAb25oqUFsz0XFjYiIuL8FT0N6IlSpAz0nmJ2mTIo6Eg+8IgY/b0+T07gmFTciIuLe9v4CGz8GLNDvbfANMjvRJTuUmsWyPX9gscDtV+qS1PmouBEREfeVc+rPy1Ed7oM6nU2NU1afnmm16d4wgtrVAkxO47pU3IiIiPv6+SnIPAZV68HVz5qdpkxy8q18qXmkSkTFjYiIuKfdP8HmzwCLfYoFn4rd0vHd78dIzymgVhV/ujWsbnYcl6biRkRE3E92Gnz3sH2504NQu725eZyg6JLU7e1j8fSwmJzGtam4ERER9/PjE3D6OIQ3hB7PmJ2mzLYknuL3I+n4eHkw6ArNI3UxKm5ERMS97PwOtn4BFg/o/w54+5mdqMw+Xm1vtbmxRTRVA31MTuP6VNyIiIj7yDoB34+xL3ceDbXiTY3jDCez8vnu92MA3KGOxCWi4kZERNzH/Ech6w+IaALdnzQ7jVN8uSGR/EIbzWqE0CYmzOw4FYKKGxERcQ/bvoHtc8DiCTdPBS9fsxOVmc1m8OmaP+eRsljUkbgkVNyIiEjFdzoFfnjUvtz1MajRxtw8TrJs7x8kpGUT7OdFv9Y1zY5TYai4ERGRis0w7P1sctIgsgV0eczsRE7zyZmOxP+Mj8HfR/NIlZSKGxERqdi2fQ27vgcPrzOXo9zjbqLEtGwW7U4B4PYOtU1OU7GouBERkYorM/nPy1HdnoCoFubmcaLP1yZgGHBV/XDqRVTcyT7NoOJGREQqJsOA70ZD7imIbg1XjTE5kPPkFVqZvS4RgDs66Pbv0lJxIyIiFdOWWbDnR/D0sc8d5eltdiKn+XFrMmlZ+USH+tGzieaRKi0VNyIiUvFkHLNPsQDQfTxENjU3j5N9vPoQALddWRsvT/2qLi19YyIiUrEYBsx7GPLSoWY8dHrY7EROte1oOhsTTuHlYWHQlZpH6lKouBERkYpl0yewbyF4+p65HOVldiKn+uw3++3fvZtHUT244s+LZQYVNyIiUnGcSoSfnrIvX/MsRDQyN4+TpecU8O0m+zxSQ9SR+JKpuBERkYrBMGDeg5CfCTHtocP9Zidyum82HiGnwEqjyGCujKtqdpwKS8WNiIhUDBumw4El4OUP/d4GD/casdcwDD5ZY78kdUdHzSNVFipuRETE9Z08BD8/Y1/u+RyE1zc1TnlYtf8EB/7IItDHk5vbaB6pslBxIyIirs1mg7kPQkEWxHaGK+81O1G5KJpHakDbWgT5ulcn6ctNxY2IiLi2HXPg0HLwDoR+b4GH+/3qSkrPYeHO4wAM6aiOxGXlfn9CRETEvax9z/6z00NQNc7cLOVk5tpErDaD9nFVaRgZbHacCk/FjYiIuK7kbZCw2j7jd/xws9OUi/xCGzPXJgBqtXEWFTciIuK61n9g/9n4RgiJNjdLOVmwI5k/MvOICPalV9Mos+O4BRU3IiLimnLTYcts+/IVd5mbpRwVdSQefEUMPl76tewM+hZFRMQ1bZltv0MqojHUucrsNOViz/FMfjuYhqeHhcHta5sdx22ouBEREddjGLDuffvyFXeBmw5o9+mZQfuubRJJdKi/yWnch4obERFxPYdWQOpu++3fLQeZnaZcnM4r5JuNRwF1JHY2FTciIuJ6ilptWg0CvxBzs5STuZuPcjqvkLrhgXSqV83sOG5FxY2IiLiWjCTY9b192U07EhuGwWdr7Ld/39a+tuaRcjIVNyIi4lo2fgS2QqjdCSKbmZ2mXGw5ks6OpAx8vDz4R9taZsdxOypuRETEdVgLYMMM+/IVI02NUp4+/83ekfiGFtFUCfQxOY37KXVxM3z4cJYtW1YeWUREpLLbPR8ykyAwApr0NTtNucjILeC7LUmA/ZKUOF+pi5vMzEx69epFgwYNePnllzl69Gh55BIRkcqoaB6ptsPAyz1bNL7ddJScAisNqgfRLraK2XHcUqmLm6+//pqjR4/y4IMP8uWXX1KnTh2uv/56vvrqKwoKCkod4O233yYuLg4/Pz/i4+NZvnx5id63cuVKvLy8aN26damPKSIiLuiP3fbZvy0ebjuPlGEYfP6bOhKXt0vqc1OtWjUeeeQRNm3axNq1a6lfvz5DhgyhRo0ajBkzhr1795ZoP7Nnz2b06NE8/fTTbNq0iS5dunD99deTkJBwwfelp6czdOhQrrnmmkuJLyIirmjdmXmkGvWBsBhzs5STjQmn2JWcia+XBwPaqCNxeSlTh+KkpCQWLFjAggUL8PT0pE+fPmzfvp2mTZsyZcqUi75/8uTJjBw5krvuuosmTZrw2muvERMTw9SpUy/4vnvvvZfbbruNjh07liW+iIi4irzTsGWmfdmNOxJ/dqYj8Y0taxAa4G1yGvdV6uKmoKCAr7/+mhtvvJHY2Fi+/PJLxowZQ1JSEh999BELFizgk08+4YUXXrjgfvLz89mwYQO9evUqtr5Xr16sWrXqvO+bPn06+/fv57nnnitR3ry8PDIyMoo9RETExWz9EvIyoGo9iOtudppykZ5dwA+/2zsS395BHYnLk1dp3xAdHY3NZmPw4MGsXbv2nH1errvuOsLCwi64n9TUVKxWK5GRkcXWR0ZGkpycfM737N27lyeffJLly5fj5VWy6BMnTuT5558v0bYiImKCYvNIjQQP9xyl5OuNR8grtNE4Kpg2MWFmx3FrpS5upkyZwj//+U/8/PzOu02VKlU4ePBgifb3985UhmGcs4OV1Wrltttu4/nnn6dhw4Ylzjt+/HjGjh3reJ6RkUFMjHteyxURqZASf4Pj28DLH1rfZnaacmEYBp+vtfcnvV0dictdqcvjxYsXn/OuqKysLEaMGFHi/YSHh+Pp6XlWK01KSspZrTlgvwV9/fr1PPjgg3h5eeHl5cULL7zAli1b8PLyYtGiRec8jq+vLyEhIcUeIiLiQopabVr8A/zd89bodYdOsi/lNP7envRrU9PsOG6v1MXNRx99RE5Ozlnrc3Jy+Pjjj0u8Hx8fH+Lj41m4cGGx9QsXLqRTp05nbR8SEsLWrVvZvHmz4zFq1CgaNWrE5s2bad++fWk/ioiImO30H7D9W/vyFXebGqU8FY1I3LdVDUL81JG4vJX4slRGRgaGYWAYBpmZmcUuS1mtVubPn0/16tVLdfCxY8cyZMgQ2rVrR8eOHZk2bRoJCQmMGjUKsF9SOnr0KB9//DEeHh40b9682PurV6+On5/fWetFRKSC2PQx2AqgZjuo0drsNOUiLSuf+dvsVyk0IvHlUeLiJiwsDIvFgsViOWefF4vFUuqOu4MGDeLEiRO88MILJCUl0bx5c+bPn09sbCxgv9X8YmPeiIhIBWWzwvrp9mU3nf0b4OsNR8gvtNGsRggta4WaHadSsBiGYZRkw6VLl2IYBldffTVff/01VatWdbzm4+NDbGwsNWrUKLegzpKRkUFoaCjp6enqfyMiYqZd82HWYHs/m7G7wPv8N6pUVIZhcM1/l3IgNYuXb26hlpsyKM3v7xK33HTr1g2AgwcPUru2enqLiEgZFXUkbjPELQsbgNUHTnAgNYtAH0/6tnb9BgB3UaLi5vfff6d58+Z4eHiQnp7O1q1bz7tty5YtnRZORETc1In9sP9XwALtSn6nbUVTNI9UvzY1CfIt9egrcolK9E23bt2a5ORkqlevTuvWrbFYLJzrapbFYsFqtTo9pIiIuJn1H9p/NrgWqsaZm6WcpJ7O4+ftZzoSX6nLUZdTiYqbgwcPEhER4VgWERG5ZPnZsOlT+7IbdyT+asMRCqwGrWqF0rymOhJfTiUqboruXvr7soiISKlt/wZyT0FYbajf0+w05cJmM5h5ZkRidSK+/EpU3MybN6/EO+zbt+8lhxERkUqgqCNxuxHg4WlulnKycn8qh09kE+zrxU2t1JH4citRcdO/f/8S7Ux9bkRE5IKOboBjm8DTF9oMNTtNuSnqSHxz25oE+Kgj8eVWom/cZrOVdw4REakM1p5ptWl2MwRWMzdLOUnJzGXhjuOALkmZxT3nlRcREdeTnQbbvrYvu3FH4i/XH6HQZtC2dhiNozRYrBlK1HLzxhtvcM899+Dn58cbb7xxwW0ffvhhpwQTERE3s+lTsOZBVEuo1c7sNOWieEdi3YBjlhIVN1OmTOH222/Hz8+PKVOmnHc7i8Wi4kZERM5ms8H6D+zLV9wFbjrK/bK9f3DkZA4hfl7c2DLa7DiVVonHuTnXsoiISInsXwQnD4FvKLT4p9lpyk1RR+IBbWvh5+2ed4JVBGXqc2MYxjlHKhYRESlm3Xv2n21uB58Ac7OUk+T0XH7dlQLA7epIbKpLKm4++OADmjdvjp+fH35+fjRv3pz333/f2dlERMQdnDwMe362L7vxPFKz1yVitRlcWacqDSKDzY5TqZX65vtnn32WKVOm8NBDD9GxY0cAVq9ezZgxYzh06BAvvvii00OKiEgFtmE6YEDd7hDewOw05cJqM5i9TiMSu4pSFzdTp07lvffeY/DgwY51ffv2pWXLljz00EMqbkRE5E+FebDxY/uyG9/+vWR3CsfSc6kS4E3v5lFmx6n0Sn1Zymq10q7d2bfwxcfHU1hY6JRQIiLiJnbMhewTEFITGl5vdppyU9SR+B/qSOwSSl3c3HHHHUydOvWs9dOmTeP22293SigREXETa890JI6/EzzdcxqCY6dyWLzb3pF4sC5JuYQS/UkbO3asY9lisfD++++zYMECOnToAMCaNWtITExk6FD3nSdERERKKWkLHFkLHl7Q1n1/P8xal4jNgI51q1EvIsjsOEIJi5tNmzYVex4fHw/A/v37AYiIiCAiIoLt27c7OZ6IiFRY684M2tekLwRHmpulnBRabepI7IJKVNwsXry4vHOIiIg7yTkFW7+0L7txR+Jfd6VwPCOPaoE+XNdMHYldhSbOFBER59syCwqyoXpTiO1kdppyU9SR+JZ2tfDx0q9UV3FJvbvWrVvHl19+SUJCAvn5+cVe++abb5wSTEREKijDgHVnBna9YqTbziOVmJbNsr1/ADD4Cl2SciWlLjNnzZpF586d2bFjB3PmzKGgoIAdO3awaNEiQkNDyyOjiIhUJAeXwom94BMELQeZnabczFqXgGHAVfXDqRMeaHYc+YtSFzcvv/wyU6ZM4fvvv8fHx4fXX3+dnTt3MnDgQGrXVuUqIlLpFbXatLoVfN1zGoICq40v1h8B1JHYFZW6uNm/fz833HADAL6+vmRlZWGxWBgzZgzTpk1zekAREalA0o/Crvn2ZTfuSPzLjuP8kZlHRLAv1zZ1zzvBKrJSFzdVq1YlMzMTgJo1a7Jt2zYATp06RXZ2tnPTiYhIxbJhBhhWiL0KqjcxO025+exMR+KB7Wrh7amOxK6m1B2Ku3TpwsKFC2nRogUDBw7kkUceYdGiRSxcuJBrrrmmPDKKiEhFYC34yzxS7jv796HULFbsS8VigVvVkdgllbq4efPNN8nNzQVg/PjxeHt7s2LFCgYMGMCzzz7r9IAiIlJB7P4RTidDYAQ0vsnsNOVm5plB+7o2iCCmaoDJaeRcSl3cVK1a1bHs4eHB448/zuOPP+7UUCIiUgFtmG7/2eYO8PIxN0s5yS+08ZU6Eru8Sxrnxmq1MmfOHHbu3InFYqFJkyb069cPLy/3nBRNREQuIu0g7F9kX247zNws5ejn7cmcyMonMsSXaxpXNzuOnEepq5Ft27bRr18/kpOTadSoEQB79uwhIiKCefPm0aJFC6eHFBERF7fxI/vPeldD1Thzs5SjohGJB11RGy91JHZZpT4zd911F82aNePIkSNs3LiRjRs3kpiYSMuWLbnnnnvKI6OIiLiywnzY9Kl9Of5Oc7OUo/1/nGb1gRN4WODWK2LMjiMXUOqWmy1btrB+/XqqVKniWFelShVeeuklrrjiCqeGExGRCmDX95D1BwRFQqPrzU5TbmaeabXp0ag6NcL8TU4jF1LqlptGjRpx/Pjxs9anpKRQv359p4QSEZEKpKgjcduh4OltbpZykltg5auN6khcUZSouMnIyHA8Xn75ZR5++GG++uorjhw5wpEjR/jqq68YPXo0r7zySnnnFRERV3JiPxxcBljsxY2b+mlbMqeyC6gR6kf3RupI7OpKdFkqLCwMy19mdTUMg4EDBzrWGYYBwE033YTVai2HmCIi4pKKWm0aXAth7tui8deOxJ4e7jnLuTspUXGzePHi8s4hIiIVTWEebP7cvuzGHYn3Hs9k7aE0PD0sDFJH4gqhRMVNt27dyjuHiIhUNDu/g+wTEFwDGvQyO025+XytvdXmmsbViQr1MzmNlMQljbp36tQpPvjgA8cgfk2bNmXEiBGEhoY6O5+IiLiq9R/af7YdCp7uOYhrboGVrzeoI3FFU+q7pdavX0+9evWYMmUKaWlppKamMnnyZOrVq8fGjRvLI6OIiLiaP3bD4ZVg8XDrjsTf/55ERm4htar407VBhNlxpIRKXWqPGTOGvn378t577zmmWygsLOSuu+5i9OjRLFu2zOkhRUTExWyYYf/Z4DoIrWlqlPL0+W+HARh8ZW081JG4wih1cbN+/fpihQ2Al5cXjz/+OO3atXNqOBERcUEFOX92JG43wtws5WhXcgYbE07h5WHhn+1qmR1HSqHUl6VCQkJISEg4a31iYiLBwcFOCSUiIi5sx1zIPQWhMVD/GrPTlJui27+vbRpJ9WB1JK5ISl3cDBo0iJEjRzJ79mwSExM5cuQIs2bN4q677mLw4MHlkVFERFzJ+qIRiYeBh6e5WcpJdn4hczYeBeD29rEmp5HSKnVx85///IcBAwYwdOhQ6tSpQ2xsLMOHD+eWW265pBGK3377beLi4vDz8yM+Pp7ly5efd9sVK1bQuXNnqlWrhr+/P40bN2bKlCmlPqaIiFyilJ2QuAYsntDmDrPTlJvvtySRmVdIbLUAOtWrZnYcKaVS9bmxWq2sXr2a5557jokTJ7J//34Mw6B+/foEBASU+uCzZ89m9OjRvP3223Tu3Jl3332X66+/nh07dlC79tm33AUGBvLggw/SsmVLAgMDWbFiBffeey+BgYGakVxE5HIoarVpdD2ERJubpRx9po7EFZrFKJo7oYT8/PzYuXMncXFxZT54+/btadu2LVOnTnWsa9KkCf3792fixIkl2seAAQMIDAzkk08+KdH2GRkZhIaGkp6eTkhIyCXlFhGplPKz4b+NIS8d7vga6vc0O1G52HY0nRv/twJvTwurx19DeJCv2ZGE0v3+LvVlqRYtWnDgwIFLDlckPz+fDRs20KtX8VEte/XqxapVq0q0j02bNrFq1aoLjqCcl5dXbOLPjIyMMuUWEam0tn9jL2zCYqHu1WanKTefrrG32lzXLEqFTQVV6uLmpZde4rHHHuP7778nKSnpkguH1NRUrFYrkZGRxdZHRkaSnJx8wffWqlULX19f2rVrxwMPPMBdd9113m0nTpxIaGio4xETo3lBREQuSdElqfhh4FHqXx8Vwu7kTL48MyLx0I51zA0jl6zU49z07t0bgL59+541U7jFYin1rOB/3cdf93Mhy5cv5/Tp06xZs4Ynn3yS+vXrn/dOrfHjxzN27FjH84yMDBU4IiKllbwVjq4HDy9oM8TsNOXCMAyem7cNq83gumaRXBlX1exIcolKXdw4a4bw8PBwPD09z2qlSUlJOas15++K+vu0aNGC48ePM2HChPMWN76+vvj6qllRRKRMilptGt8IQdXNzVJOftiaxJoDafh6efDMDU3NjiNlUKrixjAMatSoQUFBAQ0bNiw2SnFp+fj4EB8fz8KFC7n55psd6xcuXEi/fv1KlSkvL++Sc4iIyEXknYbfv7Avt7vT3CzlJDu/kJd+2AnAfd3rEVO19HcAi+socXVy6NAh+vXrx7Zt2wCIiYnhm2++oW3btpd88LFjxzJkyBDatWtHx44dmTZtGgkJCYwaNQqwX1I6evQoH3/8MQBvvfUWtWvXpnHjxoB93Jv//Oc/PPTQQ5ecQURELmLb15CfCVXrQp2uZqcpF28t3kdSei61qvgzqls9s+NIGZW4uHniiSfIzc3lk08+wc/Pj//7v/9j1KhRrF279pIPPmjQIE6cOMELL7xAUlISzZs3Z/78+cTG2keDTEpKKjbVg81mY/z48Rw8eBAvLy/q1avHpEmTuPfeey85g4iIXMT6D+0/44e7ZUfiQ6lZvLfsIADP3tgUP2/3HHW5MinxODc1atRg5syZjtuujxw5QmxsLKdPn8bf379cQzqTxrkRESmFY5tgWnfw9IGxOyEw3OxETjdixjoW7Uqha8MIPrrziove1CLmKJdxbpKTkx2Xg8B+O7a/vz/Hjx+/9KQiIuLaijoSN7nJLQubX3ceZ9GuFLw9LTx3U1MVNm6ixMWNxWLB42/NkR4eHpRygGMREakocjNg61f25XYjzM1SDnILrLzw/Q4ARnSOo15EkMmJxFlK3OfGMAwaNmxYrKo9ffo0bdq0KVb0pKWlOTehiIiYY+uXUJAF4Q0htrPZaZzu/eUHOHwim+rBvjx0TQOz44gTlbi4mT59ennmEBERV2IYsKFoROLh4GaXa46eyuHNxfsAeKpPE4J8L31oE3E9JT6bw4YNK88cIiLiSo5utI9K7OkLrc49SGpF9vIPO8ktsHFFnSr0a13D7DjiZO53T5+IiJRd0e3fzfpDgHtNQ7BqXyo/bE3CwwLP922uTsRuSMWNiIgUl3PKPnAfQLx7jUhcYLUx4bvtANzRIZamNTQkiDtScSMiIsX9/gUU5kBEY6jdwew0TvXx6sPsOX6aKgHejL22odlxpJyouBERkT8V60h8p1t1JP4jM4/XFu4B4PHejQkL8DE5kZSXSy5u8vPz2b17N4WFhc7MIyIiZkpcCyk7wMsfWg0yO41TvfLTLjLzCmlRM5SB7WLMjiPlqNTFTXZ2NiNHjiQgIIBmzZo55n56+OGHmTRpktMDiojIZVTUatN8APhXMTeLE21MOMlXG44A8Hy/Znh6uE+LlJyt1MXN+PHj2bJlC0uWLMHPz8+xvmfPnsyePdup4URE5DLKToPtc+zLbtSR2GozeG6uvRPxLfG1aFvbfYo2ObdSj1r07bffMnv2bDp06FDs9rmmTZuyf/9+p4YTEZHLaMssKMyFyOZQq53ZaZzmi/WJbD2aTrCvF0/0bnzxN0iFV+qWmz/++IPq1auftT4rK0tjBYiIVFRuOiLxqex8Xv1pFwCjr21IRLCvyYnkcih1cXPFFVfwww8/OJ4XFTTvvfceHTt2dF4yERG5fA6vgtQ94B0ALQeancZpJi/cw8nsAhpGBjG0Y6zZceQyKfVlqYkTJ9K7d2927NhBYWEhr7/+Otu3b2f16tUsXbq0PDKKiEh5c3Qk/gf4hZqbxUl2HMvg0zWHAZjQtxnenhr9pLIo9Znu1KkTq1atIjs7m3r16rFgwQIiIyNZvXo18fHx5ZFRRETKU9YJ2DHXvtzOPToSG4bBhHnbsRlwQ4toOtULNzuSXEalarkpKCjgnnvu4dlnn+Wjjz4qr0wiInI5bfkcrPkQ3QpqtDU7jVPM23KMtYfS8Pf25KkbmpgdRy6zUrXceHt7M2fOnPLKIiIil5thwHr3GpH4dF4hL/2wE4AHetSjZpi/yYnkciv1Zambb76Zb7/9thyiiIjIZXdwGaTtB58gaHGL2Wmc4n+L9pKSmUdstQDu6lLX7DhiglJ3KK5fvz7//ve/WbVqFfHx8QQGBhZ7/eGHH3ZaOBERKWdFHYlb/BN8g83N4gT7/zjNhysOAvCvG5vi5+1pciIxg8UwDKM0b4iLizv/ziwWDhw4UOZQ5SkjI4PQ0FDS09MJCdFU9yJSiZ3+AyY3AVsB3LvM3uemAjMMg6EfrmX53lSublydD4dfYXYkcaLS/P4udcvNwYMHLzmYiIi4kM2f2gubGm0rfGEDsGDHcZbvTcXH04N/3djU7DhiIt30LyJSGdlssGGGfdkNbv/OLbDy7+93AHB31zjqhAde5B3izkrdcjNixIgLvv7hhx9echgREblM9vwIJw+Bb4h94L4K7p2l+zlyMofoUD8e6FHf7DhislIXNydPniz2vKCggG3btnHq1CmuvvpqpwUTEZFyUpALPz9lX243AnwqditHYlo2U5fYJ25++oYmBPiU+lebuJlS/wk41zg3NpuN+++/n7p1dcudiIjLW/0/e6tNUBR0fczsNGX24g87yCu00bFuNW5oEW12HHEBTulz4+HhwZgxY5gyZYozdiciIuXlVCIs+699udeLFf7272V7/uDn7cfx9LAwoW8zx2TOUrk5rUPx/v37KSwsdNbuRESkPCx4BgpzoHanCj9oX36hjQnfbQdgaMdYGkVV7EJNnKfUl6XGjh1b7LlhGCQlJfHDDz8wbNgwpwUTEREnO7AUdnwLFg/o82qFn2phxqqDHPgji/AgH0b3bGh2HHEhpS5uNm3aVOy5h4cHERER/Pe//73onVQiImISawH8+Lh9ud1IiGphbp4ySsnI5fVf9gLweO/GhPp7m5xIXEmpi5vFixeXRw4RESlPa9+DP3ZBQDXo8ZTZacps4o+7yMq30jomjFva1jI7jrgYDeInIuLuTqfAkon25Wv+BQFVzc1TRusOpTFn01EsFni+bzM8PCr25TVxvhK13LRp06bEPdA3btxYpkAiIuJkv0yAvAyIbg1thpidpkysNoPn5to7EQ9qF0OrmDBzA4lLKlFx079//3KOISIi5SJxLWz+zL7c5z/gUbFnyf58bQI7kjII8fNi3HWNzI4jLqpExc1zzz1X3jlERMTZbFaYP86+3Pp2iKnYs2SnZeXzn593A/Bor0ZUC/I1OZG4KvW5ERFxVxs/hqTN9vmjek4wO02Z/WfBbtJzCmgcFczt7WubHUdcWIlabqpWrcqePXsIDw+nSpUqF+x/k5aW5rRwIiJyibLT4NcX7Mvdx0NQdXPzlNGu5Axmrk0A7J2IvTz1f3M5vxIVN1OmTCE42D7y42uvvVaeeURExBkWvww5aRDRBK682+w0Zfbu0gMYBvRpEUX7utXMjiMurkTFzV9HHtYoxCIiLi55K6z/wL7c51XwrNgD3B05mc28LccAuL97fZPTSEVQpnnhc3JyKCgoKLYuJCSkTIFERKQMDMPeidiwQbObIa6r2YnK7MMVh7DaDDrXr0bzmqFmx5EKoNQXLbOysnjwwQepXr06QUFBVKlSpdhDRERMtPVLSFgN3gH2Wb8ruPTsAmats/e1ubdrPZPTSEVR6uLm8ccfZ9GiRbz99tv4+vry/vvv8/zzz1OjRg0+/vjj8sgoIiIlkZcJC561L3d5FEIr/rQEn/52mOx8K02iQ+jSINzsOFJBlPqy1HfffcfHH39M9+7dGTFiBF26dKF+/frExsby2Wefcfvtt5dHThERuZilr8LpZKgSB50eMjtNmeUWWJm+8iAAo7rVLfFI+SKlbrlJS0sjLi4OsPevKbr1+6qrrmLZsmWlDvD2228TFxeHn58f8fHxLF++/LzbfvPNN1x77bVEREQQEhJCx44d+fnnn0t9TBERt5O6F9ZMtS/3ngReFX+Au282HiX1dD41w/zp0yLa7DhSgZS6uKlbty6HDh0CoGnTpnzxxReAvUUnLCysVPuaPXs2o0eP5umnn2bTpk106dKF66+/noSEhHNuv2zZMq699lrmz5/Phg0b6NGjBzfddBObNm0q7ccQEXEfhgE/Pg62AmhwHTTqbXaiMrPaDN5bfgCAkVfF4a1xbaQULIZhGKV5w5QpU/D09OThhx9m8eLF3HDDDVitVgoLC5k8eTKPPPJIiffVvn172rZty9SpUx3rmjRpQv/+/Zk4cWKJ9tGsWTMGDRrEv/71rxJtn5GRQWhoKOnp6bqzS0Tcw87vYfbt4OkD96+BahW/4+1P25IY9elGQv29WfXk1QT6lunmXnEDpfn9XeI/LQcOHCAuLo4xY8Y41vXo0YNdu3axfv166tWrR6tWrUocMj8/nw0bNvDkk08WW9+rVy9WrVpVon3YbDYyMzOpWrVqiY8rIuJWCnLg5/H25U4PuUVhYxgG7yy1t9oM7RirwkZKrcR/Yho0aEBSUhLVq9uH8B40aBBvvPEGtWvXpnbt0s/xkZqaitVqJTIystj6yMhIkpOTS7SP//73v2RlZTFw4MDzbpOXl0deXp7jeUZGRqmzioi4rJWvw6kECKlpv0PKDaw7dJLNiafw8fJgWKc6ZseRCqjEFzH/fvVq/vz5ZGVllTnA33u/G4ZRoh7xM2fOZMKECcyePdtRcJ3LxIkTCQ0NdTxiYmLKnFlExCWcPAwrptiXe70IPoHm5nGSd5fuB+CW+FqEa+ZvuQSm9dAKDw/H09PzrFaalJSUs1pz/m727NmMHDmSL774gp49e15w2/Hjx5Oenu54JCYmljm7iIhL+PkpKMyFOl3soxG7gT3HM/l1VwoWC9zdpa7ZcaSCKnFxY7FYzmpRKcuYAz4+PsTHx7Nw4cJi6xcuXEinTp3O+76ZM2cyfPhwPv/8c2644YaLHsfX15eQkJBiDxGRCm//Itj1PVg84fpXwU3GgJm2zN7XpnezKOLC3aMlSi6/Eve5MQyD4cOH4+trbyLMzc1l1KhRBAYW/8P3zTfflPjgY8eOZciQIbRr146OHTsybdo0EhISGDVqFGBvdTl69Khj5OOZM2cydOhQXn/9dTp06OBo9fH39yc0VPONiEglUZgPPz5hX77yHohsam4eJ0lOz2Xu5qMA3NNVrTZy6Upc3Px9NvA77rijzAcfNGgQJ06c4IUXXiApKYnmzZszf/58YmNjAUhKSio25s27775LYWEhDzzwAA888ECxbDNmzChzHhGRCuG3dyB1DwSEQ/cnL759BTF95UEKrAbt46rSprbmKpRLV+pxbio6jXMjIhVaZjL8Lx7yT0O/t6BN2f+j6QoycgvoNHERp/MK+XB4O65ufOG+l1L5lOb3t4Z8FBGpSBb+y17Y1GwHrW4zO43TfP5bAqfzCmkYGUT3hue/A1akJFTciIhUFIdXw++zAQv0eRU83OOf8LxCKx+usE+QeU/Xenh4uEfnaDGPe/zNEBFxdzYr/DjOvtx2CNSMNzePE83ddIyUzDyiQvzo26qG2XHEDai4ERGpCDZMh+St4BcK1zxndhqnsdkMpv1lgkwfL/1akrLTnyIREVeXdQJ+/bd9ucczEBhubh4nWrQrhX0ppwn29eLWKzWCvDiHihsREVe3+CXIPQWRzaHdCLPTONW7y+xTLdzeIZZgP2+T04i7UHEjIuLKMpJgo30gU3pPAk/3mSF7w+E01h06iY+nB3d2rmN2HHEjKm5ERFzZb1PBVgC1O0JcF7PTONW7S+19bW5uU5PIED+T04g7UXEjIuKqcjNg/XT7cudHzM3iZPv/OM3CnccBuFtTLYiTqbgREXFVG2ZAXgaEN4IG15mdxqneX34Aw4CeTSKpXz3I7DjiZlTciIi4osJ8WPO2fbnTQ24zYB9ASmYuX2+wT5A5qptabcT53Odvi4iIO9n6JWQmQVAUtBxodhqnmrHyEPlWG/GxVWhXp6rZccQNqbgREXE1NhusesO+3OE+8PI1N48Tnc4r5JM1hwG4V31tpJyouBERcTX7FsIfu8AnGNrdaXYap5q1NoHM3ELqRgTSs4lm/pbyoeJGRMTVrHzd/rPdnfbpFtxEgdXGB0UTZHapqwkypdyouBERcSWJ6+DwSvDwtl+SciPfbTlGUnouEcG+9G9T0+w44sZU3IiIuJJVZ1ptWg6EEPeZIdswDMegfXd2roOft6fJicSdqbgREXEVqftg5/f25U4PmZvFyZbs+YPdxzMJ9PHk9vaxZscRN6fiRkTEVax+EzCgYW+o3sTsNE417UyrzeAraxPqrwkypXypuBERcQWnU2Dz5/ZlN5tqYUviKVYfOIGXh4URV8WZHUcqARU3IiKu4Ld3wZoHNdvZJ8l0I9OW2Vtt+rauQY0wf5PTSGWg4kZExGx5p2Hd+/blzo+AxX1ukT6UmsWP25IAuEeD9sllouJGRMRsmz6B3FNQtR40vsHsNE71/ooD2Azo0SiCxlEhZseRSkLFjYiImawFsPot+3Knh8DDfW6RTj2dx5frjwBwT9d6JqeRykTFjYiImbbPgfRECIyAVoPNTuNUH68+TF6hjVa1QulQVxNkyuWj4kZExCyG8edUC+1HgbefuXmcKDu/kI9XHwLg3m71sLhRPyJxfSpuRETMsv9XOL4NvAPhipFmp3GqL9Ylciq7gNhqAVzXLMrsOFLJqLgRETHLyjfsP+OHgX8Vc7M4UaHVxnvL7RNk3t2lLp6aIFMuMxU3IiJmOLYJDi4Fiyd0uN/sNE71w9Ykjp7KoVqgD7fE1zI7jlRCKm5ERMxQ1GrT4hYIizE3ixMZhuEYtG9YJ02QKeZQcSMicrmlHYQd39qXOz1sahRnW7nvBNuPZeDv7cmQDpogU8yh4kZE5HJb/RYYNqh3DUQ1NzuNU727bD8Ag66IoUqgj8lppLJScSMicjllnYBNn9qX3WyCzG1H01m+NxVPDwsjNUGmmEjFjYjI5bTuPSjMgejWENfV7DROVdTX5oYW0cRUDTA5jVRmKm5ERC6X/Gz77N/gdhNkJqZl88NWTZAprkHFjYjI5bL5M8hJgyp1oElfs9M41QcrDmK1GXRpEE7zmqFmx5FKTsWNiMjlYC2E1W/alzs+CJ5e5uZxopNZ+cxelwjAvZogU1yAihsRkcth5zw4eQgCqkHr281O41SfrDlMToGVptEhdK5fzew4IipuRETK3V8nyLzyHvBxn862uQVWPlp1CIB7u9XVBJniElTciIiUt4PLIGkzePnDFXebncapvtpwhBNZ+dQM8+eGFtFmxxEBVNyIiJS/olabtkMg0H0u25zMyueNX/cCcHeXOLw89StFXIP+JIqIlKfkbbD/V7B4QMcHzE7jNIZh8My320jJzKNuRCC3Xlnb7EgiDipuRETK06ozE2Q27W+/BdxNzN18jB+2JuHpYWHKwNaaIFNcioobEZHycioBtn5lX+7sPhNkHjuVw7NztwHw8NUNaBUTZm4gkb9RcSMiUl7WTAXDCnHdoEYbs9M4hc1mMO6rLWTmFtIqJowHemhcG3E9phc3b7/9NnFxcfj5+REfH8/y5cvPu21SUhK33XYbjRo1wsPDg9GjR1++oCIipZFzEjZ8ZF92owkyZ6w6xMp9J/Dz9mDKwFbqRCwuydQ/lbNnz2b06NE8/fTTbNq0iS5dunD99deTkJBwzu3z8vKIiIjg6aefplWrVpc5rYhIKaz7AAqyILIF1Lva7DROsfd4Jq/8tAuAp/s0oW5EkMmJRM7N1OJm8uTJjBw5krvuuosmTZrw2muvERMTw9SpU8+5fZ06dXj99dcZOnQooaGau0REXFRBLvz2jn3ZTSbIzC+0MeaLzeQV2ujaMII7OsSaHUnkvEwrbvLz89mwYQO9evUqtr5Xr16sWrXKacfJy8sjIyOj2ENEpFxtmQlZf0BoDDTrb3Yap/jfor1sO5pBWIA3/3dLS41ELC7NtOImNTUVq9VKZGRksfWRkZEkJyc77TgTJ04kNDTU8YiJiXHavkVEzmKzwqr/2Zc7PgCe3ubmcYINh0/y1uJ9ALzUvwWRIX4mJxK5MNN7gv29+jcMw6n/Ixg/fjzp6emOR2JiotP2LSJylt3zIW0/+IVBmyFmpymzrLxCHv1iMzYD+reuwQ0tNcWCuD4vsw4cHh6Op6fnWa00KSkpZ7XmlIWvry++vr5O25+IyHkZBqx4zb585d3gW/E73L40fyeHTmQTHerH8/2amx1HpERMa7nx8fEhPj6ehQsXFlu/cOFCOnXqZFIqEZEy2PMzHF0Pnr5w5b1mpymzxbtS+Pw3+92r//lnK0L9K/4lNqkcTGu5ARg7dixDhgyhXbt2dOzYkWnTppGQkMCoUaMA+yWlo0eP8vHHHzves3nzZgBOnz7NH3/8webNm/Hx8aFp06ZmfAQREbusEzDvIfty+3sgKMLcPGWUlpXPuK9+B2BE5zg61w83OZFIyZla3AwaNIgTJ07wwgsvkJSURPPmzZk/fz6xsfZbDJOSks4a86ZNmz9H+dywYQOff/45sbGxHDp06HJGFxH5k2HA949AVgpENIYez5idqEwMw+DpOVtJPZ1H/epBPN67kdmRRErFYhiGYXaIyykjI4PQ0FDS09MJCQkxO46IuIPNM+HbUeDhDXf/CtEVe5DRbzYeYewXW/DysPDtA51pXlPjion5SvP72/S7pUREKrRTCfDj4/blHuMrfGFz9FQOz83dDsDong1U2EiFpOJGRORS2Wzw7f2QlwEx7aHzaLMTlYnNZvDoF5vJzCukbe0wRnXTpJhSMam4ERG5VGvehkPLwTsQbn4HPDzNTlQmH648yJoDaQT4eDJ5YGtNiikVlv7kiohciuM74NcX7Mu9X4aqdc3NU0a7kzN59efdADxzQ1PqhAeanEjk0qm4EREprcJ8+OYesOZBw97QdpjZicokv9DG6NmbyS+0cXXj6gy+UtPUSMWm4kZEpLSWTITjWyGgGtz0RoWf9fu1X/awMymDKgHeTPpHC02KKRWeihsRkdJIWAMrX7Mv3/Q6BDtvuhgzrD+UxjtL9wMwcUALqgdrUkyp+FTciIiUVF4mzLkXDBu0vh2a3GR2ojI5nVfI2C+2YDPgH21r0bu5JsUU96DiRkSkpH5+Ck4egtDa0HuS2WnK7MXvd5CQlk3NMH+e66spbMR9qLgRESmJ3T/Cxo8BC9w8Ffwq9gjnv+w4zqx1iVgs8N+BrQjx06SY4j5U3IiIXExW6p+TYnZ6EOpcZW6eMjpxOo8nv7FPinnXVXF0qFvN5EQizqXiRkTkQgwDvnsEsv6A6s3g6mfNTlQmhmEw/putpJ7Op1FkMI/20qSY4n5U3IiIXMjmz2DX9/ZJMQe8C16+Zicqk682HGHBjuN4e1qYPKgVft4Ve1RlkXNRcSMicj4nD8GPT9qXr34GolqYGqesEtOyef67HQCMubYhzWpoUkxxTypuRETOxWaFOfdBfibU7gSdHjI7UZlYbQaPfrGF03mFtIutwr1dNSmmuC8VNyIi57L6TUhYBT5B9rujKvikmO8vP8DaQ2kEnpkU09NDoxCL+1JxIyLyd8nbYNGL9uXek6BKHVPjlNXOpAz+u2APAP+6qSm1qwWYnEikfKm4ERH5q8K8M5Ni5kOjPtDmDrMTlUleoZUxszeTb7XRs0l1BrbTpJji/lTciIj81aIXIWU7BIS7xaSYkxfuYVdyJtUCfZg4oKUmxZRKQcWNiEiRQyth1f/sy33/B0ER5uYpo98OnGDasgOAfVLMiOCKfRu7SEmpuBERAcjNgDmjAAPaDIHGfcxOVCaZuQU8+uUWDAMGtqtFr2ZRZkcSuWxU3IiIAPw0HtITICwWek80O02Z/fv7HRw5mUOtKv48e6MmxZTKxcvsACIiptv5PWz+FPukmO+Cb7DZiS6JYRgs25vKtGX7WbnvBBYLTB7YmmBNiimVjIobEancTqfAdw/blzs/ArEdzc1zCQqsNr7bcoxpyw6wKzkTAE8PC+Oua8SVcVVNTidy+am4EZHKyzDss31nn4DIFtDjKbMTlcrpvEJmrU3ggxUHSUrPBSDAx5Nbr6jNiKvqUKuKxrORyknFjYhUXhs/hj0/gacPDJhWYSbFPJ6Ry/SVh/jst8Nk5hYCEB7ky52d63BH+1hCA3QZSio3FTciUjmlHYSfz7TUXPMviHT9Trd7j2cybdkBvt18lAKrAUDdiEDu6VKX/m1qaoZvkTNU3IhI5ZKZDL+9A+s/hPzTEHsVdHjA7FTnZRgGaw+mMW3ZAX7dleJYf0WdKtzTtR7XNK6Oh+aJEilGxY2IVA5/7IZVb8DvX9inVgCIaAw3vwMerjcqhtVm8PP2ZN5ddoAtiacA+2DJ1zWN4p5udWlbu4q5AUVcmIobEXFfhgEJq2HlG7Dnxz/Xx7SHTg/b545yscImJ9/KVxsSeX/FQQ6fyAbAx8uDf8bX4q4udYkLDzQ5oYjrU3EjIu7HZoVdP8DK1+Ho+jMrLdD4BntRU7u9qfHOJS0rn49XH+Lj1YdJy7K3LIUFeDO0QyxDO9UhPKhidHYWcQUqbkTEfRTkwObPYfWbkGafUwlPX2g9GDo+COENzM13DodPZPH+8oN8uSGR3AIbADFV/bnrqrr8s10tAnz0z7RIaelvjYhUfNlpsPY9WDsNslPt6/zC4Iq7oP29EFTd1HjnsiXxFNOWHeDHbUnY7Dc+0aJmKPd2q0vvZlF4ebrW5TKRikTFjYhUXCcPweq3YOMnUJhjXxdaGzo+AG3uAN8gU+P9nc1msGRPCu8uPcBvB9Mc67s3iuCernXpWLcaFovufBIpKxU3IlLxHN1ov/Npx1ww7JdyiGppnz6haX/wdJ1/2pLSc1i57wSr9qWyYl8qKZl5AHh7Wujbqib3dK1Lo6iKOZeViKtynX8BREQuxDBg3y/2TsKHlv+5vt410PlhiOtmv1faZOk5Baw58Gcxs/+PrGKvB/l6cVv72tzZuQ7Rof4mpRRxbypuRMS1ZSbDnp/tA++l7LCv8/CC5rdAp4cgqrmp8fIKrWw4fJKV+1JZue8Evx855ehDA+BhgRa1wriqfjU61w+nbe0qGklYpJypuBER15KbAYdXwoElcGAp/LHzz9d8giF+GHS4D0JrmRLPZjPYkZTBin2prNyXyrpDaY67nIrUjQjkqvrhdK4fToe61Qj111xPIpeTihsRMVdhPhxZZy9mDi6FI+vBsP5lAwtEt4Jm/SH+TvAPu6zxDMMgIS2bFftSWbXvBKv2p3Iyu6DYNhHBvo5ipnP9arrcJGIyFTcicnnZbJCy/c+WmcMroSC7+DZV60Ld7vZ+NHFdIaDqZY2YejqPVftPsHJvKiv3p3LkZE6x14N8vehQtyqd64dzVf1w6lcP0l1OIi5ExY2IlL+Th88UM0vg4LI/x6IpEhBuL2bqdrMXNFViL2u8rLxC1h5KO1PMnGBnUkax1709LbSpXcXROtOyVijeGodGxGWpuBER58tOs19iKmqdOXmw+OvegVCn85+tM9WbXrY5njJzC9hxLINtxzLYfiyd7Ucz2PfHaax/7QUMNIkOcXQCvjKuqkYKFqlA9LdVRMqmMN/e6TdpCyT9DkfW2n/yl2LB4gm1rjjTOtMdasaDl0+5R0s9ncf2YxlsO5rOjjPFzKET2efctmaYP10a2FtmOtarprmcRCowFTciUnL5WZC8zV7IJJ8pZlJ2gq3g7G2rN/2zmIntBL7lN1CdYRgcS89l29F0th/LYPuZn8kZuefcvmaYP01rhNC8RijNaoTQrGYIUSF+6jcj4iZU3IjIuWWnQfLv9gImaYt9OXUvxVpkiviFQXRL+11N0a2hThcIjiyXWDabwcETWX9pjclg27F0TmWfXWBZLBBXLZBmNe1FTFExUyWw/FuNRMQ8phc3b7/9Nv/3f/9HUlISzZo147XXXqNLly7n3X7p0qWMHTuW7du3U6NGDR5//HFGjRp1GROLuBnDsA+UV1TAFF1eSk849/bB0fapDqJb/VnQhMaUy+jA+YU29qZkFmuN2ZGUQXa+9axtvTwsNIgMPlPEhNCsZihNokMI8jX9nzkRucxM/Vs/e/ZsRo8ezdtvv03nzp159913uf7669mxYwe1a9c+a/uDBw/Sp08f7r77bj799FNWrlzJ/fffT0REBP/4xz9M+ARSIRkGFObZbz+2We1jqvz1Z4nWFZ5ZtmHYCiksLCQ/P5+8ggLyrJBn8yTX5kGuzZMCvLDiSaHFi0K8KMTT/tPiRQFeFBieWC2eFBheFOBJoeGB1TCw2QwKbfafVsPAagOrzYbVaoBRiKUwFw9bHpaCPDxseXgZ+fhbCvGz5ONLIb6WfPyMfHwowIcCfMnHm3y8jQJ8jHy8bbmEZB0iLH0nvnknzvlV5YfEkh/RnIKIFtiiWmKLaoFXcBQFVht5hTbyCq3kZtnISz9FXqHVvq7A9udyoY28Amvxn2feZ9/OdsH3pZ7OJ99qOyuXn7cHTaJD/tIaE0rDqCB8vTTyr4iAxTCMc7QxXx7t27enbdu2TJ061bGuSZMm9O/fn4kTJ561/RNPPMG8efPYufPPEUtHjRrFli1bWL16dYmOmZGRQWhoKOnp6YSEhJT9Q5xhLSwk5eh+p+1PirNY87AUZGMpyMKjIBtLQY7juaUg276uMBtLfhaWwjPPC7L+sl02HgX21ywF2ViMs39hugqbYbEXPXhSiL04smHBh0J8zxQpnhbn/rW1Ghb2GTXZZtRhh60O22xx7DBiySTAqce5FMF+Xn8WMTXtP+PCA/HSrdgilUppfn+b1nKTn5/Phg0bePLJJ4ut79WrF6tWrTrne1avXk2vXr2Krbvuuuv44IMPKCgowNv77CHO8/LyyMvLczzPyMg4axtnOJl6jOjpV5bLvqV8WQ0LVjyw4UEhntjwwHrmYV9n/2k1PM6xneXMtp5Y8cDAAy8P8LUU4mOx4W2x4n2mvcbLKCxqw7EvG/Y2HI+/9WHxsBhniphzdNI9V36LN4UePlg9/Cj08KHQw4cCy18f3uTjQ77Fh3y8ycObfMOHPIs3xy0R7POsx36PWLJsPhRaDQpsNgqsNoKsBr5WgwKrjUKrjQKbfdkw7PMl+Xl74uvlga+XJ77eHn8ue3mceV70+tnb+HmX5H2ehAV4U6uKvzr6ikipmFbcpKamYrVaiYws3ukwMjKS5OTkc74nOTn5nNsXFhaSmppKdHT0We+ZOHEizz//vPOCX0Cuofljyks+3uTgSza+ZONHDr7kGH5k4UvOmefZ+JFt+DqWi/00/Mjmz9cKPP2xePvj5+ODv683AT6ef3l4OZb9zywH/mX5r9sEnln2P7P+kgZ2s1nBWgDWfPvlLmu+/bmt4Mz6AvulMC8/8PIt/tPTF08PDy7nxRibzcDDQ8WGiLgu03va/f1/ZIZhXPB/aefa/lzri4wfP56xY8c6nmdkZBATE3Opcc8rPKo2PJ968Q3lkvgBzruI6GI8PO0Pbz+zk5SIChsRcXWmFTfh4eF4enqe1UqTkpJyVutMkaioqHNu7+XlRbVq1c75Hl9fX3x9NRiXiIhIZWFajzwfHx/i4+NZuHBhsfULFy6kU6dO53xPx44dz9p+wYIFtGvX7pz9bURERKTyMfV2g7Fjx/L+++/z4YcfsnPnTsaMGUNCQoJj3Jrx48czdOhQx/ajRo3i8OHDjB07lp07d/Lhhx/ywQcf8Nhjj5n1EURERMTFmNrnZtCgQZw4cYIXXniBpKQkmjdvzvz584mNtc8InJSURELCnwOJxcXFMX/+fMaMGcNbb71FjRo1eOONNzTGjYiIiDiYOs6NGcprnBsREREpP6X5/a1RsERERMStqLgRERERt6LiRkRERNyKihsRERFxKypuRERExK2ouBERERG3ouJGRERE3IqKGxEREXErKm5ERETErZg6/YIZigZkzsjIMDmJiIiIlFTR7+2STKxQ6YqbzMxMAGJiYkxOIiIiIqWVmZlJaGjoBbepdHNL2Ww2jh07RnBwMBaLxan7zsjIICYmhsTERM1b5cJ0nioGnaeKQeepYnCH82QYBpmZmdSoUQMPjwv3qql0LTceHh7UqlWrXI8REhJSYf/wVCY6TxWDzlPFoPNUMVT083SxFpsi6lAsIiIibkXFjYiIiLgVFTdO5Ovry3PPPYevr6/ZUeQCdJ4qBp2nikHnqWKobOep0nUoFhEREfemlhsRERFxKypuRERExK2ouBERERG3ouJGRERE3IqKGyd5++23iYuLw8/Pj/j4eJYvX252pEpt4sSJXHHFFQQHB1O9enX69+/P7t27i21jGAYTJkygRo0a+Pv70717d7Zv325SYgH7ebNYLIwePdqxTufJNRw9epQ77riDatWqERAQQOvWrdmwYYPjdZ0n8xUWFvLMM88QFxeHv78/devW5YUXXsBmszm2qTTnyZAymzVrluHt7W289957xo4dO4xHHnnECAwMNA4fPmx2tErruuuuM6ZPn25s27bN2Lx5s3HDDTcYtWvXNk6fPu3YZtKkSUZwcLDx9ddfG1u3bjUGDRpkREdHGxkZGSYmr7zWrl1r1KlTx2jZsqXxyCOPONbrPJkvLS3NiI2NNYYPH2789ttvxsGDB41ffvnF2Ldvn2MbnSfzvfjii0a1atWM77//3jh48KDx5ZdfGkFBQcZrr73m2KaynCcVN05w5ZVXGqNGjSq2rnHjxsaTTz5pUiL5u5SUFAMwli5dahiGYdhsNiMqKsqYNGmSY5vc3FwjNDTUeOedd8yKWWllZmYaDRo0MBYuXGh069bNUdzoPLmGJ554wrjqqqvO+7rOk2u44YYbjBEjRhRbN2DAAOOOO+4wDKNynSddliqj/Px8NmzYQK9evYqt79WrF6tWrTIplfxdeno6AFWrVgXg4MGDJCcnFztvvr6+dOvWTefNBA888AA33HADPXv2LLZe58k1zJs3j3bt2vHPf/6T6tWr06ZNG9577z3H6zpPruGqq67i119/Zc+ePQBs2bKFFStW0KdPH6BynadKN3Gms6WmpmK1WomMjCy2PjIykuTkZJNSyV8ZhsHYsWO56qqraN68OYDj3JzrvB0+fPiyZ6zMZs2axcaNG1m3bt1Zr+k8uYYDBw4wdepUxo4dy1NPPcXatWt5+OGH8fX1ZejQoTpPLuKJJ54gPT2dxo0b4+npidVq5aWXXmLw4MFA5fr7pOLGSSwWS7HnhmGctU7M8eCDD/L777+zYsWKs17TeTNXYmIijzzyCAsWLMDPz++82+k8mctms9GuXTtefvllANq0acP27duZOnUqQ4cOdWyn82Su2bNn8+mnn/L555/TrFkzNm/ezOjRo6lRowbDhg1zbFcZzpMuS5VReHg4np6eZ7XSpKSknFUdy+X30EMPMW/ePBYvXkytWrUc66OiogB03ky2YcMGUlJSiI+Px8vLCy8vL5YuXcobb7yBl5eX41zoPJkrOjqapk2bFlvXpEkTEhISAP19chXjxo3jySef5NZbb6VFixYMGTKEMWPGMHHiRKBynScVN2Xk4+NDfHw8CxcuLLZ+4cKFdOrUyaRUYhgGDz74IN988w2LFi0iLi6u2OtxcXFERUUVO2/5+fksXbpU5+0yuuaaa9i6dSubN292PNq1a8ftt9/O5s2bqVu3rs6TC+jcufNZQyns2bOH2NhYQH+fXEV2djYeHsV/rXt6ejpuBa9U58nEzsxuo+hW8A8++MDYsWOHMXr0aCMwMNA4dOiQ2dEqrfvuu88IDQ01lixZYiQlJTke2dnZjm0mTZpkhIaGGt98842xdetWY/DgwW55S2RF89e7pQxD58kVrF271vDy8jJeeuklY+/evcZnn31mBAQEGJ9++qljG50n8w0bNsyoWbOm41bwb775xggPDzcef/xxxzaV5TypuHGSt956y4iNjTV8fHyMtm3bOm45FnMA53xMnz7dsY3NZjOee+45IyoqyvD19TW6du1qbN261bzQYhjG2cWNzpNr+O6774zmzZsbvr6+RuPGjY1p06YVe13nyXwZGRnGI488YtSuXdvw8/Mz6tatazz99NNGXl6eY5vKcp4shmEYZrYciYiIiDiT+tyIiIiIW1FxIyIiIm5FxY2IiIi4FRU3IiIi4lZU3IiIiIhbUXEjIiIibkXFjYiIiLgVFTcicpYlS5ZgsVg4depUmfYzfPhw+vfv75RMZujevTujR4++6HZdu3bl888/L/9Af3HLLbcwefLky3pMkYpCxY2IG3vnnXcIDg6msLDQse706dN4e3vTpUuXYtsuX74ci8XCnj176NSpE0lJSYSGhpZ7xnfffZdWrVoRGBhIWFgYbdq04ZVXXin34zrL999/T3JyMrfeeqtT9jdjxgw6dOhw0e3+9a9/8dJLL5GRkeGU44q4ExU3Im6sR48enD59mvXr1zvWLV++nKioKNatW0d2drZj/ZIlS6hRowYNGzbEx8eHqKgoLBZLueb74IMPGDt2LA8//DBbtmxh5cqVPP7445w+fbpcj+tMb7zxBnfeeedZExZeqnnz5tGvX7+LbteyZUvq1KnDZ5995pTjirgTFTcibqxRo0bUqFGDJUuWONYtWbKEfv36Ua9ePVatWlVsfY8ePRzLf70sNWPGDMLCwvj5559p0qQJQUFB9O7dm6SkJMf7rVYrY8eOJSwsjGrVqvH4449zsdldvvvuOwYOHMjIkSOpX78+zZo1Y/Dgwfz73/92bFN0aev555+nevXqhISEcO+995Kfn+/YxjAMXn31VerWrYu/vz+tWrXiq6++KnasHTt20KdPH4KCgoiMjGTIkCGkpqY6Xs/KymLo0KEEBQURHR3Nf//734t+v6mpqfzyyy/07du32HqLxcK7777LjTfeSEBAAE2aNGH16tXs27eP7t27ExgYSMeOHdm/f3+x9+Xm5rJgwQLH/t5++20aNGiAn58fkZGR3HLLLcW279u3LzNnzrxoTpHKRsWNiJvr3r07ixcvdjxfvHgx3bt3p1u3bo71+fn5rF692lHcnEt2djb/+c9/+OSTT1i2bBkJCQk89thjjtf/+9//8uGHH/LBBx+wYsUK0tLSmDNnzgWzRUVFsWbNGg4fPnzB7X799Vd27tzJ4sWLmTlzJnPmzOH55593vP7MM88wffp0pk6dyvbt2xkzZgx33HEHS5cuBSApKYlu3brRunVr1q9fz08//cTx48cZOHCgYx/jxo1j8eLFzJkzhwULFrBkyRI2bNhwwVwrVqxwFC9/9+9//5uhQ4eyefNmGjduzG233ca9997L+PHjHS1pDz744FmfMyoqimbNmrF+/XoefvhhXnjhBXbv3s1PP/1E165di21/5ZVXsnbtWvLy8i6YU6TSMXfeThEpb9OmTTMCAwONgoICIyMjw/Dy8jKOHz9uzJo1y+jUqZNhGIaxdOlSAzD2799vGIZhLF682ACMkydPGoZhGNOnTzcAY9++fY79vvXWW0ZkZKTjeXR0tDFp0iTH84KCAqNWrVpGv379zpvt2LFjRocOHQzAaNiwoTFs2DBj9uzZhtVqdWwzbNgwo2rVqkZWVpZj3dSpU42goCDDarUap0+fNvz8/IxVq1YV2/fIkSONwYMHG4ZhGM8++6zRq1evYq8nJiYagLF7924jMzPT8PHxMWbNmuV4/cSJE4a/v3+xGcr/bsqUKUbdunXPWg8YzzzzjOP56tWrDcD44IMPHOtmzpxp+Pn5FXvf3XffbYwdO9YwDMP4+uuvjZCQECMjI+O8x9+yZYsBGIcOHTrvNiKVkZd5ZZWIXA49evQgKyuLdevWcfLkSRo2bEj16tXp1q0bQ4YMISsriyVLllC7dm3q1q173v0EBARQr149x/Po6GhSUlIASE9PJykpiY4dOzpe9/Lyol27dhe8NBUdHc3q1avZtm0bS5cuZdWqVQwbNoz333+fn376ydGPpVWrVgQEBDje17FjR06fPk1iYiIpKSnk5uZy7bXXFtt3fn4+bdq0AWDDhg0sXryYoKCgszLs37+fnJwc8vPzi+WvWrUqjRo1Om92gJycHPz8/M75WsuWLR3LkZGRALRo0aLYutzcXDIyMggJCcEwDL777jtmzZoFwLXXXktsbCx169ald+/e9O7dm5tvvrnY9+Dv7w9QrO+UiICKGxE3V79+fWrVqsXixYs5efIk3bp1A+yXhOLi4li5ciWLFy/m6quvvuB+vL29iz23WCwX7VNTUs2bN6d58+Y88MADrFixgi5durB06dILXiYrymCz2QD44YcfqFmzZrHXfX19AbDZbNx0003nvAsrOjqavXv3XlLu8PBwTp48ec7X/vp9FXXMPte6ovxr164lPz+fq666CoDg4GA2btzIkiVLWLBgAf/617+YMGEC69atIywsDIC0tDQAIiIiLim/iLtSnxuRSqBHjx4sWbKEJUuW0L17d8f6bt268fPPP7NmzZqLFhIXEhoaSnR0NGvWrHGsKywsvGiflXNp2rQpYO/gW2TLli3k5OQ4nq9Zs4agoCBq1apF06ZN8fX1JSEhgfr16xd7xMTEANC2bVu2b99OnTp1ztomMDCQ+vXr4+3tXSz/yZMn2bNnzwWztmnThuTk5PMWOKUxd+5cbrjhBjw9PR3rvLy86NmzJ6+++iq///47hw4dYtGiRY7Xt23bRq1atQgPDy/z8UXciVpuRCqBHj168MADD1BQUOBouQF7cXPfffeRm5tbpuIG4JFHHmHSpEk0aNCAJk2aMHny5IsOAnjfffdRo0YNrr76amrVqkVSUhIvvvgiERERxS4R5efnM3LkSJ555hkOHz7Mc889x4MPPoiHhwfBwcE89thjjBkzBpvNxlVXXUVGRgarVq0iKCiIYcOG8cADD/Dee+8xePBgxo0bR3h4OPv27WPWrFm89957BAUFMXLkSMaNG0e1atWIjIzk6aefvujt3W3atCEiIoKVK1dy4403lun7mzdvXrFO0t9//z0HDhyga9euVKlShfnz52Oz2YpdKlu+fDm9evUq03FF3JGKG5FKoEePHuTk5NC4cWNH/w+wFzeZmZnUq1fP0cpxqR599FGSkpIYPnw4Hh4ejBgxgptvvpn09PTzvqdnz558+OGHTJ06lRMnThAeHk7Hjh359ddfqVatmmO7a665hgYNGtC1a1fy8vK49dZbmTBhguP1f//731SvXp2JEydy4MABwsLCaNu2LU899RQANWrUYOXKlTzxxBNcd9115OXlERsbS+/evR0FzP/93/9x+vRp+vbtS3BwMI8++ugFswN4enoyYsQIPvvsszIVN/v372ffvn1cd911jnVhYWF88803TJgwgdzcXBo0aMDMmTNp1qwZYL9tfM6cOfz888+XfFwRd2UxnHXRXESkHAwfPpxTp07x7bffmh3lnI4fP06zZs3YsGEDsbGxl7SPyZMn88svvzB//vwSv+ett95i7ty5LFiw4JKOKeLO1OdGRKQMIiMj+eCDD0hISLjkfdSqVYvx48eX6j3e3t7873//u+RjirgztdyIiEtz9ZYbEXE9Km5ERETEreiylIiIiLgVFTciIiLiVlTciIiIiFtRcSMiIiJuRcWNiIiIuBUVNyIiIuJWVNyIiIiIW1FxIyIiIm5FxY2IiIi4lf8H4laONs8dnzkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Original wind fragility curve data\n",
    "wind_speeds_50 = np.array([0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85])\n",
    "failure_probs_50 = np.array([0, 0, 0, 0, 0, 0.0021, 0.0027, 0.0017, 0.0032, 0.0124, 0.025, 0.0468, 0.1155, 0.219, 0.2995, 0.4177, 0.5325, 0.6033])\n",
    "\n",
    "# Scaled wind fragility curve data\n",
    "scaling_factor = 0.81\n",
    "wind_speeds_30 = wind_speeds_50 * scaling_factor\n",
    "failure_probs_30 = failure_probs_50 * scaling_factor\n",
    "\n",
    "# Plot both curves on the same graph\n",
    "plt.plot(wind_speeds_50, failure_probs_50, label='50 m/s')\n",
    "plt.plot(wind_speeds_30, failure_probs_30, label='30 m/s')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Wind Speed (m/s)')\n",
    "plt.ylabel('Failure Probability')\n",
    "plt.title('Wind Fragility Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc83d355-8c98-4ae6-97b1-3203a29a4fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten(xs):\n",
    "    for x in xs:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            yield from flatten(x)\n",
    "        else:\n",
    "            yield x\n",
    "\n",
    "def query_b(geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    This function builds an SQL query from the values passed to the retrieve() function.\n",
    "    Arguments:\n",
    "         *geoType* : Type of geometry (osm layer) to search for.\n",
    "         *keyCol* : A list of keys/columns that should be selected from the layer.\n",
    "         ***valConstraint* : A dictionary of constraints for the values. e.g. WHERE 'value'>20 or 'value'='constraint'\n",
    "    Returns:\n",
    "        *string: : a SQL query string.\n",
    "    \"\"\"\n",
    "    query = \"SELECT \" + \"osm_id\"\n",
    "    for a in keyCol: query+= \",\"+ a  \n",
    "    query += \" FROM \" + geoType + \" WHERE \"\n",
    "    # If there are values in the dictionary, add constraint clauses\n",
    "    if valConstraint: \n",
    "        for a in [*valConstraint]:\n",
    "            # For each value of the key, add the constraint\n",
    "            for b in valConstraint[a]: query += a + b\n",
    "        query+= \" AND \"\n",
    "    # Always ensures the first key/col provided is not Null.\n",
    "    query+= \"\"+str(keyCol[0]) +\" IS NOT NULL\" \n",
    "    return query \n",
    "\n",
    "\n",
    "def retrieve(osm_path,geoType,keyCol,**valConstraint):\n",
    "    \"\"\"\n",
    "    Function to extract specified geometry and keys/values from OpenStreetMap\n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.     \n",
    "        *geoType* : Type of Geometry to retrieve. e.g. lines, multipolygons, etc.\n",
    "        *keyCol* : These keys will be returned as columns in the dataframe.\n",
    "        ***valConstraint: A dictionary specifiying the value constraints.  \n",
    "        A key can have multiple values (as a list) for more than one constraint for key/value.  \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with all columns, geometries, and constraints specified.    \n",
    "    \"\"\"\n",
    "    driver=ogr.GetDriverByName('OSM')\n",
    "    data = driver.Open(osm_path)\n",
    "    query = query_b(geoType,keyCol,**valConstraint)\n",
    "    sql_lyr = data.ExecuteSQL(query)\n",
    "    features =[]\n",
    "    # cl = columns \n",
    "    cl = ['osm_id'] \n",
    "    for a in keyCol: cl.append(a)\n",
    "    if data is not None:\n",
    "        print('query is finished, lets start the loop')\n",
    "        for feature in tqdm(sql_lyr,desc='extract'):\n",
    "            #try:\n",
    "            if feature.GetField(keyCol[0]) is not None:\n",
    "                geom1 = (feature.geometry().ExportToWkt())\n",
    "                #print(geom1)\n",
    "                geom = from_wkt(feature.geometry().ExportToWkt()) \n",
    "                if geom is None:\n",
    "                    continue\n",
    "                # field will become a row in the dataframe.\n",
    "                field = []\n",
    "                for i in cl: field.append(feature.GetField(i))\n",
    "                field.append(geom)   \n",
    "                features.append(field)\n",
    "            #except:\n",
    "            #    print(\"WARNING: skipped OSM feature\")   \n",
    "    else:\n",
    "        print(\"ERROR: Nonetype error when requesting SQL. Check required.\")    \n",
    "    cl.append('geometry')                   \n",
    "    if len(features) > 0:\n",
    "        return pd.DataFrame(features,columns=cl)\n",
    "    else:\n",
    "        print(\"WARNING: No features or No Memory. returning empty GeoDataFrame\") \n",
    "        return pd.DataFrame(columns=['osm_id','geometry'])\n",
    "\n",
    "def power_polyline(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract all energy linestrings from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'lines',['power','voltage'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).rename(columns={'power': 'asset'})\n",
    "    \n",
    "    #print(df) #check infra keys\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# def power_polygon(osm_path): # check with joel, something was wrong here with extracting substations\n",
    "#     \"\"\"\n",
    "#     Function to extract energy polygons from OpenStreetMap  \n",
    "#     Arguments:\n",
    "#         *osm_path* : file path to the .osm.pbf file of the region \n",
    "#         for which we want to do the analysis.        \n",
    "#     Returns:\n",
    "#         *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "#     \"\"\"\n",
    "#     df = retrieve(osm_path,'multipolygons',['other_tags']) \n",
    "    \n",
    "#     df = df.loc[(df.other_tags.str.contains('power'))]   #keep rows containing power data         \n",
    "#     df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})     \n",
    "    \n",
    "#     df['asset'].loc[df['asset'].str.contains('\"power\"=>\"substation\"', case=False)]  = 'substation' #specify row\n",
    "#     df['asset'].loc[df['asset'].str.contains('\"power\"=>\"plant\"', case=False)] = 'plant' #specify row\n",
    "    \n",
    "#     df = df.loc[(df.asset == 'substation') | (df.asset == 'plant')]\n",
    "            \n",
    "#     return df.reset_index(drop=True) \n",
    "\n",
    "def electricity(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract building polygons from OpenStreetMap    \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with all unique building polygons.    \n",
    "    \"\"\"\n",
    "    df = retrieve(osm_path,'multipolygons',['power'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).rename(columns={'power': 'asset'})\n",
    "    \n",
    "    #df = df[df.asset!='generator']\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"substation\"', case=False)]  = 'substation' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"plant\"', case=False)] = 'plant' #specify row\n",
    "        \n",
    "    df = df.loc[(df.asset == 'substation') | (df.asset == 'plant')]\n",
    "    \n",
    "    print(df['asset'].unique())\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# def retrieve_poly_subs(osm_path, w_list, b_list):\n",
    "#     \"\"\"\n",
    "#     Function to extract electricity substation polygons from OpenStreetMap\n",
    "#     Arguments:\n",
    "#         *osm_path* : file path to the .osm.pbf file of the region\n",
    "#         for which we want to do the analysis.\n",
    "#         *w_list* :  white list of keywords to search in the other_tags columns\n",
    "#         *b_list* :  black list of keywords of rows that should not be selected\n",
    "#     Returns:\n",
    "#         *GeoDataFrame* : a geopandas GeoDataFrame with specified unique substation.\n",
    "#     \"\"\"\n",
    "#     df = retrieve(osm_path,'multipolygons',['other_tags'])\n",
    "#     df = df[df.other_tags.str.contains('substation', case=False, na=False)]\n",
    "#     #df = df.loc[(df.other_tags.str.contains('substation'))]\n",
    "#     df = df[~df.other_tags.str.contains('|'.join(b_list))]\n",
    "#     #df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})\n",
    "#     df['asset']  = 'substation' #specify row\n",
    "#     #df = df.loc[(df.asset == 'substation')] #specify row\n",
    "#     return df.reset_index(drop=True)\n",
    "\n",
    "def power_point(osm_path):\n",
    "    \"\"\"\n",
    "    Function to extract energy points from OpenStreetMap  \n",
    "    Arguments:\n",
    "        *osm_path* : file path to the .osm.pbf file of the region \n",
    "        for which we want to do the analysis.        \n",
    "    Returns:\n",
    "        *GeoDataFrame* : a geopandas GeoDataFrame with specified unique energy linestrings.\n",
    "    \"\"\"   \n",
    "    df = retrieve(osm_path,'points',['other_tags']) \n",
    "    df = df.loc[(df.other_tags.str.contains('power'))]  #keep rows containing power data       \n",
    "    df = df.reset_index(drop=True).rename(columns={'other_tags': 'asset'})     \n",
    "        \n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"tower\"', case=False)]  = 'power_tower' #specify row\n",
    "    df['asset'].loc[df['asset'].str.contains('\"power\"=>\"pole\"', case=False)] = 'power_pole' #specify row\n",
    "    #df['asset'].loc[df['asset'].str.contains('\"utility\"=>\"power\"', case=False)] = 'power_tower' #specify row\n",
    "    \n",
    "    df = df.loc[(df.asset == 'power_tower') | (df.asset == 'power_pole')]\n",
    "            \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bb2e396-29d1-420f-ba0a-785a56aed978",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|██████████████████████████████████████████████████████████████████████| 439/439 [00:02<00:00, 163.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|█████████████████████████████████████████████████████████████████████████| 33/33 [00:08<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plant' 'substation']\n",
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|█████████████████████████████████████████████████████████████████| 45489/45489 [00:05<00:00, 9014.30it/s]\n"
     ]
    }
   ],
   "source": [
    "#new\n",
    "osm_power_infra = extract_osm_infrastructure('LAO',osm_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b7d8f5e-5298-4367-b57b-0d2480fbbe9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|██████████████████████████████████████████████████████████████████████| 439/439 [00:02<00:00, 154.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|█████████████████████████████████████████████████████████████████████████| 33/33 [00:08<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   osm_id       asset                                           geometry\n",
      "0    None       plant  MULTIPOLYGON (((102.119 20.808, 102.119 20.808...\n",
      "1    None       plant  MULTIPOLYGON (((101.816 19.244, 101.817 19.244...\n",
      "2    None   generator  MULTIPOLYGON (((102.736 18.332, 102.736 18.332...\n",
      "3    None       plant  MULTIPOLYGON (((102.776 18.751, 102.776 18.751...\n",
      "4    None       plant  MULTIPOLYGON (((101.283 19.69, 101.282 19.689,...\n",
      "5    None  substation  MULTIPOLYGON (((105.997 16.96, 105.998 16.96, ...\n",
      "6    None       plant  MULTIPOLYGON (((102.547 18.531, 102.547 18.531...\n",
      "7    None       plant  MULTIPOLYGON (((104.637 18.294, 104.637 18.294...\n",
      "8    None       plant  MULTIPOLYGON (((104.539 18.21, 104.539 18.209,...\n",
      "9    None       plant  MULTIPOLYGON (((102.346 21.41, 102.345 21.411,...\n",
      "10   None       plant  MULTIPOLYGON (((102.118 18.792, 102.118 18.791...\n",
      "11   None       plant  MULTIPOLYGON (((101.407 21.033, 101.408 21.033...\n",
      "12   None   generator  MULTIPOLYGON (((107.406 15.375, 107.406 15.376...\n",
      "13   None       plant  MULTIPOLYGON (((101.242 21.198, 101.242 21.198...\n",
      "14   None       plant  MULTIPOLYGON (((102.938 18.535, 102.939 18.535...\n",
      "15   None       plant  MULTIPOLYGON (((103.289 19.137, 103.29 19.137,...\n",
      "16   None       plant  MULTIPOLYGON (((103.356 19.187, 103.356 19.187...\n",
      "17   None       plant  MULTIPOLYGON (((106.731 14.842, 106.732 14.841...\n",
      "18   None       plant  MULTIPOLYGON (((102.222 19.749, 102.222 19.748...\n",
      "19   None       plant  MULTIPOLYGON (((105.153 17.68, 105.152 17.68, ...\n",
      "20   None       plant  MULTIPOLYGON (((102.545 18.532, 102.546 18.532...\n",
      "21   None       plant  MULTIPOLYGON (((103.352 19.3, 103.352 19.3, 10...\n",
      "22   None       plant  MULTIPOLYGON (((102.368 19.685, 102.368 19.684...\n",
      "23   None       plant  MULTIPOLYGON (((105.956 13.943, 105.955 13.944...\n",
      "24   None   generator  MULTIPOLYGON (((102.119 20.814, 102.119 20.814...\n",
      "25   None       plant  MULTIPOLYGON (((103.526 18.646, 103.526 18.646...\n",
      "26   None       plant  MULTIPOLYGON (((103.521 19.066, 103.522 19.066...\n",
      "27   None       plant  MULTIPOLYGON (((103.309 19.138, 103.309 19.138...\n",
      "28   None       plant  MULTIPOLYGON (((103.348 19.158, 103.348 19.158...\n",
      "29   None       plant  MULTIPOLYGON (((100.899 20.247, 100.9 20.247, ...\n",
      "30   None       plant  MULTIPOLYGON (((103.173 18.443, 103.173 18.443...\n",
      "31   None       plant  MULTIPOLYGON (((106.222 15.576, 106.223 15.576...\n",
      "32   None       plant  MULTIPOLYGON (((106.296 15.371, 106.296 15.37,...\n",
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|█████████████████████████████████████████████████████████████████| 45489/45489 [00:04<00:00, 9472.30it/s]\n"
     ]
    }
   ],
   "source": [
    "#old\n",
    "osm_power_infra = extract_osm_infrastructure('LAO',osm_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36fe9f2e-0be0-47fd-83ca-67f3a272279d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reproject(df_ds, current_crs=\"epsg:4326\", approximate_crs=\"epsg:3857\"):\n",
    "\n",
    "    # Extract the input geometries as a numpy array of coordinates\n",
    "    geometries = df_ds['geometry']\n",
    "    coords = pygeos.get_coordinates(geometries)\n",
    "\n",
    "    # Transform the coordinates using pyproj\n",
    "    transformer = pyproj.Transformer.from_crs(current_crs, approximate_crs, always_xy=True)\n",
    "    new_coords = transformer.transform(coords[:, 0], coords[:, 1])\n",
    "\n",
    "    # Create a new GeoSeries with the reprojected coordinates\n",
    "    return pygeos.set_coordinates(geometries.copy(), np.array(new_coords).T)\n",
    "\n",
    "def buffer_assets(assets, buffer_size=100):\n",
    "    \"\"\"\n",
    "    Create a buffer of a specified size around the geometries in a GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        assets (GeoDataFrame): A GeoDataFrame containing geometries to be buffered.\n",
    "        buffer_size (int, optional): The distance in the units of the GeoDataFrame's CRS to buffer the geometries.\n",
    "            Defaults to 100.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A new GeoDataFrame with an additional column named 'buffered' containing the buffered\n",
    "            geometries.\n",
    "    \"\"\"\n",
    "    # Create a buffer of the specified size around the geometries\n",
    "    assets['buffered'] = pygeos.buffer(assets.geometry.values, buffer_size)\n",
    "    \n",
    "    return assets\n",
    "\n",
    "def load_curves_maxdam(vul_curve_path,hazard_type):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        data_path ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "\n",
    "    if hazard_type == 'tc':\n",
    "        sheet_name = 'wind_curves'\n",
    "    \n",
    "    elif hazard_type == 'fl':\n",
    "        sheet_name = 'flooding_curves'\n",
    "    \n",
    "    # load curves and maximum damages as separate inputs\n",
    "    curves = pd.read_excel(vul_curve_path,sheet_name=sheet_name,skiprows=10,index_col=[0])\n",
    "    \n",
    "    maxdam = pd.read_excel(vul_curve_path,sheet_name=sheet_name,index_col=[0],header=[0,1]).iloc[:7]\n",
    "    maxdam = maxdam.rename({'substation_point':'substation'},level=0,axis=1)\n",
    "\n",
    "    curves.columns = maxdam.columns\n",
    "        \n",
    "    #transpose maxdam so its easier work with the dataframe\n",
    "    maxdam = maxdam.T\n",
    "\n",
    "    #interpolate the curves to fill missing values\n",
    "    curves = curves.interpolate()\n",
    "       \n",
    "    return curves,maxdam\n",
    "\n",
    "\n",
    "def overlay_hazard_assets(df_ds, assets):\n",
    "    \"\"\"\n",
    "    Overlay a set of assets with a hazard dataset and return the subset of assets that intersect with\n",
    "    one or more hazard polygons or lines.\n",
    "    \n",
    "    Args:\n",
    "        df_ds (GeoDataFrame): A GeoDataFrame containing the hazard dataset.\n",
    "        assets (GeoDataFrame): A GeoDataFrame containing the assets to be overlaid with the hazard dataset.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: A numpy array of integers representing the indices of the hazard geometries that intersect with\n",
    "            the assets. If the assets have a 'buffered' column, the buffered geometries are used for the overlay.\n",
    "    \"\"\"\n",
    "    hazard_tree = pygeos.STRtree(df_ds.geometry.values)\n",
    "    if (pygeos.get_type_id(assets.iloc[0].geometry) == 3) | (pygeos.get_type_id(assets.iloc[0].geometry) == 6):\n",
    "        return  hazard_tree.query_bulk(assets.geometry,predicate='intersects')    \n",
    "    else:\n",
    "        return  hazard_tree.query_bulk(assets.buffered,predicate='intersects')\n",
    "    \n",
    "    \n",
    "def get_damage_per_asset_per_rp(asset,df_ds,assets,curves,maxdam,return_period,country):\n",
    "    \"\"\"\n",
    "    Calculates the damage per asset per return period based on asset type, hazard curves and maximum damage\n",
    "\n",
    "    Args:\n",
    "        asset (tuple): Tuple with two dictionaries, containing the asset index and the hazard point index of the asset\n",
    "        df_ds (pandas.DataFrame): A pandas DataFrame containing hazard points with a 'geometry' column\n",
    "        assets (geopandas.GeoDataFrame): A GeoDataFrame containing asset geometries and asset type information\n",
    "        curves (dict): A dictionary with the asset types as keys and their corresponding hazard curves as values\n",
    "        maxdam (pandas.DataFrame): A pandas DataFrame containing the maximum damage for each asset type\n",
    "        return_period (str): The return period for which the damage should be calculated\n",
    "        country (str): The country for which the damage should be calculated\n",
    "\n",
    "    Returns:\n",
    "        list or tuple: Depending on the input, the function either returns a list of tuples with the asset index, the curve name and the calculated damage, or a tuple with None, None, None if no hazard points are found\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = df_ds.iloc[asset[1]['hazard_point'].values].reset_index()\n",
    "    get_hazard_points = get_hazard_points.loc[pygeos.intersects(get_hazard_points.geometry.values,assets.iloc[asset[0]].geometry)]\n",
    "\n",
    "    asset_type = assets.iloc[asset[0]].asset\n",
    "    asset_geom = assets.iloc[asset[0]].geometry\n",
    "\n",
    "    if asset_type in ['plant','substation','generator']:\n",
    "        #if plant,substation are points, do not calculate the area\n",
    "        if pygeos.area(asset_geom) == 0:\n",
    "            maxdam_asset = maxdam.loc[asset_type].MaxDam\n",
    "            lowerdam_asset = maxdam.loc[asset_type].LowerDam\n",
    "            upperdam_asset = maxdam.loc[asset_type].UpperDam\n",
    "        else:\n",
    "            maxdam_asset = maxdam.loc[asset_type].MaxDam/pygeos.area(asset_geom)\n",
    "            lowerdam_asset = maxdam.loc[asset_type].LowerDam/pygeos.area(asset_geom)\n",
    "            upperdam_asset = maxdam.loc[asset_type].UpperDam/pygeos.area(asset_geom)\n",
    "    else:\n",
    "        maxdam_asset = maxdam.loc[asset_type].MaxDam\n",
    "        lowerdam_asset = maxdam.loc[asset_type].LowerDam\n",
    "        upperdam_asset = maxdam.loc[asset_type].UpperDam\n",
    "\n",
    "    hazard_intensity = curves[asset_type].index.values\n",
    "    \n",
    "    if isinstance(curves[asset_type],pd.core.series.Series):\n",
    "        fragility_values = curves[asset_type].values.flatten()\n",
    "        only_one = True\n",
    "        curve_name = curves[asset_type].name\n",
    "    elif len(curves[asset_type].columns) == 1:\n",
    "        fragility_values = curves[asset_type].values.flatten()      \n",
    "        only_one = True   \n",
    "        curve_name = curves[asset_type].columns[0]\n",
    "    else:\n",
    "        fragility_values = curves[asset_type].values#.T[0]\n",
    "        maxdam_asset = maxdam_asset.values#[0]\n",
    "        only_one = False\n",
    "\n",
    "    if len(get_hazard_points) == 0:\n",
    "        if only_one:\n",
    "            return [return_period,asset[0],curve_name,0,0,0]\n",
    "        else:\n",
    "            return [return_period,asset[0],curves[asset_type].columns[0],0,0,0]\n",
    "            \n",
    "    else:\n",
    "        if only_one:    \n",
    "            # run the calculation as normal when the asset just has a single curve\n",
    "            if pygeos.get_type_id(asset_geom) == 1:            \n",
    "                get_hazard_points['overlay_meters'] = pygeos.length(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "                return [return_period,asset[0],curve_name,np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,\n",
    "                                                             fragility_values))*get_hazard_points.overlay_meters*maxdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,\n",
    "                                                             fragility_values))*get_hazard_points.overlay_meters*lowerdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,hazard_intensity,\n",
    "                                                             fragility_values))*get_hazard_points.overlay_meters*upperdam_asset)]\n",
    "\n",
    "            elif (pygeos.get_type_id(asset_geom) == 3) | (pygeos.get_type_id(asset_geom) == 6) :\n",
    "                get_hazard_points['overlay_m2'] = pygeos.area(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "                return [return_period,asset[0],curve_name,get_hazard_points.apply(lambda x: np.interp(x[return_period],hazard_intensity, \n",
    "                                                                  fragility_values)*maxdam_asset*x.overlay_m2,axis=1).sum(),\n",
    "                                                          get_hazard_points.apply(lambda x: np.interp(x[return_period],hazard_intensity, \n",
    "                                                                  fragility_values)*lowerdam_asset*x.overlay_m2,axis=1).sum(),\n",
    "                                                          get_hazard_points.apply(lambda x: np.interp(x[return_period],hazard_intensity, \n",
    "                                                                  fragility_values)*upperdam_asset*x.overlay_m2,axis=1).sum()]  \n",
    "\n",
    "            else:\n",
    "                return [return_period,asset[0],curve_name,np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                             hazard_intensity,fragility_values))*maxdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                             hazard_intensity,fragility_values))*lowerdam_asset),\n",
    "                                                          np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                             hazard_intensity,fragility_values))*upperdam_asset)]\n",
    "        else:\n",
    "            # run the calculation when the asset has multiple curves\n",
    "            if pygeos.get_type_id(asset_geom) == 1:            \n",
    "                get_hazard_points['overlay_meters'] = pygeos.length(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            elif (pygeos.get_type_id(asset_geom) == 3) | (pygeos.get_type_id(asset_geom) == 6) :\n",
    "                get_hazard_points['overlay_m2'] = pygeos.area(pygeos.intersection(get_hazard_points.geometry.values,asset_geom))\n",
    "            \n",
    "            collect_all = []\n",
    "            for iter_,curve_ids in enumerate(curves[asset_type].columns):\n",
    "                if pygeos.get_type_id(asset_geom) == 1:\n",
    "                    collect_all.append([return_period,asset[0],curves[asset_type].columns[iter_],\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*get_hazard_points.overlay_meters*maxdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*get_hazard_points.overlay_meters*lowerdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*get_hazard_points.overlay_meters*upperdam_asset[iter_])])\n",
    "                                   \n",
    "                elif (pygeos.get_type_id(asset_geom) == 3) | (pygeos.get_type_id(asset_geom) == 6) :\n",
    "                    collect_all.append([return_period,asset[0],curves[asset_type].columns[iter_],\n",
    "                                        get_hazard_points.apply(lambda x: np.interp(x[return_period], hazard_intensity,\n",
    "                                                                                    fragility_values.T[iter_])*maxdam_asset[iter_]*x.overlay_m2,axis=1).sum(),\n",
    "                                        get_hazard_points.apply(lambda x: np.interp(x[return_period], hazard_intensity,\n",
    "                                                                                    fragility_values.T[iter_])*lowerdam_asset[iter_]*x.overlay_m2,axis=1).sum(),\n",
    "                                        get_hazard_points.apply(lambda x: np.interp(x[return_period], hazard_intensity,\n",
    "                                                                                    fragility_values.T[iter_])*upperdam_asset[iter_]*x.overlay_m2,axis=1).sum()])\n",
    "\n",
    "                else:\n",
    "                    collect_all.append([return_period,asset[0],curves[asset_type].columns[iter_],\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*maxdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*lowerdam_asset[iter_]),\n",
    "                                        np.sum((np.interp(get_hazard_points[return_period].values,\n",
    "                                                          hazard_intensity,fragility_values.T[iter_]))*upperdam_asset[iter_])])\n",
    "            return collect_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1bf0e992-b4d1-46b0-a952-488ac8c5b2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Infrastructure type</th>\n",
       "      <th colspan=\"3\" halign=\"left\">plant</th>\n",
       "      <th colspan=\"3\" halign=\"left\">substation</th>\n",
       "      <th>power_tower</th>\n",
       "      <th colspan=\"3\" halign=\"left\">power_pole</th>\n",
       "      <th>line</th>\n",
       "      <th>minor_line</th>\n",
       "      <th>cable</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Code</th>\n",
       "      <th>F1_1_1</th>\n",
       "      <th>F1_1_2</th>\n",
       "      <th>F1_1_3</th>\n",
       "      <th>F2_1_1</th>\n",
       "      <th>F2_1_2</th>\n",
       "      <th>F2_1_3</th>\n",
       "      <th>F3_1</th>\n",
       "      <th>F4_1_1</th>\n",
       "      <th>F4_1_2</th>\n",
       "      <th>F4_1_3</th>\n",
       "      <th>F5_1</th>\n",
       "      <th>F5_2</th>\n",
       "      <th>F5_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depth (cm)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290.0</th>\n",
       "      <td>0.276</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295.0</th>\n",
       "      <td>0.284</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300.0</th>\n",
       "      <td>0.292</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305.0</th>\n",
       "      <td>0.300</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305.0</th>\n",
       "      <td>0.300</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Infrastructure type  plant               substation               power_tower  \\\n",
       "Code                F1_1_1 F1_1_2 F1_1_3     F2_1_1 F2_1_2 F2_1_3        F3_1   \n",
       "Depth (cm)                                                                      \n",
       "0.0                  0.000  0.000  0.000      0.000  0.000  0.000        0.00   \n",
       "5.0                  0.004  0.004  0.004      0.003  0.003  0.003        0.00   \n",
       "10.0                 0.008  0.008  0.008      0.007  0.007  0.007        0.00   \n",
       "15.0                 0.012  0.012  0.012      0.010  0.010  0.010        0.00   \n",
       "20.0                 0.016  0.016  0.016      0.013  0.013  0.013        0.00   \n",
       "...                    ...    ...    ...        ...    ...    ...         ...   \n",
       "290.0                0.276  0.276  0.276      0.150  0.150  0.150        0.02   \n",
       "295.0                0.284  0.284  0.284      0.154  0.154  0.154        0.02   \n",
       "300.0                0.292  0.292  0.292      0.157  0.157  0.157        0.02   \n",
       "305.0                0.300  0.300  0.300      0.150  0.150  0.150        0.02   \n",
       "305.0                0.300  0.300  0.300      0.150  0.150  0.150        0.02   \n",
       "\n",
       "Infrastructure type power_pole                line minor_line cable  \n",
       "Code                    F4_1_1 F4_1_2 F4_1_3  F5_1       F5_2  F5_3  \n",
       "Depth (cm)                                                           \n",
       "0.0                       0.00   0.00   0.00  0.00       0.00  0.00  \n",
       "5.0                       0.00   0.00   0.00  0.00       0.00  0.00  \n",
       "10.0                      0.00   0.00   0.00  0.00       0.00  0.00  \n",
       "15.0                      0.00   0.00   0.00  0.00       0.00  0.00  \n",
       "20.0                      0.00   0.00   0.00  0.00       0.00  0.00  \n",
       "...                        ...    ...    ...   ...        ...   ...  \n",
       "290.0                     0.02   0.02   0.02  0.02       0.02  0.02  \n",
       "295.0                     0.02   0.02   0.02  0.02       0.02  0.02  \n",
       "300.0                     0.02   0.02   0.02  0.02       0.02  0.02  \n",
       "305.0                     0.02   0.02   0.02  0.02       0.02  0.02  \n",
       "305.0                     0.02   0.02   0.02  0.02       0.02  0.02  \n",
       "\n",
       "[71 rows x 13 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_curves_maxdam(vul_curve_path,'fl')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9896d9f-4592-4552-8876-c1340f3e09f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_storm_data(climate_model,basin,bbox):\n",
    "    \"\"\"\n",
    "    Load storm data from a NetCDF file and process it to return a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - climate_model (str): name of the climate model\n",
    "    - basin (str): name of the basin\n",
    "    - bbox (tuple): bounding box coordinates in the format (minx, miny, maxx, maxy)\n",
    "    - ne_crs (str): CRS string of the North-East projection\n",
    "\n",
    "    Returns:\n",
    "    - df_ds (pd.DataFrame): pandas DataFrame with interpolated wind speeds for different return periods and geometry column\n",
    "    \"\"\"\n",
    "    # set paths\n",
    "    # data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "\n",
    "    filename = os.path.join(tc_path, f'STORM_FIXED_RETURN_PERIODS{climate_model}_{basin}.nc')\n",
    "    \n",
    "    # load data from NetCDF file\n",
    "    with xr.open_dataset(filename) as ds:\n",
    "        \n",
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        ds = ds.rio.clip_box(minx=bbox[0], miny=bbox[1], maxx=bbox[2], maxy=bbox[3])\n",
    "        \n",
    "        #convert 10-min sustained wind speed to 3-s gust wind speed\n",
    "        ds['mean_3s'] = ds['mean']/0.88*1.11\n",
    "\n",
    "        # get the mean values\n",
    "        df_ds = ds['mean_3s'].to_dataframe().unstack(level=2).reset_index()\n",
    "\n",
    "        # create geometry values and drop lat lon columns\n",
    "        df_ds['geometry'] = [pygeos.points(x) for x in list(zip(df_ds['lon'], df_ds['lat']))]\n",
    "        df_ds = df_ds.drop(['lat', 'lon'], axis=1, level=0)\n",
    "        \n",
    "        # interpolate wind speeds of 1,2,5,25,and 250-yr return period\n",
    "        ## rename columns to return periods (must be integer for interpolating)\n",
    "        df_ds_geometry = pd.DataFrame()\n",
    "        df_ds_geometry['geometry'] = df_ds['geometry']\n",
    "        df_ds = df_ds.drop(['geometry'], axis=1, level=0)\n",
    "        df_ds = df_ds['mean_3s']\n",
    "        df_ds.columns = [int(x) for x in ds['mean_3s']['rp']]\n",
    "        df_ds[1] = np.nan\n",
    "        df_ds[2] = np.nan\n",
    "        df_ds[5] = np.nan\n",
    "        df_ds[25] = np.nan\n",
    "        df_ds[250] = np.nan\n",
    "        df_ds = df_ds.reindex(sorted(df_ds.columns), axis=1)\n",
    "        df_ds = df_ds.interpolate(method='pchip', axis=1, limit_direction='both')\n",
    "        df_ds['geometry'] = df_ds_geometry['geometry']\n",
    "        df_ds = df_ds[[1, 2, 5, 10, 25, 50, 100, 250, 500, 1000, 'geometry']]\n",
    "        \n",
    "        # rename columns to return periods\n",
    "        df_ds.columns = ['1_{}{}'.format(int(x), climate_model) for x in [1, 2, 5, 10, 25, 50, 100, 250, 500, 1000]] +['geometry']\n",
    "        df_ds['geometry'] = pygeos.buffer(df_ds.geometry, radius=0.1/2, cap_style='square').values\n",
    "        \n",
    "        # reproject the geometry column to the specified CRS\n",
    "        df_ds['geometry'] = reproject(df_ds)\n",
    "            \n",
    "        # drop all non values to reduce size\n",
    "        #df_ds = df_ds.loc[~df_ds['1_10000{}'.format(climate_model)].isna()].reset_index(drop=True)\n",
    "        df_ds = df_ds.fillna(0)\n",
    "\n",
    "    return df_ds\n",
    "\n",
    "def open_storm_data(country_code):\n",
    "    \"\"\"\n",
    "    This function loads STORM data for a given country code, clips it based on the country geometry,\n",
    "    and combines data from different basins and climate models.\n",
    "\n",
    "    Args:\n",
    "    - country_code (str): a 3-letter ISO code of the country of interest\n",
    "\n",
    "    Returns:\n",
    "    - df_ds (dict): a dictionary containing STORM data for different climate models, organized by basin\n",
    "    \"\"\"\n",
    "    # set paths\n",
    "    # data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "    \n",
    "    # list of available climate models\n",
    "    climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "\n",
    "    # dictionary of basins for each country\n",
    "    country_basin = {\n",
    "        \"BRN\": [\"WP\"],\n",
    "        \"KHM\": [\"WP\"],\n",
    "        \"CHN\": [\"WP\", \"NI\"],\n",
    "        \"IDN\": [\"SI\", \"SP\", \"NI\", \"WP\"],\n",
    "        \"JPN\": [\"WP\"],\n",
    "        \"LAO\": [\"WP\"],\n",
    "        \"MYS\": [\"WP\", \"NI\"],\n",
    "        \"MNG\": [\"WP\", \"NI\"],\n",
    "        \"MMR\": [\"NI\", \"WP\"],\n",
    "        \"PRK\": [\"WP\"],\n",
    "        \"PHL\": [\"WP\"],\n",
    "        \"SGP\": [\"WP\"],\n",
    "        \"KOR\": [\"WP\"],\n",
    "        \"TWN\": [\"WP\"],\n",
    "        \"THA\": [\"WP\", \"NI\"],\n",
    "        \"VNM\": [\"WP\"]\n",
    "    }\n",
    "\n",
    "    # load country geometry file and create geometry to clip\n",
    "    ne_countries = gpd.read_file(os.path.join(data_path,'..',\"natural_earth\",\"ne_10m_admin_0_countries.shp\"))\n",
    "    bbox = ne_countries.loc[ne_countries['ISO_A3']==country_code].geometry.buffer(1).values[0].bounds\n",
    "    # ne_countries = gpd.read_file('C:/Users/mye500/OneDrive - Vrije Universiteit Amsterdam/01_Research-Projects/01_risk_assessment/base_map/base_map_adm_0.gpkg')\n",
    "    # bbox = ne_countries.loc[ne_countries['GID_0']==country_code].geometry.buffer(1).values[0].bounds\n",
    "\n",
    "    df_ds = {}\n",
    "    for climate_model in climate_models:\n",
    "        concat_prep = []\n",
    "\n",
    "        #combine STORM data from different basins\n",
    "        if \"WP\" in country_basin[country_code]:\n",
    "            WP = load_storm_data(climate_model,'WP',bbox)\n",
    "            concat_prep.append(WP)\n",
    "        if \"SP\" in country_basin[country_code]:\n",
    "            SP = load_storm_data(climate_model,'SP',bbox)\n",
    "            concat_prep.append(SP)\n",
    "        if \"NI\" in country_basin[country_code]:            \n",
    "            NI = load_storm_data(climate_model,'NI',bbox)\n",
    "            concat_prep.append(NI)            \n",
    "        if \"SI\" in country_basin[country_code]:       \n",
    "            SI = load_storm_data(climate_model,'SI',bbox)\n",
    "            concat_prep.append(SI)            \n",
    "                   \n",
    "        df_ds_cl = pd.concat(concat_prep, keys=country_basin[country_code])\n",
    "        df_ds_cl = df_ds_cl.reset_index(drop=True)\n",
    "        df_ds[climate_model] = df_ds_cl\n",
    "\n",
    "    return df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67f24e70-2570-4b51-b5a2-d238b74abba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "twn_wind = open_storm_data('TWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e9b1fa7-e368-4f50-a046-0ee32690bad2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.77332750074388"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ne\n",
    "twn_wind['_CMCC-CM2-VHR4']['1_1_CMCC-CM2-VHR4'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6b43b71-49ee-4aee-aca7-db18ecbd8b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.77332750074388"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gdam\n",
    "twn_wind('TWN')['_CMCC-CM2-VHR4']#['1_1_CMCC-CM2-VHR4'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c7ff6dd-a3f2-4e2a-becd-e5cffa38edde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clip_flood_data(country_code):\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "\n",
    "    # load country geometry file and create geometry to clip\n",
    "    ne_countries = gpd.read_file(ne_path)\n",
    "    geometry = ne_countries.loc[ne_countries['ISO_A3']==country_code].geometry.values[0]\n",
    "    geoms = [mapping(geometry)]\n",
    "    \n",
    "    #climate_model: historical, rcp4p5, rcp8p5; time_period: hist, 2030, 2050, 2080\n",
    "    rps = ['0001','0002','0005','0010','0025','0050','0100','0250','0500','1000']\n",
    "    climate_models = ['historical','rcp8p5']\n",
    "    \n",
    "    for rp in rps:\n",
    "        #global input_file\n",
    "        for climate_model in climate_models:\n",
    "            if climate_model=='historical':\n",
    "                input_file = os.path.join(fl_path,'global',\n",
    "                                          'inuncoast_{}_nosub_hist_rp{}_0.tif'.format(climate_model,rp)) \n",
    " \n",
    "            elif climate_model=='rcp8p5':\n",
    "                input_file = os.path.join(fl_path,'global',\n",
    "                                          'inuncoast_{}_nosub_2030_rp{}_0.tif'.format(climate_model,rp))\n",
    "\n",
    "            # load raster file and save clipped version\n",
    "            with rasterio.open(input_file) as src:\n",
    "                out_image, out_transform = mask(src, geoms, crop=True)\n",
    "                out_meta = src.meta\n",
    "\n",
    "                out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "\n",
    "                if 'scistor' in fl_path:\n",
    "                    file_path = os.path.join(fl_path,'country','_'.join([country_code]+input_file.split('_')[6:]))\n",
    "                else:\n",
    "                    file_path = os.path.join(fl_path,'country','_'.join([country_code]+input_file.split('_')[3:]))\n",
    "\n",
    "                with rasterio.open(file_path, \"w\", **out_meta) as dest:\n",
    "                    dest.write(out_image)\n",
    "\n",
    "def load_flood_data(country_code,climate_model):\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "     \n",
    "    rps = ['0001','0002','0005','0010','0025','0050','0100','0250','0500','1000']\n",
    "    collect_df_ds = []\n",
    "    \n",
    "    if climate_model=='historical':\n",
    "        print('Loading historical coastal flood data ...')\n",
    "        for rp in rps:\n",
    "            #for file in files:\n",
    "            file_path = os.path.join(fl_path,'country','{}_{}_nosub_hist_rp{}_0.tif'.format(country_code,climate_model,rp))\n",
    "            with xr.open_dataset(file_path) as ds: #, engine=\"rasterio\"\n",
    "                df_ds = ds.to_dataframe().reset_index()\n",
    "                df_ds['geometry'] = pygeos.points(df_ds.x,y=df_ds.y)\n",
    "                df_ds = df_ds.rename(columns={'band_data': 'rp'+rp}) #rename to return period\n",
    "                \n",
    "                # move from meters to centimeters\n",
    "                df_ds['rp'+rp] = (df_ds['rp'+rp]*100)         \n",
    "                df_ds = df_ds.drop(['band','x', 'y','spatial_ref'], axis=1)\n",
    "                df_ds = df_ds.dropna()\n",
    "                df_ds = df_ds.reset_index(drop=True)\n",
    "                df_ds.geometry= pygeos.buffer(df_ds.geometry,radius=0.0089932/2,cap_style='square').values  # the original value here is 0.00833???\n",
    "                df_ds['geometry'] = reproject(df_ds)\n",
    "                collect_df_ds.append(df_ds)\n",
    "\n",
    "        df_all = collect_df_ds[0].merge(collect_df_ds[1]).merge(collect_df_ds[2]).merge(collect_df_ds[3]).merge(collect_df_ds[4])\\\n",
    "                 .merge(collect_df_ds[5]).merge(collect_df_ds[6]).merge(collect_df_ds[7]).merge(collect_df_ds[8]).merge(collect_df_ds[9])\n",
    "        df_all = df_all.loc[df_all['rp1000']>0].reset_index(drop=True)\n",
    "\n",
    "    elif climate_model=='rcp8p5':\n",
    "        print('Loading future coastal flood data ...')\n",
    "        for rp in rps:\n",
    "            #for file in files:\n",
    "            file_path = os.path.join(fl_path,'country','{}_{}_nosub_2030_rp{}_0.tif'.format(country_code,climate_model,rp))\n",
    "            with xr.open_dataset(file_path) as ds: #, engine=\"rasterio\"\n",
    "                df_ds = ds.to_dataframe().reset_index()\n",
    "                df_ds['geometry'] = pygeos.points(df_ds.x,y=df_ds.y)\n",
    "                df_ds = df_ds.rename(columns={'band_data': 'rp'+rp}) #rename to return period\n",
    "                df_ds['rp'+rp] = (df_ds['rp'+rp]*100)\n",
    "                df_ds = df_ds.drop(['band','x', 'y','spatial_ref'], axis=1)\n",
    "                df_ds = df_ds.dropna()\n",
    "                df_ds = df_ds.reset_index(drop=True)\n",
    "                df_ds.geometry= pygeos.buffer(df_ds.geometry,radius=0.00833/2,cap_style='square').values\n",
    "                df_ds['geometry'] = reproject(df_ds)\n",
    "                collect_df_ds.append(df_ds)\n",
    "\n",
    "        df_all = collect_df_ds[0].merge(collect_df_ds[1]).merge(collect_df_ds[2]).merge(collect_df_ds[3]).merge(collect_df_ds[4])\\\n",
    "                 .merge(collect_df_ds[5]).merge(collect_df_ds[6]).merge(collect_df_ds[7]).merge(collect_df_ds[8]).merge(collect_df_ds[9])\n",
    "\n",
    "        df_all = df_all.loc[df_all['rp1000']>0].reset_index(drop=True)\n",
    "    return df_all\n",
    "\n",
    "def open_flood_data(country_code):\n",
    "    climate_models = ['historical','rcp8p5']\n",
    "    df_ds = {}\n",
    "    for climate_model in climate_models:\n",
    "        df_ds_sc = load_flood_data(country_code,climate_model)\n",
    "        df_ds[climate_model] = df_ds_sc\n",
    "    \n",
    "    return df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2160cb1f-75ac-48f0-898e-6fdcb8bb3b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_flood_data('JPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5ac9f48-e434-4893-8c04-07e663921d94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n",
      "CPU times: total: 2min\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prk_flood = open_flood_data('PRK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3de688-606c-4dd2-abe3-f37015c7442e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OSM data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934161b6-5f01-4315-87b3-9e4a7bd70876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_osm_infrastructure(country_code,osm_data_path):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        country_code (_type_): _description_\n",
    "        osm_data_path (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "    \n",
    "    # lines\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    osm_lines = power_polyline(osm_path)\n",
    "    osm_lines['geometry'] = reproject(osm_lines)\n",
    "    osm_lines = buffer_assets(osm_lines.loc[osm_lines.asset.isin(\n",
    "        ['cable','minor_cable','line','minor_line'])],buffer_size=100).reset_index(drop=True)\n",
    "    \n",
    "    # polygons\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    osm_polygons = electricity(osm_path)\n",
    "    osm_polygons['geometry'] = reproject(osm_polygons)\n",
    "    \n",
    "    # points\n",
    "    osm_path = os.path.join(osm_data_path,'{}.osm.pbf'.format(country_code))\n",
    "    osm_points = power_point(osm_path)\n",
    "    osm_points['geometry'] = reproject(osm_points)\n",
    "    osm_points = buffer_assets(osm_points.loc[osm_points.asset.isin(\n",
    "        ['power_tower','power_pole'])],buffer_size=100).reset_index(drop=True)\n",
    "    \n",
    "    return osm_lines,osm_polygons,osm_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b16fbb7a-d740-4b39-bfdd-127321fb4cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|████████████████████████████████████████████████████████████████████| 2470/2470 [00:06<00:00, 360.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|███████████████████████████████████████████████████████████████████████| 368/368 [00:16<00:00, 22.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query is finished, lets start the loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract: 100%|████████████████████████████████████████████████████████████| 1608621/1608621 [02:36<00:00, 10296.68it/s]\n"
     ]
    }
   ],
   "source": [
    "osm_power_infra = extract_osm_infrastructure('TWN',osm_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abb7a943-1950-445c-8328-f6db95b7f163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_damage_osm(country_code,osm_power_infra,hazard_type):\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "\n",
    "    # load curves and maxdam\n",
    "    curves,maxdam = load_curves_maxdam(vul_curve_path,hazard_type)\n",
    "    \n",
    "    # read infrastructure data:\n",
    "    osm_lines,osm_poly,osm_points = osm_power_infra\n",
    "    #print(osm_lines['asset'].unique())\n",
    "    \n",
    "    #calculate damaged lines/polygons/points in loop by climate_model\n",
    "    damaged_lines = {}\n",
    "    damaged_poly = {}\n",
    "    damaged_points = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        # read wind data\n",
    "        climate_models = ['','_CMCC-CM2-VHR4'] #,'_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM'\n",
    "        df_ds = open_storm_data(country_code)\n",
    "\n",
    "        # remove assets that will not have any damage\n",
    "        osm_lines = osm_lines.loc[osm_lines.asset != 'cable'].reset_index(drop=True)\n",
    "        osm_lines['asset'] = osm_lines['asset'].replace(['minor_line'], 'line')\n",
    "        osm_poly = osm_poly.loc[osm_poly.asset != 'plant'].reset_index(drop=True)\n",
    "        \n",
    "        for climate_model in climate_models:\n",
    "            return_periods = ['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),'1_10{}'.format(climate_model),\n",
    "                              '1_25{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),\n",
    "                              '1_250{}'.format(climate_model),'1_500{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "            \n",
    "            # assess damage for lines\n",
    "            overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_lines).T,\n",
    "                                         columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_lines) == 0:\n",
    "                damaged_lines[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_line_damages = []\n",
    "                for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                                  desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_line_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_lines,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_line = dict(zip(osm_lines.index,osm_lines.asset))\n",
    "                results = pd.DataFrame([item for sublist in collect_line_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            # assess damage for polygons\n",
    "            if len(osm_poly) > 0:\n",
    "                overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_poly).T,\n",
    "                                        columns=['asset','hazard_point'])\n",
    "            else:\n",
    "                overlay_poly = pd.DataFrame()\n",
    "\n",
    "            if len(overlay_poly) == 0:\n",
    "                damaged_poly[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_poly_damages = []\n",
    "                for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),\n",
    "                                  desc='polygon damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_poly_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_poly,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_poly = dict(zip(osm_poly.index,osm_poly.asset))\n",
    "                \n",
    "                results = pd.DataFrame([item for sublist in collect_poly_damages \n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_poly[x])    \n",
    "\n",
    "                damaged_poly[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            #assess damage for points\n",
    "            overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_points).T,\n",
    "                                          columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_points) == 0:\n",
    "                damaged_points[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_point_damages = []\n",
    "                for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                                  desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_point_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_points,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_point = dict(zip(osm_points.index,osm_points.asset))\n",
    "                \n",
    "                results = pd.DataFrame([item for sublist in collect_point_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])    \n",
    "\n",
    "                damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "\n",
    "    elif hazard_type=='fl':\n",
    "        # read flood data\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "        df_ds = open_flood_data(country_code)\n",
    "    \n",
    "        for climate_model in climate_models:\n",
    "            return_periods = ['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'] \n",
    "\n",
    "            # assess damage for lines\n",
    "            overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_lines).T,\n",
    "                                         columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_lines) == 0:\n",
    "                damaged_lines[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_line_damages = []\n",
    "                for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                                  desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_line_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_lines,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_line = dict(zip(osm_lines.index,osm_lines.asset))\n",
    "                results = pd.DataFrame(collect_line_damages,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                #sum damage of line, cable, and minor_line\n",
    "                results['curve'] = results['curve'].replace(['cable', 'minor_line'], 'line')\n",
    "                results['asset_type'] = results['asset_type'].replace(['cable', 'minor_line'], 'line')\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            # assess damage for polygons\n",
    "            if len(osm_poly) > 0:\n",
    "                overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_poly).T,\n",
    "                                        columns=['asset','hazard_point'])\n",
    "            else:\n",
    "                overlay_poly = pd.DataFrame()\n",
    "\n",
    "            if len(overlay_poly) == 0:\n",
    "                damaged_poly[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_poly_damages = []\n",
    "                for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),\n",
    "                                  desc='polygon damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_poly_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_poly,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_poly = dict(zip(osm_poly.index,osm_poly.asset))\n",
    "                               \n",
    "                #results = pd.DataFrame(collect_poly_damages ,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                results = pd.DataFrame([item for sublist in collect_poly_damages \n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_poly[x])    \n",
    "\n",
    "                damaged_poly[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            # assess damage for points\n",
    "            overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_points).T,\n",
    "                                          columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_points) == 0:\n",
    "                damaged_points[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_point_damages = []\n",
    "                for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                                  desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_point_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_points,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_point = dict(zip(osm_points.index,osm_points.asset))\n",
    "                \n",
    "               \n",
    "                #results = pd.DataFrame(collect_point_damages ,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results = pd.DataFrame(np.array(list(flatten(collect_point_damages))).reshape(\n",
    "                    int(len(list(flatten(collect_point_damages)))/6), 6),\n",
    "                                       columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results['asset'] = results['asset'].astype(int)\n",
    "                results[['meandam','lowerdam','upperdam']] = results[['meandam','lowerdam','upperdam']].astype(float)\n",
    "                \n",
    "                #return collect_point_damages,get_asset_type_point\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])    \n",
    "\n",
    "                damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "    return damaged_lines,damaged_poly,damaged_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a4922e6-f273-465d-aabf-1431c1e91d1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_damage_osm(country_code,osm_power_infra,hazard_type):\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "\n",
    "    # load curves and maxdam\n",
    "    curves,maxdam = load_curves_maxdam(vul_curve_path,hazard_type)\n",
    "    \n",
    "    # read infrastructure data:\n",
    "    osm_lines,osm_poly,osm_points = osm_power_infra\n",
    "    #print(osm_lines['asset'].unique())\n",
    "    \n",
    "    #calculate damaged lines/polygons/points in loop by climate_model\n",
    "    damaged_lines = {}\n",
    "    damaged_poly = {}\n",
    "    damaged_points = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        # read wind data\n",
    "        climate_models = ['','_CMCC-CM2-VHR4'] #,'_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM'\n",
    "        df_ds = open_storm_data(country_code)\n",
    "\n",
    "        # remove assets that will not have any damage\n",
    "        osm_lines = osm_lines.loc[osm_lines.asset != 'cable'].reset_index(drop=True)\n",
    "        osm_lines['asset'] = osm_lines['asset'].replace(['minor_line'], 'line')\n",
    "        osm_poly = osm_poly.loc[osm_poly.asset != 'plant'].reset_index(drop=True)            \n",
    "    \n",
    "    elif hazard_type=='fl':\n",
    "        # read flood data\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "        df_ds = open_flood_data(country_code)\n",
    "        \n",
    "    for climate_model in climate_models:\n",
    "        if hazard_type=='tc':\n",
    "            return_periods = ['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),'1_10{}'.format(climate_model),\n",
    "                              '1_25{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),\n",
    "                              '1_250{}'.format(climate_model),'1_500{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "        elif hazard_type == 'fl':\n",
    "            return_periods = ['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000']     \n",
    "    \n",
    "        # assess damage for lines\n",
    "        overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_lines).T,\n",
    "                                     columns=['asset','hazard_point'])\n",
    "\n",
    "        if len(overlay_lines) == 0:\n",
    "            damaged_lines[climate_model] = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            collect_line_damages = []\n",
    "            for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                              desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_line_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                            df_ds[climate_model],\n",
    "                                                                            osm_lines,\n",
    "                                                                            curves,\n",
    "                                                                            maxdam,\n",
    "                                                                            return_period,\n",
    "                                                                            country_code))\n",
    "\n",
    "            get_asset_type_line = dict(zip(osm_lines.index,osm_lines.asset))\n",
    "            \n",
    "            if hazard_type == 'tc':\n",
    "                results = pd.DataFrame([item for sublist in collect_line_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            elif hazard_type == 'fl':\n",
    "                results = pd.DataFrame(collect_line_damages,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                #sum damage of line, cable, and minor_line\n",
    "                results['curve'] = results['curve'].replace(['cable', 'minor_line'], 'line')\n",
    "                results['asset_type'] = results['asset_type'].replace(['cable', 'minor_line'], 'line')\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "                \n",
    "        # assess damage for polygons\n",
    "        if len(osm_poly) > 0:\n",
    "            overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_poly).T,\n",
    "                                    columns=['asset','hazard_point'])\n",
    "        else:\n",
    "            overlay_poly = pd.DataFrame()\n",
    "\n",
    "        if len(overlay_poly) == 0:\n",
    "            damaged_poly[climate_model] = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            collect_poly_damages = []\n",
    "            for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),\n",
    "                              desc='polygon damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_poly_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                            df_ds[climate_model],\n",
    "                                                                            osm_poly,\n",
    "                                                                            curves,\n",
    "                                                                            maxdam,\n",
    "                                                                            return_period,\n",
    "                                                                            country_code))\n",
    "\n",
    "            get_asset_type_poly = dict(zip(osm_poly.index,osm_poly.asset))\n",
    "            \n",
    "            results = pd.DataFrame([item for sublist in collect_poly_damages \n",
    "                                    for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "            results['asset_type'] = results.asset.apply(lambda x : get_asset_type_poly[x])    \n",
    "\n",
    "            damaged_poly[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "                \n",
    "\n",
    "        #assess damage for points\n",
    "        overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_points).T,\n",
    "                                      columns=['asset','hazard_point'])\n",
    "\n",
    "        if len(overlay_points) == 0:\n",
    "            damaged_points[climate_model] = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            collect_point_damages = []\n",
    "            for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                              desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_point_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                            df_ds[climate_model],\n",
    "                                                                            osm_points,\n",
    "                                                                            curves,\n",
    "                                                                            maxdam,\n",
    "                                                                            return_period,\n",
    "                                                                            country_code))\n",
    "\n",
    "            get_asset_type_point = dict(zip(osm_points.index,osm_points.asset))\n",
    "            \n",
    "            if hazard_type == 'tc':\n",
    "                results = pd.DataFrame([item for sublist in collect_point_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])    \n",
    "                \n",
    "                damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "            \n",
    "            elif hazard_type == 'fl':\n",
    "                results = pd.DataFrame(np.array(list(flatten(collect_point_damages))).reshape(\n",
    "                    int(len(list(flatten(collect_point_damages)))/6), 6),\n",
    "                                       columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                results['asset'] = results['asset'].astype(int)\n",
    "                results[['meandam','lowerdam','upperdam']] = results[['meandam','lowerdam','upperdam']].astype(float)\n",
    "                \n",
    "                #return collect_point_damages,get_asset_type_point\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])    \n",
    "\n",
    "                damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "    return damaged_lines,damaged_poly,damaged_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d355b5f-1ad4-4d91-b4a3-1506667bb7b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "osm_damage_infra = assess_damage_osm('TWN',osm_power_infra,'tc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "313d91ab-0996-4aa6-8de6-49b0cb2bf424",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rp</th>\n",
       "      <th>curve</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>meandam</th>\n",
       "      <th>lowerdam</th>\n",
       "      <th>upperdam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>W2_1_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>2.396356e+08</td>\n",
       "      <td>1.797267e+08</td>\n",
       "      <td>2.995445e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>W2_1_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>4.792712e+08</td>\n",
       "      <td>3.594534e+08</td>\n",
       "      <td>5.990890e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>W2_1_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>1.198178e+09</td>\n",
       "      <td>8.986336e+08</td>\n",
       "      <td>1.497723e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>W2_2_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>2.047256e+08</td>\n",
       "      <td>1.535442e+08</td>\n",
       "      <td>2.559070e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>W2_2_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>4.094511e+08</td>\n",
       "      <td>3.070884e+08</td>\n",
       "      <td>5.118139e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.200</td>\n",
       "      <td>W2_6_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>1.817048e+08</td>\n",
       "      <td>1.362786e+08</td>\n",
       "      <td>2.271310e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.200</td>\n",
       "      <td>W2_6_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>4.542620e+08</td>\n",
       "      <td>3.406965e+08</td>\n",
       "      <td>5.678275e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.200</td>\n",
       "      <td>W2_7_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>4.891202e+07</td>\n",
       "      <td>3.668402e+07</td>\n",
       "      <td>6.114003e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.200</td>\n",
       "      <td>W2_7_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>9.782404e+07</td>\n",
       "      <td>7.336803e+07</td>\n",
       "      <td>1.222801e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.200</td>\n",
       "      <td>W2_7_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>2.445601e+08</td>\n",
       "      <td>1.834201e+08</td>\n",
       "      <td>3.057001e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rp   curve  asset_type       meandam      lowerdam      upperdam\n",
       "0    0.001  W2_1_1  substation  2.396356e+08  1.797267e+08  2.995445e+08\n",
       "1    0.001  W2_1_2  substation  4.792712e+08  3.594534e+08  5.990890e+08\n",
       "2    0.001  W2_1_3  substation  1.198178e+09  8.986336e+08  1.497723e+09\n",
       "3    0.001  W2_2_1  substation  2.047256e+08  1.535442e+08  2.559070e+08\n",
       "4    0.001  W2_2_2  substation  4.094511e+08  3.070884e+08  5.118139e+08\n",
       "..     ...     ...         ...           ...           ...           ...\n",
       "205  0.200  W2_6_2  substation  1.817048e+08  1.362786e+08  2.271310e+08\n",
       "206  0.200  W2_6_3  substation  4.542620e+08  3.406965e+08  5.678275e+08\n",
       "207  0.200  W2_7_1  substation  4.891202e+07  3.668402e+07  6.114003e+07\n",
       "208  0.200  W2_7_2  substation  9.782404e+07  7.336803e+07  1.222801e+08\n",
       "209  0.200  W2_7_3  substation  2.445601e+08  1.834201e+08  3.057001e+08\n",
       "\n",
       "[210 rows x 6 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract nested dict by key\n",
    "osm_damage_infra[1]['_CMCC-CM2-VHR4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e56c1d61-40b5-4892-ac70-c20c522eb661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def country_analysis_osm(country_code,hazard_type):\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "    \n",
    "    # extract infrastructure data from OSM\n",
    "    osm_power_infra = extract_osm_infrastructure(country_code,osm_data_path)\n",
    "    \n",
    "    # assess damage to hazard_type\n",
    "    osm_damage_infra = assess_damage_osm(country_code,osm_power_infra,hazard_type)\n",
    "    \n",
    "    line_risk = {}\n",
    "    plant_risk = {}\n",
    "    substation_risk = {}\n",
    "    tower_risk = {}\n",
    "    pole_risk = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "\n",
    "        for i in range(len(osm_damage_infra)):\n",
    "            for climate_model in climate_models:\n",
    "                df = osm_damage_infra[i][climate_model]\n",
    "\n",
    "                if len(df) == 0:\n",
    "                    print(\"No {}_{} risk of infra_type {} in {}\".format(hazard_type,climate_model,i,country_code))\n",
    "\n",
    "                else:\n",
    "                    with pd.ExcelWriter(os.path.join(output_path,'damage','{}_osm_{}{}_damage_{}'.format(country_code,hazard_type,climate_model,i)+'.xlsx')) as writer:\n",
    "                        df.to_excel(writer)\n",
    "\n",
    "                    df['rp'] = df['rp'].replace(['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),\n",
    "                                                '1_10{}'.format(climate_model),'1_25{}'.format(climate_model),'1_50{}'.format(climate_model),\n",
    "                                                '1_100{}'.format(climate_model),'1_250{}'.format(climate_model),'1_500{}'.format(climate_model),\n",
    "                                                '1_1000{}'.format(climate_model)],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "\n",
    "                    curve_code_substation = ['W2_1_1','W2_1_2','W2_1_3','W2_2_1','W2_2_2','W2_2_3','W2_3_1','W2_3_2','W2_3_3',\n",
    "                                            'W2_4_1','W2_4_2','W2_4_3','W2_5_1','W2_5_2','W2_5_3','W2_6_1','W2_6_2','W2_6_3',\n",
    "                                            'W2_7_1','W2_7_2','W2_7_3']\n",
    "\n",
    "                    curve_code_tower = ['W3_1','W3_2','W3_3','W3_4','W3_5','W3_6','W3_7','W3_8','W3_9','W3_10','W3_11','W3_12',\n",
    "                                        'W3_13','W3_14','W3_15','W3_16','W3_17','W3_18','W3_19','W3_20','W3_21','W3_22','W3_23',\n",
    "                                        'W3_24','W3_25','W3_26','W3_27','W3_28','W3_29','W3_30']\n",
    "\n",
    "                    curve_code_pole = ['W4_1','W4_2','W4_3','W4_4','W4_5','W4_6','W4_7','W4_8','W4_9','W4_10','W4_11','W4_12',\n",
    "                                    'W4_13','W4_14','W4_15','W4_16','W4_17','W4_18','W4_19','W4_20','W4_21','W4_22','W4_23',\n",
    "                                    'W4_24','W4_25','W4_26','W4_27','W4_28','W4_29','W4_30','W4_31','W4_32','W4_33','W4_34',\n",
    "                                    'W4_35','W4_36','W4_37','W4_38','W4_39','W4_40','W4_41','W4_42','W4_43','W4_44','W4_45',\n",
    "                                    'W4_46','W4_47','W4_48','W4_49','W4_50','W4_51','W4_52','W4_53','W4_54','W4_55']\n",
    "\n",
    "                    curve_code_line = ['W5_1','W5_2','W5_3']\n",
    "\n",
    "                    #assess risk for power lines\n",
    "                    if i == 0:\n",
    "                        for curve_code in curve_code_line:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power lines ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                line_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "\n",
    "                    #assess risk for power substations                \n",
    "                    elif i == 1:                        \n",
    "                        for curve_code in curve_code_substation:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of substations ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                substation_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "\n",
    "                    #assess risk for power towers and power poles\n",
    "                    elif i == 2:\n",
    "                        for curve_code in curve_code_tower:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power towers ...\")\n",
    "\n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                tower_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "\n",
    "                            for curve_code in curve_code_pole:\n",
    "                                loss_list = df.loc[df['curve'] == curve_code]\n",
    "                                loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                                if len(loss_list) == 0:\n",
    "                                    print(\"No risk of power poles ...\")\n",
    "\n",
    "                                else:                    \n",
    "                                    loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                    loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                    loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                    RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                    RPS = RPS.rp.values.tolist()\n",
    "                                    pole_risk[climate_model,curve_code] = {\n",
    "                                        'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                        'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                        'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                    }\n",
    "\n",
    "    elif hazard_type=='fl':\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "    \n",
    "        for i in range(len(osm_damage_infra)):\n",
    "            for climate_model in climate_models:\n",
    "                df = osm_damage_infra[i][climate_model]\n",
    "                    \n",
    "                if len(df) == 0:\n",
    "                    print(\"No {}_{} risk of infra_type {} in {}\".format(hazard_type,climate_model,i,country_code))\n",
    "\n",
    "                else:\n",
    "                    with pd.ExcelWriter(os.path.join(output_path,'damage','{}_osm_{}_{}_damage_{}'.format(country_code,hazard_type,climate_model,i)+'.xlsx')) as writer:\n",
    "                        df.to_excel(writer)\n",
    "\n",
    "                    df['rp'] = df['rp'].replace(['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "                    \n",
    "                    curve_code_plant = ['F1_1_1','F1_1_2','F1_1_3']\n",
    "                    curve_code_substation = ['F2_1_1','F2_1_2','F2_1_3']\n",
    "                    curve_code_tower = ['F3_1']\n",
    "                    curve_code_pole = ['F4_1_1','F4_1_2','F4_1_3']\n",
    "                    curve_code_line = ['F5_1']\n",
    "                    curve_code_minor_line = ['F5_2']\n",
    "                    curve_code_cable = ['F5_3']\n",
    "\n",
    "                    #assess risk for power lines\n",
    "                    if i == 0:\n",
    "                        for curve_code in curve_code_line:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power lines ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                line_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "\n",
    "                    #assess risk for power plants and substations                \n",
    "                    elif i == 1:\n",
    "                        for curve_code in curve_code_plant:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of plants ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                plant_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "\n",
    "                        for curve_code in curve_code_substation:    \n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of substations ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                substation_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                    }\n",
    "\n",
    "                    #assess risk for power towers and power poles\n",
    "                    elif i == 2:\n",
    "                        for curve_code in curve_code_tower:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power towers ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                tower_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "                            \n",
    "                        for curve_code in curve_code_pole:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power poles ...\")\n",
    "                            \n",
    "                            else:                    \n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                pole_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "                                \n",
    "    return pd.DataFrame(line_risk),pd.DataFrame(plant_risk),pd.DataFrame(substation_risk),pd.DataFrame(tower_risk),pd.DataFrame(pole_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ba717-0015-4527-bf1a-a9556f87a541",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Government data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef53a27c-f539-43db-9870-1ee933ddf7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_pg_infrastructure(country_code):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        country_code (_type_): _description_\n",
    "        pg_type (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "\n",
    "    files = [x for x in os.listdir(pg_data_path)  if country_code in x ]\n",
    "    pg_types = ['line','point']\n",
    "    \n",
    "    for pg_type in pg_types:\n",
    "        if os.path.isfile(os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))):\n",
    "            if pg_type=='line':\n",
    "                for file in files: \n",
    "                    file_path = os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))\n",
    "\n",
    "                    pg_data_country = gpd.read_file(file_path)\n",
    "                    pg_data_country = pd.DataFrame(pg_data_country.copy())\n",
    "                    pg_data_country.geometry = pygeos.from_shapely(pg_data_country.geometry)\n",
    "                    pg_data_country['geometry'] = reproject(pg_data_country)\n",
    "\n",
    "                pg_lines = buffer_assets(pg_data_country.loc[pg_data_country.asset.isin(['line'])],buffer_size=100).reset_index(drop=True)\n",
    "\n",
    "            elif pg_type=='point':\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(pg_data_path,'{}_{}.gpkg'.format(country_code,pg_type))\n",
    "\n",
    "                    pg_data_country = gpd.read_file(file_path)\n",
    "                    pg_data_country = pd.DataFrame(pg_data_country.copy())\n",
    "                    pg_data_country.geometry = pygeos.from_shapely(pg_data_country.geometry)\n",
    "                    pg_data_country['geometry'] = reproject(pg_data_country)\n",
    "\n",
    "                pg_points = buffer_assets(pg_data_country.loc[pg_data_country.asset.isin(['plant','substation','power_tower','power_pole'])],buffer_size=100).reset_index(drop=True)\n",
    "\n",
    "    return pg_lines,pg_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6a7cdb0-f9a6-4d0c-9dd9-c9d88d7e98e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "pg_infra = extract_pg_infrastructure('JPN')\n",
    "print(type(pg_infra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c78d9c1-f773-42ea-ad91-0acb4aad54bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>asset</th>\n",
       "      <th>geometry</th>\n",
       "      <th>buffered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (15911096.018 5305017.559)</td>\n",
       "      <td>POLYGON ((15911196.018 5305017.559, 15911194.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (15830095.626 5281047.141)</td>\n",
       "      <td>POLYGON ((15830195.626 5281047.141, 15830193.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (15784471.912 5372262.28)</td>\n",
       "      <td>POLYGON ((15784571.912 5372262.28, 15784569.99...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (15725429.458 5296354.502)</td>\n",
       "      <td>POLYGON ((15725529.458 5296354.502, 15725527.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (15678341.881 5273735.047)</td>\n",
       "      <td>POLYGON ((15678441.881 5273735.047, 15678439.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (14654065.093 3892423.909)</td>\n",
       "      <td>POLYGON ((14654165.093 3892423.909, 14654163.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (14567941.182 3869470.184)</td>\n",
       "      <td>POLYGON ((14568041.182 3869470.184, 14568039.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (14554522.443 3832084.632)</td>\n",
       "      <td>POLYGON ((14554622.443 3832084.632, 14554620.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (14530856.666 3761834.532)</td>\n",
       "      <td>POLYGON ((14530956.666 3761834.532, 14530954.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>substation</td>\n",
       "      <td>POINT (14591119.005 3740566.664)</td>\n",
       "      <td>POLYGON ((14591219.005 3740566.664, 14591217.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id       asset                          geometry  \\\n",
       "0    1  substation  POINT (15911096.018 5305017.559)   \n",
       "1    2  substation  POINT (15830095.626 5281047.141)   \n",
       "2    3  substation   POINT (15784471.912 5372262.28)   \n",
       "3    4  substation  POINT (15725429.458 5296354.502)   \n",
       "4    5  substation  POINT (15678341.881 5273735.047)   \n",
       "..  ..         ...                               ...   \n",
       "63  64  substation  POINT (14654065.093 3892423.909)   \n",
       "64  65  substation  POINT (14567941.182 3869470.184)   \n",
       "65  66  substation  POINT (14554522.443 3832084.632)   \n",
       "66  67  substation  POINT (14530856.666 3761834.532)   \n",
       "67  68  substation  POINT (14591119.005 3740566.664)   \n",
       "\n",
       "                                             buffered  \n",
       "0   POLYGON ((15911196.018 5305017.559, 15911194.0...  \n",
       "1   POLYGON ((15830195.626 5281047.141, 15830193.7...  \n",
       "2   POLYGON ((15784571.912 5372262.28, 15784569.99...  \n",
       "3   POLYGON ((15725529.458 5296354.502, 15725527.5...  \n",
       "4   POLYGON ((15678441.881 5273735.047, 15678439.9...  \n",
       "..                                                ...  \n",
       "63  POLYGON ((14654165.093 3892423.909, 14654163.1...  \n",
       "64  POLYGON ((14568041.182 3869470.184, 14568039.2...  \n",
       "65  POLYGON ((14554622.443 3832084.632, 14554620.5...  \n",
       "66  POLYGON ((14530956.666 3761834.532, 14530954.7...  \n",
       "67  POLYGON ((14591219.005 3740566.664, 14591217.0...  \n",
       "\n",
       "[68 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_infra[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c0797c42-fd24-45f7-a0c0-51783767aa2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_damage_pg(country_code,pg_infra,hazard_type):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        country_code (_type_): _description_\n",
    "        pg_data_country (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "\n",
    "    # load curves and maxdam\n",
    "    curves,maxdam = load_curves_maxdam(vul_curve_path,hazard_type)\n",
    "    \n",
    "    # read infrastructure data:\n",
    "    pg_lines,pg_points = pg_infra\n",
    "    \n",
    "    #calculate damaged lines/polygons/points in loop by climate_model\n",
    "    damaged_lines = {}\n",
    "    damaged_points = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        # read wind data\n",
    "        climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "        df_ds = open_storm_data(country_code)\n",
    "        \n",
    "        # remove assets that will not have any damage\n",
    "        pg_points = pg_points.loc[pg_points.asset != 'plant'].reset_index(drop=True)\n",
    "    \n",
    "    elif hazard_type == 'fl':\n",
    "        # read flood data\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "        df_ds = open_flood_data(country_code)\n",
    "        \n",
    "    for climate_model in climate_models:\n",
    "        if hazard_type=='tc':\n",
    "            return_periods = ['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),'1_10{}'.format(climate_model),\n",
    "                              '1_25{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),\n",
    "                              '1_250{}'.format(climate_model),'1_500{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "        elif hazard_type == 'fl':\n",
    "            return_periods = ['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'] \n",
    "            \n",
    "        # assess damage for lines\n",
    "        overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],pg_lines).T,\n",
    "                                     columns=['asset','hazard_point'])\n",
    "\n",
    "        if len(overlay_lines) == 0:\n",
    "            damaged_lines[climate_model] = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            collect_line_damages = []\n",
    "            for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                              desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_line_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                            df_ds[climate_model],\n",
    "                                                                            pg_lines,\n",
    "                                                                            curves,\n",
    "                                                                            maxdam,\n",
    "                                                                            return_period,\n",
    "                                                                            country_code))\n",
    "\n",
    "            get_asset_type_line = dict(zip(pg_lines.index,pg_lines.asset))\n",
    "            \n",
    "            if hazard_type=='tc':\n",
    "                results = pd.DataFrame([item for sublist in collect_line_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "                \n",
    "            elif hazard_type == 'fl':\n",
    "                results = pd.DataFrame(collect_line_damages,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                #sum damage of line, cable, and minor_line\n",
    "                results['curve'] = results['curve'].replace(['cable', 'minor_line'], 'line')\n",
    "                results['asset_type'] = results['asset_type'].replace(['cable', 'minor_line'], 'line')\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index() \n",
    "\n",
    "        # assess damage for points\n",
    "        overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],pg_points).T,\n",
    "                                      columns=['asset','hazard_point'])\n",
    "\n",
    "        if len(overlay_points) == 0:\n",
    "            damaged_points[climate_model] = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            collect_point_damages = []\n",
    "            for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                              desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                for return_period in return_periods:\n",
    "                    collect_point_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                            df_ds[climate_model],\n",
    "                                                                            pg_points,\n",
    "                                                                            curves,\n",
    "                                                                            maxdam,\n",
    "                                                                            return_period,\n",
    "                                                                            country_code))\n",
    "\n",
    "            get_asset_type_point = dict(zip(pg_points.index,pg_points.asset))\n",
    "            \n",
    "            if hazard_type == 'tc':\n",
    "                results = pd.DataFrame([item for sublist in collect_point_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "            elif hazard_type == 'fl':\n",
    "                results = pd.DataFrame(collect_point_damages ,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "            results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])    \n",
    "\n",
    "            damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "                \n",
    "    return damaged_lines,damaged_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9a76e-2690-4f96-bc6d-255885bbe73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_damage_pg(country_code,pg_power_infra,hazard_type):\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "\n",
    "    # load curves and maxdam\n",
    "    curves,maxdam = load_curves_maxdam(vul_curve_path,hazard_type)\n",
    "    \n",
    "    # read infrastructure data:\n",
    "    osm_lines,osm_poly,osm_points = osm_power_infra\n",
    "    #print(osm_lines['asset'].unique())\n",
    "    \n",
    "    #calculate damaged lines/polygons/points in loop by climate_model\n",
    "    damaged_lines = {}\n",
    "    damaged_poly = {}\n",
    "    damaged_points = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        # read wind data\n",
    "        climate_models = ['','_CMCC-CM2-VHR4'] #,'_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM'\n",
    "        df_ds = open_storm_data(country_code)\n",
    "\n",
    "        # remove assets that will not have any damage\n",
    "        osm_lines = osm_lines.loc[osm_lines.asset != 'cable'].reset_index(drop=True)\n",
    "        osm_lines['asset'] = osm_lines['asset'].replace(['minor_line'], 'line')\n",
    "        osm_poly = osm_poly.loc[osm_poly.asset != 'plant'].reset_index(drop=True)\n",
    "        \n",
    "        for climate_model in climate_models:\n",
    "            return_periods = ['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),'1_10{}'.format(climate_model),\n",
    "                              '1_25{}'.format(climate_model),'1_50{}'.format(climate_model),'1_100{}'.format(climate_model),\n",
    "                              '1_250{}'.format(climate_model),'1_500{}'.format(climate_model),'1_1000{}'.format(climate_model)]\n",
    "            \n",
    "            # assess damage for lines\n",
    "            overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_lines).T,\n",
    "                                         columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_lines) == 0:\n",
    "                damaged_lines[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_line_damages = []\n",
    "                for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                                  desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_line_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_lines,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_line = dict(zip(osm_lines.index,osm_lines.asset))\n",
    "                results = pd.DataFrame([item for sublist in collect_line_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            # assess damage for polygons\n",
    "            if len(osm_poly) > 0:\n",
    "                overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_poly).T,\n",
    "                                        columns=['asset','hazard_point'])\n",
    "            else:\n",
    "                overlay_poly = pd.DataFrame()\n",
    "\n",
    "            if len(overlay_poly) == 0:\n",
    "                damaged_poly[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_poly_damages = []\n",
    "                for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),\n",
    "                                  desc='polygon damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_poly_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_poly,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_poly = dict(zip(osm_poly.index,osm_poly.asset))\n",
    "                \n",
    "                results = pd.DataFrame([item for sublist in collect_poly_damages \n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_poly[x])    \n",
    "\n",
    "                damaged_poly[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            #assess damage for points\n",
    "            overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_points).T,\n",
    "                                          columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_points) == 0:\n",
    "                damaged_points[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_point_damages = []\n",
    "                for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                                  desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_point_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_points,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_point = dict(zip(osm_points.index,osm_points.asset))\n",
    "                \n",
    "                results = pd.DataFrame([item for sublist in collect_point_damages\n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])    \n",
    "\n",
    "                damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "\n",
    "    elif hazard_type=='fl':\n",
    "        # read flood data\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "        df_ds = open_flood_data(country_code)\n",
    "    \n",
    "        for climate_model in climate_models:\n",
    "            return_periods = ['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'] \n",
    "\n",
    "            # assess damage for lines\n",
    "            overlay_lines = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_lines).T,\n",
    "                                         columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_lines) == 0:\n",
    "                damaged_lines[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_line_damages = []\n",
    "                for asset in tqdm(overlay_lines.groupby('asset'),total=len(overlay_lines.asset.unique()),\n",
    "                                  desc='polyline damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_line_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_lines,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_line = dict(zip(osm_lines.index,osm_lines.asset))\n",
    "                results = pd.DataFrame(collect_line_damages,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_line[x])\n",
    "\n",
    "                #sum damage of line, cable, and minor_line\n",
    "                results['curve'] = results['curve'].replace(['cable', 'minor_line'], 'line')\n",
    "                results['asset_type'] = results['asset_type'].replace(['cable', 'minor_line'], 'line')\n",
    "\n",
    "                damaged_lines[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            # assess damage for polygons\n",
    "            if len(osm_poly) > 0:\n",
    "                overlay_poly = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_poly).T,\n",
    "                                        columns=['asset','hazard_point'])\n",
    "            else:\n",
    "                overlay_poly = pd.DataFrame()\n",
    "\n",
    "            if len(overlay_poly) == 0:\n",
    "                damaged_poly[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_poly_damages = []\n",
    "                for asset in tqdm(overlay_poly.groupby('asset'),total=len(overlay_poly.asset.unique()),\n",
    "                                  desc='polygon damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_poly_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_poly,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_poly = dict(zip(osm_poly.index,osm_poly.asset))\n",
    "                               \n",
    "                #results = pd.DataFrame(collect_poly_damages ,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                results = pd.DataFrame([item for sublist in collect_poly_damages \n",
    "                                        for item in sublist],columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_poly[x])    \n",
    "\n",
    "                damaged_poly[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "            # assess damage for points\n",
    "            overlay_points = pd.DataFrame(overlay_hazard_assets(df_ds[climate_model],osm_points).T,\n",
    "                                          columns=['asset','hazard_point'])\n",
    "\n",
    "            if len(overlay_points) == 0:\n",
    "                damaged_points[climate_model] = pd.DataFrame()\n",
    "\n",
    "            else:\n",
    "                collect_point_damages = []\n",
    "                for asset in tqdm(overlay_points.groupby('asset'),total=len(overlay_points.asset.unique()),\n",
    "                                  desc='point damage calculation for {} {} ({})'.format(country_code,hazard_type,climate_model)):\n",
    "                    for return_period in return_periods:\n",
    "                        collect_point_damages.append(get_damage_per_asset_per_rp(asset,\n",
    "                                                                                df_ds[climate_model],\n",
    "                                                                                osm_points,\n",
    "                                                                                curves,\n",
    "                                                                                maxdam,\n",
    "                                                                                return_period,\n",
    "                                                                                country_code))\n",
    "\n",
    "                get_asset_type_point = dict(zip(osm_points.index,osm_points.asset))\n",
    "                \n",
    "               \n",
    "                #results = pd.DataFrame(collect_point_damages ,columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results = pd.DataFrame(np.array(list(flatten(collect_point_damages))).reshape(\n",
    "                    int(len(list(flatten(collect_point_damages)))/6), 6),\n",
    "                                       columns=['rp','asset','curve','meandam','lowerdam','upperdam'])\n",
    "                \n",
    "                results['asset'] = results['asset'].astype(int)\n",
    "                results[['meandam','lowerdam','upperdam']] = results[['meandam','lowerdam','upperdam']].astype(float)\n",
    "                \n",
    "                #return collect_point_damages,get_asset_type_point\n",
    "                results['asset_type'] = results.asset.apply(lambda x : get_asset_type_point[x])    \n",
    "\n",
    "                damaged_points[climate_model] = results.groupby(['rp','curve','asset_type']).sum().drop(['asset'], axis=1).reset_index()\n",
    "\n",
    "    return damaged_lines,damaged_poly,damaged_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5680499-fc0f-454f-ab9b-4452a2c120ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for JPN tc (): 100%|███████████████████████████████████████| 31/31 [00:03<00:00,  9.78it/s]\n",
      "point damage calculation for JPN tc (): 100%|██████████████████████████████████████████| 68/68 [00:08<00:00,  7.87it/s]\n",
      "polyline damage calculation for JPN tc (_CMCC-CM2-VHR4): 100%|█████████████████████████| 31/31 [00:03<00:00,  9.75it/s]\n",
      "point damage calculation for JPN tc (_CMCC-CM2-VHR4): 100%|████████████████████████████| 68/68 [00:08<00:00,  7.79it/s]\n",
      "polyline damage calculation for JPN tc (_CNRM-CM6-1-HR): 100%|█████████████████████████| 31/31 [00:03<00:00,  9.72it/s]\n",
      "point damage calculation for JPN tc (_CNRM-CM6-1-HR): 100%|████████████████████████████| 68/68 [00:08<00:00,  7.76it/s]\n",
      "polyline damage calculation for JPN tc (_EC-Earth3P-HR): 100%|█████████████████████████| 31/31 [00:03<00:00,  9.77it/s]\n",
      "point damage calculation for JPN tc (_EC-Earth3P-HR): 100%|████████████████████████████| 68/68 [00:08<00:00,  7.71it/s]\n",
      "polyline damage calculation for JPN tc (_HadGEM3-GC31-HM): 100%|███████████████████████| 31/31 [00:03<00:00,  9.77it/s]\n",
      "point damage calculation for JPN tc (_HadGEM3-GC31-HM): 100%|██████████████████████████| 68/68 [00:08<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 50s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg_damage_infra_tc = assess_damage_pg('JPN',pg_infra,'tc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64cfcf4a-3381-4a83-9f58-6b254ce75336",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rp</th>\n",
       "      <th>curve</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>meandam</th>\n",
       "      <th>lowerdam</th>\n",
       "      <th>upperdam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1</td>\n",
       "      <td>W2_1_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>2.638182e+05</td>\n",
       "      <td>1.978637e+05</td>\n",
       "      <td>3.297728e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1</td>\n",
       "      <td>W2_1_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>5.276365e+05</td>\n",
       "      <td>3.957274e+05</td>\n",
       "      <td>6.595456e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1</td>\n",
       "      <td>W2_1_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>1.319091e+06</td>\n",
       "      <td>9.893184e+05</td>\n",
       "      <td>1.648864e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_1</td>\n",
       "      <td>W2_2_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>1.826395e+04</td>\n",
       "      <td>1.369796e+04</td>\n",
       "      <td>2.282994e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1</td>\n",
       "      <td>W2_2_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>3.652790e+04</td>\n",
       "      <td>2.739592e+04</td>\n",
       "      <td>4.565987e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1_500</td>\n",
       "      <td>W2_6_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>2.243239e+08</td>\n",
       "      <td>1.682430e+08</td>\n",
       "      <td>2.804049e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>1_500</td>\n",
       "      <td>W2_6_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>5.608099e+08</td>\n",
       "      <td>4.206074e+08</td>\n",
       "      <td>7.010123e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1_500</td>\n",
       "      <td>W2_7_1</td>\n",
       "      <td>substation</td>\n",
       "      <td>4.358600e+07</td>\n",
       "      <td>3.268950e+07</td>\n",
       "      <td>5.448250e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1_500</td>\n",
       "      <td>W2_7_2</td>\n",
       "      <td>substation</td>\n",
       "      <td>8.717200e+07</td>\n",
       "      <td>6.537900e+07</td>\n",
       "      <td>1.089650e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1_500</td>\n",
       "      <td>W2_7_3</td>\n",
       "      <td>substation</td>\n",
       "      <td>2.179300e+08</td>\n",
       "      <td>1.634475e+08</td>\n",
       "      <td>2.724125e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rp   curve  asset_type       meandam      lowerdam      upperdam\n",
       "0      1_1  W2_1_1  substation  2.638182e+05  1.978637e+05  3.297728e+05\n",
       "1      1_1  W2_1_2  substation  5.276365e+05  3.957274e+05  6.595456e+05\n",
       "2      1_1  W2_1_3  substation  1.319091e+06  9.893184e+05  1.648864e+06\n",
       "3      1_1  W2_2_1  substation  1.826395e+04  1.369796e+04  2.282994e+04\n",
       "4      1_1  W2_2_2  substation  3.652790e+04  2.739592e+04  4.565987e+04\n",
       "..     ...     ...         ...           ...           ...           ...\n",
       "205  1_500  W2_6_2  substation  2.243239e+08  1.682430e+08  2.804049e+08\n",
       "206  1_500  W2_6_3  substation  5.608099e+08  4.206074e+08  7.010123e+08\n",
       "207  1_500  W2_7_1  substation  4.358600e+07  3.268950e+07  5.448250e+07\n",
       "208  1_500  W2_7_2  substation  8.717200e+07  6.537900e+07  1.089650e+08\n",
       "209  1_500  W2_7_3  substation  2.179300e+08  1.634475e+08  2.724125e+08\n",
       "\n",
       "[210 rows x 6 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_damage_infra_tc[1]['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f7d5fb3-c1c9-4e1e-a73c-fea26042357f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for JPN fl (historical): 100%|███████████████████████████████| 5/5 [00:00<00:00, 25.00it/s]\n",
      "point damage calculation for JPN fl (historical): 100%|██████████████████████████████████| 1/1 [00:00<00:00, 45.45it/s]\n",
      "polyline damage calculation for JPN fl (rcp8p5): 100%|███████████████████████████████████| 5/5 [00:00<00:00, 24.04it/s]\n",
      "point damage calculation for JPN fl (rcp8p5): 100%|██████████████████████████████████████| 1/1 [00:00<00:00, 50.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8min 7s\n",
      "Wall time: 8min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg_damage_infra_fl = assess_damage_pg('JPN',pg_infra,'fl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc6d1154-a26e-4dfe-9ee0-86f6bb80c10d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'historical':        rp       curve  asset_type        meandam       lowerdam       upperdam\n",
       "  0  rp0001  substation  substation  490588.160384  367941.120288  613235.200480\n",
       "  1  rp0002  substation  substation  517498.328609  388123.746457  646872.910762\n",
       "  2  rp0005  substation  substation  559647.208366  419735.406274  699559.010457\n",
       "  3  rp0010  substation  substation  579966.806294  434975.104721  724958.507868\n",
       "  4  rp0025  substation  substation  608736.040167  456552.030125  760920.050208\n",
       "  5  rp0050  substation  substation  632107.357808  474080.518356  790134.197260\n",
       "  6  rp0100  substation  substation  652340.431603  489255.323702  815425.539504\n",
       "  7  rp0250  substation  substation  682990.145845  512242.609384  853737.682306\n",
       "  8  rp0500  substation  substation  699857.025260  524892.768945  874821.281575\n",
       "  9  rp1000  substation  substation  721527.021566  541145.266174  901908.776957,\n",
       "  'rcp8p5':        rp       curve  asset_type        meandam       lowerdam       upperdam\n",
       "  0  rp0001  substation  substation  563081.188997  422310.891747  703851.486246\n",
       "  1  rp0002  substation  substation  576128.595727  432096.446795  720160.744658\n",
       "  2  rp0005  substation  substation  610238.220801  457678.665601  762797.776001\n",
       "  3  rp0010  substation  substation  636848.474749  477636.356061  796060.593436\n",
       "  4  rp0025  substation  substation  663008.857279  497256.642959  828761.071599\n",
       "  5  rp0050  substation  substation  688989.026262  516741.769697  861236.282828\n",
       "  6  rp0100  substation  substation  707410.163024  530557.622268  884262.703781\n",
       "  7  rp0250  substation  substation  731011.743016  548258.807262  913764.678770\n",
       "  8  rp0500  substation  substation  756650.033516  567487.525137  945812.541895\n",
       "  9  rp1000  substation  substation  773409.866355  580057.399766  966762.332944},\n",
       " {})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_damage_infra_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d174ad-3c24-44ea-88cb-0b88ee406e40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def country_analysis_pg(country_code,hazard_type):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        country_code (_type_): _description_\n",
    "        hazard_type (str, optional): _description_. Defaults to 'OSM'.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # set paths\n",
    "    #data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "    \n",
    "    # extract infrastructure data from gov data\n",
    "    pg_power_infra = extract_pg_infrastructure(country_code)\n",
    "    \n",
    "    # assess damage to hazard_type\n",
    "    pg_damage_infra = assess_damage_pg(country_code,pg_power_infra,hazard_type)\n",
    "    \n",
    "    line_risk = {}\n",
    "    plant_risk = {}\n",
    "    substation_risk = {}\n",
    "\n",
    "    if hazard_type=='tc':\n",
    "        climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "\n",
    "        for i in range(len(pg_damage_infra)):\n",
    "            for climate_model in climate_models:\n",
    "                df = pg_damage_infra[i][climate_model]\n",
    "                    \n",
    "                if len(df) == 0:\n",
    "                    print(\"No {}_{} risk of infra_type {} in {}\".format(hazard_type,climate_model,i,country_code))\n",
    "\n",
    "                else:\n",
    "                    with pd.ExcelWriter(os.path.join(output_path,'damage','{}_pg_{}{}_damage_{}'.format(country_code,hazard_type,climate_model,i)+'.xlsx')) as writer:\n",
    "                        df.to_excel(writer)\n",
    "\n",
    "                    df['rp'] = df['rp'].replace(['1_1{}'.format(climate_model),'1_2{}'.format(climate_model),'1_5{}'.format(climate_model),\n",
    "                                                '1_10{}'.format(climate_model),'1_25{}'.format(climate_model),'1_50{}'.format(climate_model),\n",
    "                                                '1_100{}'.format(climate_model),'1_250{}'.format(climate_model),'1_500{}'.format(climate_model),\n",
    "                                                '1_1000{}'.format(climate_model)],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "                    \n",
    "                    curve_code_substation = ['W2_1_1','W2_1_2','W2_1_3','W2_2_1','W2_2_2','W2_2_3','W2_3_1','W2_3_2','W2_3_3',\n",
    "                                            'W2_4_1','W2_4_2','W2_4_3','W2_5_1','W2_5_2','W2_5_3','W2_6_1','W2_6_2','W2_6_3',\n",
    "                                            'W2_7_1','W2_7_2','W2_7_3']\n",
    "                    \n",
    "                    curve_code_line = ['W5_1','W5_2','W5_3']\n",
    "\n",
    "                    #assess risk for power lines\n",
    "                    if i == 0:\n",
    "                        for curve_code in curve_code_line:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power lines ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "\n",
    "                                line_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "                            #print(line_risk_curve)\n",
    "                    \n",
    "                    #assess risk for power substations                \n",
    "                    elif i == 1:                        \n",
    "                        for curve_code in curve_code_substation:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            loss_list = loss_list.sort_values(by='rp',ascending=False)\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of substations ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "\n",
    "                                substation_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "\n",
    "\n",
    "    elif hazard_type=='fl':\n",
    "        climate_models = ['historical','rcp8p5']\n",
    "    \n",
    "        for i in range(len(pg_damage_infra)):\n",
    "            for climate_model in climate_models:\n",
    "                df = pg_damage_infra[i][climate_model]\n",
    "                    \n",
    "                if len(df) == 0:\n",
    "                    print(\"No {}_{} risk of infra_type {} in {}\".format(hazard_type,climate_model,i,country_code))\n",
    "\n",
    "                else:\n",
    "                    with pd.ExcelWriter(os.path.join(output_path,'damage','{}_pg_{}_{}_damage_{}'.format(country_code,hazard_type,climate_model,i)+'.xlsx')) as writer:\n",
    "                        df.to_excel(writer)\n",
    "\n",
    "                    df['rp'] = df['rp'].replace(['rp0001','rp0002','rp0005','rp0010','rp0025','rp0050','rp0100','rp0250','rp0500','rp1000'],\n",
    "                                                [1,0.5,0.2,0.1,0.04,0.02,0.01,0.004,0.002,0.001])\n",
    "                    \n",
    "                    curve_code_plant = ['F1_1_1','F1_1_2','F1_1_3']\n",
    "                    curve_code_substation = ['F2_1_1','F2_1_2','F2_1_3']\n",
    "                    curve_code_line = ['F5_1']\n",
    "\n",
    "                    #assess risk for power lines\n",
    "                    if i == 0:\n",
    "                        for curve_code in curve_code_line:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of power lines ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                line_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "\n",
    "                    #assess risk for power plants and substations                \n",
    "                    elif i == 1:\n",
    "                        for curve_code in curve_code_plant:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of plants ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                plant_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                }\n",
    "                            \n",
    "                        for curve_code in curve_code_substation:\n",
    "                            loss_list = df.loc[df['curve'] == curve_code]\n",
    "                            if len(loss_list) == 0:\n",
    "                                print(\"No risk of substations ...\")\n",
    "                            \n",
    "                            else:\n",
    "                                loss_list_mean = loss_list.meandam.values.tolist()\n",
    "                                loss_list_lower = loss_list.lowerdam.values.tolist()\n",
    "                                loss_list_upper = loss_list.upperdam.values.tolist()\n",
    "                                RPS = loss_list.loc[loss_list['curve'] == curve_code]\n",
    "                                RPS = RPS.rp.values.tolist()\n",
    "                                substation_risk[climate_model,curve_code] = {\n",
    "                                    'mean_risk': integrate.simps(y=loss_list_mean[::-1], x=RPS[::-1]),\n",
    "                                    'lower_risk': integrate.simps(y=loss_list_lower[::-1], x=RPS[::-1]),\n",
    "                                    'upper_risk': integrate.simps(y=loss_list_upper[::-1], x=RPS[::-1])\n",
    "                                    }\n",
    "                            \n",
    "    return pd.DataFrame(line_risk),pd.DataFrame(plant_risk),pd.DataFrame(substation_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b2f45-0e6a-4452-b85a-1cf009ec3d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pg_risk_tc = country_analysis_pg('JPN','tc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c6222f6c-3c12-4b7a-84ba-544e31de677f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_CMCC-CM2-VHR4</th>\n",
       "      <th>_CNRM-CM6-1-HR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W2_7_3\n",
       "mean_risk   1.434933e...</td>\n",
       "      <td>W2_7_3\n",
       "mean_risk   1.544563e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      _CMCC-CM2-VHR4  \\\n",
       "0                    W2_7_3\n",
       "mean_risk   1.434933e...   \n",
       "\n",
       "                                      _CNRM-CM6-1-HR  \n",
       "0                    W2_7_3\n",
       "mean_risk   1.544563e...  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pg_risk_tc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5755396e-ef87-4815-ba01-1f91eb685bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical coastal flood data ...\n",
      "Loading future coastal flood data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for JPN fl (historical): 100%|███████████████████████████████| 5/5 [00:00<00:00, 24.88it/s]\n",
      "point damage calculation for JPN fl (historical): 100%|██████████████████████████████████| 1/1 [00:00<00:00, 41.67it/s]\n",
      "polyline damage calculation for JPN fl (rcp8p5): 100%|███████████████████████████████████| 5/5 [00:00<00:00, 23.80it/s]\n",
      "point damage calculation for JPN fl (rcp8p5): 100%|██████████████████████████████████████| 1/1 [00:00<00:00, 49.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No risk of plants ...\n",
      "No risk of plants ...\n"
     ]
    }
   ],
   "source": [
    "pg_risk_fl = country_analysis_pg('JPN','fl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f076a1e2-6f11-47d7-b19b-595720d8de7b",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dda764da-ee7e-49ea-9572-e263c80ddecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def risk_output(country_code,hazard_type,infra_type):\n",
    "    # set paths\n",
    "    data_path,tc_path,fl_path,osm_data_path,pg_data_path,vul_curve_path,output_path,ne_path = set_paths()\n",
    "  \n",
    "    if infra_type == 'osm':\n",
    "        line_risk,plant_risk,substation_risk,tower_risk,pole_risk = country_analysis_osm(country_code,hazard_type)\n",
    "        \n",
    "        if hazard_type == 'tc':\n",
    "            climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "\n",
    "            for climate_model in climate_models:\n",
    "                if climate_model == '':\n",
    "                    writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}_{}_risk'.format(country_code,infra_type,hazard_type,'present')+'.xlsx'),\n",
    "                                            engine='openpyxl')\n",
    "                else:\n",
    "                    writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}{}_risk'.format(country_code,infra_type,hazard_type,climate_model)+'.xlsx'),\n",
    "                                            engine='openpyxl')\n",
    "                    \n",
    "                # write each dataframe to a different sheet\n",
    "                if len(line_risk) != 0:\n",
    "                    line_risk[climate_model].to_excel(writer, sheet_name='line_risk')\n",
    "                if len(substation_risk) != 0:\n",
    "                    substation_risk[climate_model].to_excel(writer, sheet_name='substation_risk')\n",
    "                if len(tower_risk) != 0:\n",
    "                    tower_risk[climate_model].to_excel(writer, sheet_name='tower_risk')\n",
    "                if len(pole_risk) != 0:\n",
    "                    pole_risk[climate_model].to_excel(writer, sheet_name='pole_risk')\n",
    "                \n",
    "                # save the Excel file\n",
    "                writer.save()\n",
    "\n",
    "        elif hazard_type == 'fl':\n",
    "            climate_models = ['historical','rcp8p5']\n",
    "\n",
    "            for climate_model in climate_models:\n",
    "\n",
    "                # create a Pandas Excel writer using openpyxl engine\n",
    "                writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}_{}_risk'.format(country_code,infra_type,hazard_type,climate_model)+'.xlsx'), engine='openpyxl')\n",
    "                \n",
    "                # write each dataframe to a different sheet\n",
    "                if len(line_risk) != 0:\n",
    "                    line_risk[climate_model].to_excel(writer, sheet_name='line_risk')\n",
    "                if len(plant_risk) != 0:\n",
    "                    plant_risk[climate_model].to_excel(writer, sheet_name='plant_risk')\n",
    "                if len(substation_risk) != 0:\n",
    "                    substation_risk[climate_model].to_excel(writer, sheet_name='substation_risk')\n",
    "                if len(tower_risk) != 0:\n",
    "                    tower_risk[climate_model].to_excel(writer, sheet_name='tower_risk')\n",
    "                if len(pole_risk) != 0:\n",
    "                    pole_risk[climate_model].to_excel(writer, sheet_name='pole_risk')\n",
    "                \n",
    "                # save the Excel file\n",
    "                writer.save()\n",
    "\n",
    "    elif infra_type == 'gov':\n",
    "        line_risk,plant_risk,substation_risk = country_analysis_pg(country_code,hazard_type)\n",
    "        \n",
    "        if hazard_type == 'tc':\n",
    "            climate_models = ['','_CMCC-CM2-VHR4','_CNRM-CM6-1-HR','_EC-Earth3P-HR','_HadGEM3-GC31-HM']\n",
    "\n",
    "            for climate_model in climate_models:\n",
    "                if climate_model == '':\n",
    "                    writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}_{}_risk'.format(country_code,infra_type,hazard_type,'present')+'.xlsx'),\n",
    "                                            engine='openpyxl')\n",
    "                else:\n",
    "                    writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}{}_risk'.format(country_code,infra_type,hazard_type,climate_model)+'.xlsx'),\n",
    "                                            engine='openpyxl')\n",
    "                \n",
    "                # write each dataframe to a different sheet\n",
    "                if len(line_risk) != 0:\n",
    "                    line_risk[climate_model].to_excel(writer, sheet_name='line_risk')\n",
    "                if len(substation_risk) != 0:\n",
    "                    substation_risk[climate_model].to_excel(writer, sheet_name='substation_risk')\n",
    "                    \n",
    "                # save the Excel file\n",
    "                writer.save()\n",
    "\n",
    "        elif hazard_type == 'fl':\n",
    "            climate_models = ['historical','rcp8p5']\n",
    "\n",
    "            for climate_model in climate_models:\n",
    "\n",
    "                # create a Pandas Excel writer using openpyxl engine\n",
    "                writer = pd.ExcelWriter(os.path.join(output_path,'risk','{}_{}_{}_risk'.format(country_code,infra_type,hazard_type)+'.xlsx'), engine='openpyxl')\n",
    "                \n",
    "                # write each dataframe to a different sheet\n",
    "                if len(line_risk) != 0:\n",
    "                    line_risk[climate_model].to_excel(writer, sheet_name='line_risk')\n",
    "                if len(plant_risk) != 0:\n",
    "                    plant_risk[climate_model].to_excel(writer, sheet_name='plant_risk')\n",
    "                if len(substation_risk) != 0:\n",
    "                    substation_risk[climate_model].to_excel(writer, sheet_name='substation_risk')\n",
    "\n",
    "                # save the Excel file\n",
    "                writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d978c878-bbbf-47c5-abc2-2df27355e8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polyline damage calculation for JPN tc (): 100%|███████████████████████████████████████| 31/31 [00:03<00:00,  9.50it/s]\n",
      "point damage calculation for JPN tc (): 100%|██████████████████████████████████████████| 68/68 [00:09<00:00,  7.22it/s]\n",
      "polyline damage calculation for JPN tc (_CMCC-CM2-VHR4): 100%|█████████████████████████| 31/31 [00:03<00:00,  9.05it/s]\n",
      "point damage calculation for JPN tc (_CMCC-CM2-VHR4): 100%|████████████████████████████| 68/68 [00:09<00:00,  7.20it/s]\n",
      "polyline damage calculation for JPN tc (_CNRM-CM6-1-HR): 100%|█████████████████████████| 31/31 [00:03<00:00,  9.49it/s]\n",
      "point damage calculation for JPN tc (_CNRM-CM6-1-HR): 100%|████████████████████████████| 68/68 [00:09<00:00,  7.30it/s]\n",
      "polyline damage calculation for JPN tc (_EC-Earth3P-HR): 100%|█████████████████████████| 31/31 [00:03<00:00,  9.07it/s]\n",
      "point damage calculation for JPN tc (_EC-Earth3P-HR): 100%|████████████████████████████| 68/68 [00:09<00:00,  7.30it/s]\n",
      "polyline damage calculation for JPN tc (_HadGEM3-GC31-HM): 100%|███████████████████████| 31/31 [00:03<00:00,  9.42it/s]\n",
      "point damage calculation for JPN tc (_HadGEM3-GC31-HM): 100%|██████████████████████████| 68/68 [00:09<00:00,  7.09it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ''",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrisk_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJPN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[62], line 61\u001b[0m, in \u001b[0;36mrisk_output\u001b[1;34m(country_code, hazard_type, infra_type)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# write each dataframe to a different sheet\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line_risk) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 61\u001b[0m     \u001b[43mline_risk\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclimate_model\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_excel(writer, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mline_risk\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(substation_risk) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     63\u001b[0m     substation_risk[climate_model]\u001b[38;5;241m.\u001b[39mto_excel(writer, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubstation_risk\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "risk_output('JPN','tc','gov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "f25448e6-5b5d-434d-8332-8471a4a7e2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rp</th>\n",
       "      <th>curve</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>meandam</th>\n",
       "      <th>lowerdam</th>\n",
       "      <th>upperdam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>1.658665e+09</td>\n",
       "      <td>4.523633e+08</td>\n",
       "      <td>2.261816e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.500</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>1.730781e+09</td>\n",
       "      <td>4.720311e+08</td>\n",
       "      <td>2.360156e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>2.388258e+09</td>\n",
       "      <td>6.513430e+08</td>\n",
       "      <td>3.256715e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.100</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>2.552940e+09</td>\n",
       "      <td>6.962564e+08</td>\n",
       "      <td>3.481282e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.040</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>2.767454e+09</td>\n",
       "      <td>7.547601e+08</td>\n",
       "      <td>3.773801e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.020</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>2.921201e+09</td>\n",
       "      <td>7.966912e+08</td>\n",
       "      <td>3.983456e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.010</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>3.080762e+09</td>\n",
       "      <td>8.402079e+08</td>\n",
       "      <td>4.201039e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.004</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>3.356950e+09</td>\n",
       "      <td>9.155320e+08</td>\n",
       "      <td>4.577660e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.002</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>3.528684e+09</td>\n",
       "      <td>9.623684e+08</td>\n",
       "      <td>4.811842e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001</td>\n",
       "      <td>plant</td>\n",
       "      <td>plant</td>\n",
       "      <td>3.698459e+09</td>\n",
       "      <td>1.008671e+09</td>\n",
       "      <td>5.043354e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rp  curve asset_type       meandam      lowerdam      upperdam\n",
       "0   1.000  plant      plant  1.658665e+09  4.523633e+08  2.261816e+09\n",
       "2   0.500  plant      plant  1.730781e+09  4.720311e+08  2.360156e+09\n",
       "4   0.200  plant      plant  2.388258e+09  6.513430e+08  3.256715e+09\n",
       "6   0.100  plant      plant  2.552940e+09  6.962564e+08  3.481282e+09\n",
       "8   0.040  plant      plant  2.767454e+09  7.547601e+08  3.773801e+09\n",
       "10  0.020  plant      plant  2.921201e+09  7.966912e+08  3.983456e+09\n",
       "12  0.010  plant      plant  3.080762e+09  8.402079e+08  4.201039e+09\n",
       "14  0.004  plant      plant  3.356950e+09  9.155320e+08  4.577660e+09\n",
       "16  0.002  plant      plant  3.528684e+09  9.623684e+08  4.811842e+09\n",
       "18  0.001  plant      plant  3.698459e+09  1.008671e+09  5.043354e+09"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_damage_infra[1]['historical'].loc[osm_damage_infra[1]['historical']['asset_type'] == 'plant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "b3065e13-2e48-4dd3-b027-5d7815a9f94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.000\n",
       "1     1.000\n",
       "2     1.000\n",
       "3     0.500\n",
       "4     0.500\n",
       "5     0.500\n",
       "6     0.200\n",
       "7     0.200\n",
       "8     0.200\n",
       "9     0.100\n",
       "10    0.100\n",
       "11    0.100\n",
       "12    0.040\n",
       "13    0.040\n",
       "14    0.040\n",
       "15    0.020\n",
       "16    0.020\n",
       "17    0.020\n",
       "18    0.010\n",
       "19    0.010\n",
       "20    0.010\n",
       "21    0.004\n",
       "22    0.004\n",
       "23    0.004\n",
       "24    0.002\n",
       "25    0.002\n",
       "26    0.002\n",
       "27    0.001\n",
       "28    0.001\n",
       "29    0.001\n",
       "Name: rp, dtype: float64"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_damage_infra[0]['historical'].loc[:,\"rp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769479e-4a85-4931-af47-fb1ff09b73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def clip_gridfinder(country_code):\n",
    "    base_map_path = os.path.join(data_path,'base_map')\n",
    "\n",
    "    cty_boundary_path = os.path.join(base_map_path,'gadm41_{}.gpkg'.format(country_code))\n",
    "    cty_boundary = gpd.read_file(cty_boundary_path)\n",
    "    #mask = pd.DataFrame(mask.copy())\n",
    "    #mask.geometry = pygeos.from_shapely(mask.geometry)\n",
    "    #mask['geometry'] = reproject(mask)\n",
    "\n",
    "    gridfinder_path = r'C:\\Users\\mye500\\OneDrive - Vrije Universiteit Amsterdam\\01_Research-Projects\\01_risk_assessment\\PG_data\\gridfinder\\grid.gpkg'\n",
    "    gridfinder = gpd.read_file(gridfinder_path)\n",
    "    #gridfinder = pd.DataFrame(gridfinder.copy())\n",
    "    #gridfinder.geometry = pygeos.from_shapely(gridfinder.geometry)\n",
    "    #gridfinder['geometry'] = reproject(gridfinder)\n",
    "\n",
    "    clipped = gpd.clip(gridfinder,cty_boundary)\n",
    "\n",
    "    return clipped\n",
    "\n",
    "clip_gridfinder('TWN')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
